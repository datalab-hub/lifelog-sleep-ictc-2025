{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1f085c0",
   "metadata": {},
   "source": [
    "### feature ëŒ€ìƒìœ¼ë¡œ LGBM í•™ìŠµ  \n",
    "(Developed from dacon_etri_base_mod1.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0593c4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# í˜„ì¬ ë‚ ì§œ ë° ì‹œê°„ ê°€ì ¸ì˜¤ê¸°\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"%m%d_%H%M\")  # ì˜ˆ: 0517_1530\n",
    "\n",
    "\n",
    "submission_folder = '/users/KTL/Desktop/dacon/submission/'\n",
    "submission_file = f'submission_final_mod4_1_4_{timestamp}.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bead988",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import glob \n",
    "import random \n",
    "import os \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import ast \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903c2113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed ê³ ì • \n",
    "SD = 42 \n",
    "random.seed(SD) \n",
    "np.random.seed(SD) \n",
    "os.environ['PYTHONHASHSEED'] = str(SD)\n",
    "tf.random.set_seed(SD)  # TensorFlow ì‹œë“œ ì„¤ì •\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ ì„¤ì • - VSCode ìƒëŒ€ê²½ë¡œë¡œ ë³€ê²½\n",
    "# ì‹¤ì œ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì • í•„ìš”\n",
    "base_folder =  '/users/KTL/Desktop/ETRI_lifelog_dataset'\n",
    "folder = '/ch2025_data_items'\n",
    "\n",
    "data_dir = base_folder + folder \n",
    "\n",
    "\n",
    "# Parquet íŒŒì¼ ì „ì²´ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ \n",
    "parquet_files = glob.glob(os.path.join(data_dir, 'ch2025_*.parquet')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9028dc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŒŒì¼ ì´ë¦„ì„ í‚¤ë¡œ, DataFrameì„ ê°’ìœ¼ë¡œ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ \n",
    "lifelog_data = {} \n",
    "\n",
    "# íŒŒì¼ë³„ë¡œ ì½ê¸° \n",
    "for file_path in parquet_files: \n",
    "    name = os.path.basename(file_path).replace('.parquet', '').replace('ch2025_', '') \n",
    "    lifelog_data[name] = pd.read_parquet(file_path) \n",
    "    print(f\"âœ… Loaded: {name}, shape = {lifelog_data[name].shape}\") \n",
    "\n",
    "# ë”•ì…”ë„ˆë¦¬ì— ìˆëŠ” ëª¨ë“  í•­ëª©ì„ ë…ë¦½ì ì¸ ë³€ìˆ˜ë¡œ í• ë‹¹ \n",
    "for key, df in lifelog_data.items(): \n",
    "    globals()[f\"{key}_df\"] = df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbd9424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”íŠ¸ë¦­ìŠ¤ íŒŒì¼ ì½ê¸°\n",
    "metrics_train = pd.read_csv(base_folder + '/ch2025_metrics_train.csv')\n",
    "sample_submission = pd.read_csv(base_folder+'/ch2025_submission_sample.csv')\n",
    "\n",
    "# âœ… ê¸°ì¤€ ìŒ (subject_id, lifelog_date) \n",
    "sample_submission['lifelog_date'] = pd.to_datetime(sample_submission['lifelog_date']) \n",
    "test_keys = set(zip(sample_submission['subject_id'], sample_submission['lifelog_date'].dt.date)) \n",
    "\n",
    "# âœ… DataFrame ë³„ timestamp ì»¬ëŸ¼ ìˆ˜ë™ ì§€ì • \n",
    "dataframes = { \n",
    "    'mACStatus': (mACStatus_df, 'timestamp'), \n",
    "    'mActivity': (mActivity_df, 'timestamp'), \n",
    "    'mAmbience': (mAmbience_df, 'timestamp'), \n",
    "    'mBle': (mBle_df, 'timestamp'), \n",
    "    'mGps': (mGps_df, 'timestamp'), \n",
    "    'mLight': (mLight_df, 'timestamp'), \n",
    "    'mScreenStatus': (mScreenStatus_df, 'timestamp'), \n",
    "    'mUsageStats': (mUsageStats_df, 'timestamp'), \n",
    "    'mWifi': (mWifi_df, 'timestamp'), \n",
    "    'wHr': (wHr_df, 'timestamp'), \n",
    "    'wLight': (wLight_df, 'timestamp'), \n",
    "    'wPedo': (wPedo_df, 'timestamp'), \n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77b6fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… ë¶„ë¦¬ í•¨ìˆ˜ \n",
    "def split_test_train(df, subject_col='subject_id', timestamp_col='timestamp'): \n",
    "    df[timestamp_col] = pd.to_datetime(df[timestamp_col], errors='coerce') \n",
    "    df = df.dropna(subset=[timestamp_col]) \n",
    "    df['date_only'] = df[timestamp_col].dt.date \n",
    "    df['key'] = list(zip(df[subject_col], df['date_only'])) \n",
    "    test_df = df[df['key'].isin(test_keys)].drop(columns=['date_only', 'key']) \n",
    "    train_df = df[~df['key'].isin(test_keys)].drop(columns=['date_only', 'key']) \n",
    "    return test_df, train_df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3416230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… ê²°ê³¼ ì €ì¥ \n",
    "for name, (df, ts_col) in dataframes.items(): \n",
    "    print(f\"â³ {name} ë¶„ë¦¬ ì¤‘...\") \n",
    "    test_df, train_df = split_test_train(df.copy(), subject_col='subject_id', timestamp_col=ts_col) \n",
    "    globals()[f\"{name}_test\"] = test_df \n",
    "    globals()[f\"{name}_train\"] = train_df \n",
    "    print(f\"âœ… {name}_test â†’ {test_df.shape}, {name}_train â†’ {train_df.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f34d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mACStatus(df): \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    df = df.sort_values(['subject_id', 'timestamp']) \n",
    "    results = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        status = group['m_charging'].values # 0/1 ìƒíƒœ \n",
    "        times = group['timestamp'].values # ì¶©ì „ ìƒíƒœ ë¹„ìœ¨ \n",
    "        ratio_charging = status.mean() \n",
    "        # ìƒíƒœ ì „ì´ íšŸìˆ˜ \n",
    "        transitions = (status[1:] != status[:-1]).sum() \n",
    "        # ì—°ì†ëœ 1 ìƒíƒœ ê¸¸ì´ë“¤ \n",
    "        lengths = [] \n",
    "        current_len = 0 \n",
    "        for val in status: \n",
    "            if val == 1: \n",
    "                current_len += 1 \n",
    "            elif current_len > 0: \n",
    "                lengths.append(current_len) \n",
    "                current_len = 0 \n",
    "        if current_len > 0: \n",
    "            lengths.append(current_len) \n",
    "        avg_charging_duration = np.mean(lengths) if lengths else 0 \n",
    "        max_charging_duration = np.max(lengths) if lengths else 0 \n",
    "        results.append({ \n",
    "            'subject_id': subj, \n",
    "            'date': date, \n",
    "            'charging_ratio': ratio_charging, \n",
    "            'charging_transitions': transitions, \n",
    "            'avg_charging_duration': avg_charging_duration, \n",
    "            'max_charging_duration': max_charging_duration, \n",
    "        }) \n",
    "    return pd.DataFrame(results) \n",
    "\n",
    "mACStatus_df2 = process_mACStatus(mACStatus_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c3c0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mActivity(df): \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    summary = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        counts = group['m_activity'].value_counts(normalize=True) # ë¹„ìœ¨ \n",
    "        row = {'subject_id': subj, 'date': date} \n",
    "        # 0~8 ë¹„ìœ¨ ì €ì¥ \n",
    "        for i in range(9): \n",
    "            row[f'activity_{i}_ratio'] = counts.get(i, 0) \n",
    "        # ì£¼ìš” í™œë™ ì •ë³´ \n",
    "        row['dominant_activity'] = group['m_activity'].mode()[0] \n",
    "        row['num_unique_activities'] = group['m_activity'].nunique() \n",
    "        summary.append(row) \n",
    "    return pd.DataFrame(summary) \n",
    "\n",
    "mActivity_df2 = process_mActivity(mActivity_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddb753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì§€ì •ëœ 10ê°œ ë¼ë²¨ \n",
    "top_10_labels = [ \n",
    "    \"Inside, small room\", \"Speech\", \"Silence\", \"Music\", \"Narration, monologue\", \n",
    "    \"Child speech, kid speaking\", \"Conversation\", \"Speech synthesizer\", \"Shout\", \"Babbling\" \n",
    "] \n",
    "\n",
    "def process_mAmbience_top10(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    # ì´ˆê¸°í™” \n",
    "    for label in top_10_labels + ['others']: \n",
    "        df[label] = 0.0 \n",
    "    for idx, row in df.iterrows(): \n",
    "        parsed = ast.literal_eval(row['m_ambience']) if isinstance(row['m_ambience'], str) else row['m_ambience'] \n",
    "        others_prob = 0.0 \n",
    "        for label, prob in parsed: \n",
    "            prob = float(prob) \n",
    "            if label in top_10_labels: \n",
    "                df.at[idx, label] = prob \n",
    "            else: \n",
    "                others_prob += prob \n",
    "        df.at[idx, 'others'] = others_prob \n",
    "    return df.drop(columns=['m_ambience']) \n",
    "\n",
    "mAmbience_df2= process_mAmbience_top10(mAmbience_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cc98b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_mAmbience_daily(df): \n",
    "    prob_cols = [col for col in df.columns if col not in ['subject_id', 'timestamp', 'date']] \n",
    "    # í•˜ë£¨ ë‹¨ìœ„ë¡œ í‰ê· ê°’ ìš”ì•½ \n",
    "    daily_summary = df.groupby(['subject_id', 'date'])[prob_cols].mean().reset_index() \n",
    "    return daily_summary \n",
    "\n",
    "mAmbience_df2 = summarize_mAmbience_daily(mAmbience_df2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763e541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mBle(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    features = [] \n",
    "    for idx, row in df.iterrows(): \n",
    "        entry = ast.literal_eval(row['m_ble']) if isinstance(row['m_ble'], str) else row['m_ble'] \n",
    "        rssi_list = [] \n",
    "        class_0_cnt = 0 \n",
    "        class_other_cnt = 0 \n",
    "        for device in entry: \n",
    "            try: \n",
    "                rssi = int(device['rssi']) \n",
    "                rssi_list.append(rssi) \n",
    "                if str(device['device_class']) == '0': \n",
    "                    class_0_cnt += 1 \n",
    "                else: \n",
    "                    class_other_cnt += 1 \n",
    "            except: \n",
    "                continue # malformed record \n",
    "        feature = { \n",
    "            'subject_id': row['subject_id'], \n",
    "            'date': row['date'], \n",
    "            'device_class_0_cnt': class_0_cnt, \n",
    "            'device_class_others_cnt': class_other_cnt, \n",
    "            'device_count': len(rssi_list), \n",
    "            'rssi_mean': np.mean(rssi_list) if rssi_list else np.nan, \n",
    "            'rssi_min': np.min(rssi_list) if rssi_list else np.nan, \n",
    "            'rssi_max': np.max(rssi_list) if rssi_list else np.nan, \n",
    "        } \n",
    "        features.append(feature) \n",
    "    return pd.DataFrame(features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38ac08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_mBle_daily(df): \n",
    "    # row ë‹¨ìœ„ BLE feature ì¶”ì¶œ \n",
    "    df = process_mBle(df) \n",
    "    # í•˜ë£¨ ë‹¨ìœ„ë¡œ cnt í•©ì¹˜ê¸° \n",
    "    grouped = df.groupby(['subject_id', 'date']).agg({ \n",
    "        'device_class_0_cnt': 'sum', \n",
    "        'device_class_others_cnt': 'sum', \n",
    "        'rssi_mean': 'mean', \n",
    "        'rssi_min': 'min', \n",
    "        'rssi_max': 'max', \n",
    "    }).reset_index() \n",
    "    # ì´í•© êµ¬í•´ì„œ ë¹„ìœ¨ ê³„ì‚° \n",
    "    total_cnt = grouped['device_class_0_cnt'] + grouped['device_class_others_cnt'] \n",
    "    grouped['device_class_0_ratio'] = grouped['device_class_0_cnt'] / total_cnt.replace(0, np.nan) \n",
    "    grouped['device_class_others_ratio'] = grouped['device_class_others_cnt'] / total_cnt.replace(0, np.nan) \n",
    "    # í•„ìš” ì—†ëŠ” ì›ë˜ cnt ì»¬ëŸ¼ ì œê±° \n",
    "    grouped.drop(columns=['device_class_0_cnt', 'device_class_others_cnt'], inplace=True) \n",
    "    return grouped \n",
    "\n",
    "mBle_df2 = summarize_mBle_daily(mBle_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618d366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mGps(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    features = [] \n",
    "    for idx, row in df.iterrows(): \n",
    "        gps_list = ast.literal_eval(row['m_gps']) if isinstance(row['m_gps'], str) else row['m_gps'] \n",
    "        altitudes = [] \n",
    "        latitudes = [] \n",
    "        longitudes = [] \n",
    "        speeds = [] \n",
    "        for entry in gps_list: \n",
    "            try: \n",
    "                altitudes.append(float(entry['altitude'])) \n",
    "                latitudes.append(float(entry['latitude'])) \n",
    "                longitudes.append(float(entry['longitude'])) \n",
    "                speeds.append(float(entry['speed'])) \n",
    "            except: \n",
    "                continue \n",
    "        features.append({ \n",
    "            'subject_id': row['subject_id'], \n",
    "            'date': row['date'], \n",
    "            'altitude_mean': np.mean(altitudes) if altitudes else np.nan, \n",
    "            'latitude_std': np.std(latitudes) if latitudes else np.nan, \n",
    "            'longitude_std': np.std(longitudes) if longitudes else np.nan, \n",
    "            'speed_mean': np.mean(speeds) if speeds else np.nan, \n",
    "            'speed_max': np.max(speeds) if speeds else np.nan, \n",
    "            'speed_std': np.std(speeds) if speeds else np.nan, \n",
    "        }) \n",
    "    return pd.DataFrame(features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a0be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_Gps_df2 = process_mGps(mGps_df) \n",
    "m_Gps_df2 = m_Gps_df2.groupby(['subject_id', 'date']).agg({ \n",
    "    'altitude_mean': 'mean', \n",
    "    'latitude_std': 'mean', \n",
    "    'longitude_std': 'mean', \n",
    "    'speed_mean': 'mean', \n",
    "    'speed_max': 'max', \n",
    "    'speed_std': 'mean' \n",
    "}).reset_index() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8389d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mLight(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    df['hour'] = df['timestamp'].dt.hour \n",
    "    # ë°¤(22~05ì‹œ), ë‚®(06~21ì‹œ) êµ¬ë¶„ \n",
    "    df['is_night'] = df['hour'].apply(lambda h: h >= 22 or h < 6) \n",
    "    # í•˜ë£¨ ë‹¨ìœ„ ìš”ì•½ \n",
    "    daily = df.groupby(['subject_id', 'date']).agg( \n",
    "        light_mean=('m_light', 'mean'), \n",
    "        light_std=('m_light', 'std'), \n",
    "        light_max=('m_light', 'max'), \n",
    "        light_min=('m_light', 'min'), \n",
    "        light_night_mean=('m_light', lambda x: x[df.loc[x.index, 'is_night']].mean()), \n",
    "        light_day_mean=('m_light', lambda x: x[~df.loc[x.index, 'is_night']].mean()), \n",
    "        light_night_ratio=('is_night', 'mean') # ë°¤ ì‹œê°„ ì¸¡ì • ë¹„ìœ¨ \n",
    "    ).reset_index() \n",
    "    return daily \n",
    "\n",
    "mLight_df2 = process_mLight(mLight_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ad526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mScreenStatus(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    features = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        status = group['m_screen_use'].values \n",
    "        ratio_on = status.mean() \n",
    "        transitions = (status[1:] != status[:-1]).sum() \n",
    "        # ì—°ì†ëœ 1 ìƒíƒœ ê¸¸ì´ë“¤ \n",
    "        durations = [] \n",
    "        current = 0 \n",
    "        for val in status: \n",
    "            if val == 1: \n",
    "                current += 1 \n",
    "            elif current > 0: \n",
    "                durations.append(current) \n",
    "                current = 0 \n",
    "        if current > 0: \n",
    "            durations.append(current) \n",
    "        features.append({ \n",
    "            'subject_id': subj, \n",
    "            'date': date, \n",
    "            'screen_on_ratio': ratio_on, \n",
    "            'screen_on_transitions': transitions, \n",
    "            'screen_on_duration_avg': np.mean(durations) if durations else 0, \n",
    "            'screen_on_duration_max': np.max(durations) if durations else 0, \n",
    "        }) \n",
    "    return pd.DataFrame(features) \n",
    "\n",
    "mScreenStatus_df2 = process_mScreenStatus(mScreenStatus_df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04edbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_apps = [ \n",
    "    'One UI í™ˆ', 'ì¹´ì¹´ì˜¤í†¡', 'ì‹œìŠ¤í…œ UI', 'NAVER', 'ìºì‹œì›Œí¬', \n",
    "    'ì„±ê²½ì¼ë…Q', 'YouTube', 'í†µí™”', 'ë©”ì‹œì§€', 'íƒ€ì„ìŠ¤í”„ë ˆë“œ', 'Instagram'\n",
    "] \n",
    "\n",
    "def process_mUsageStats(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    features = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        app_time = {app: 0 for app in top_apps} \n",
    "        others_time = 0 \n",
    "        for row in group['m_usage_stats']: \n",
    "            parsed = ast.literal_eval(row) if isinstance(row, str) else row \n",
    "            for entry in parsed: \n",
    "                app = entry.get('app_name') \n",
    "                time = entry.get('total_time', 0) \n",
    "                if app in top_apps: \n",
    "                    app_time[app] += int(time) \n",
    "                else: \n",
    "                    others_time += int(time) \n",
    "        feature = { \n",
    "            'subject_id': subj, \n",
    "            'date': date, \n",
    "            'others_time': others_time \n",
    "        } \n",
    "        # ê° ì•±ë³„ ì»¬ëŸ¼ ì¶”ê°€ \n",
    "        feature.update({f'{app}_time': app_time[app] for app in top_apps}) \n",
    "        features.append(feature) \n",
    "    return pd.DataFrame(features) \n",
    "\n",
    "mUsageStats_df2 = process_mUsageStats(mUsageStats_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bcd6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_mWifi(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    results = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        rssi_all = [] \n",
    "        for row in group['m_wifi']: \n",
    "            parsed = ast.literal_eval(row) if isinstance(row, str) else row \n",
    "            for ap in parsed: \n",
    "                try: \n",
    "                    rssi = int(ap['rssi']) \n",
    "                    rssi_all.append(rssi) \n",
    "                except: \n",
    "                    continue \n",
    "        results.append({ \n",
    "            'subject_id': subj, \n",
    "            'date': date, \n",
    "            'wifi_rssi_mean': np.mean(rssi_all) if rssi_all else np.nan, \n",
    "            'wifi_rssi_min': np.min(rssi_all) if rssi_all else np.nan, \n",
    "            'wifi_rssi_max': np.max(rssi_all) if rssi_all else np.nan, \n",
    "            'wifi_detected_cnt': len(rssi_all) \n",
    "        }) \n",
    "    return pd.DataFrame(results) \n",
    "\n",
    "mWifi_df2 = process_mWifi(mWifi_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83782374",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_time_block(hour): \n",
    "    if 0 <= hour < 6: \n",
    "        return 'early_morning' \n",
    "    elif 6 <= hour < 12: \n",
    "        return 'morning' \n",
    "    elif 12 <= hour < 18: \n",
    "        return 'afternoon' \n",
    "    else: \n",
    "        return 'evening' \n",
    "\n",
    "def process_wHr_by_timeblock(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    df['block'] = df['timestamp'].dt.hour.map(get_time_block) \n",
    "    results = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        block_stats = {'subject_id': subj, 'date': date} \n",
    "        for block, block_group in group.groupby('block'): \n",
    "            hr_all = [] \n",
    "            for row in block_group['heart_rate']: \n",
    "                parsed = ast.literal_eval(row) if isinstance(row, str) else row \n",
    "                hr_all.extend([int(h) for h in parsed if h is not None]) \n",
    "            if not hr_all: \n",
    "                continue \n",
    "            above_100 = [hr for hr in hr_all if hr > 100] \n",
    "            block_stats[f'hr_{block}_mean'] = np.mean(hr_all) \n",
    "            block_stats[f'hr_{block}_std'] = np.std(hr_all) \n",
    "            block_stats[f'hr_{block}_max'] = np.max(hr_all) \n",
    "            block_stats[f'hr_{block}_min'] = np.min(hr_all) \n",
    "            block_stats[f'hr_{block}_above_100_ratio'] = len(above_100) / len(hr_all) \n",
    "        results.append(block_stats) \n",
    "    return pd.DataFrame(results) \n",
    "\n",
    "wHr_df2 = process_wHr_by_timeblock(wHr_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39b5000",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_wLight_by_timeblock(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    df['block'] = df['timestamp'].dt.hour.map(get_time_block) \n",
    "    results = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        block_stats = {'subject_id': subj, 'date': date} \n",
    "        for block, block_group in group.groupby('block'): \n",
    "            lux = block_group['w_light'].dropna().values \n",
    "            if len(lux) == 0: \n",
    "                continue \n",
    "            block_stats[f'wlight_{block}_mean'] = np.mean(lux) \n",
    "            block_stats[f'wlight_{block}_std'] = np.std(lux) \n",
    "            block_stats[f'wlight_{block}_max'] = np.max(lux) \n",
    "            block_stats[f'wlight_{block}_min'] = np.min(lux) \n",
    "        results.append(block_stats) \n",
    "    return pd.DataFrame(results) \n",
    "\n",
    "wLight_df2 = process_wLight_by_timeblock(wLight_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55ce49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_wPedo(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    summary = df.groupby(['subject_id', 'date']).agg({ \n",
    "        'step': 'sum', \n",
    "        'step_frequency': 'mean', \n",
    "        'distance': 'sum', \n",
    "        'speed': ['mean', 'max'], \n",
    "        'burned_calories': 'sum' \n",
    "    }).reset_index() \n",
    "    # ì»¬ëŸ¼ ì´ë¦„ ì •ë¦¬ \n",
    "    summary.columns = ['subject_id', 'date', 'step_sum', 'step_frequency_mean', 'distance_sum', 'speed_mean', 'speed_max', 'burned_calories_sum'] \n",
    "    return summary \n",
    "\n",
    "wPedo_df2 = process_wPedo(wPedo_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c9a3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from functools import reduce \n",
    "df_list = [ \n",
    "    mACStatus_df2, \n",
    "    mActivity_df2, \n",
    "    mAmbience_df2, \n",
    "    mBle_df2, \n",
    "    m_Gps_df2, \n",
    "    mLight_df2, \n",
    "    mScreenStatus_df2, \n",
    "    mUsageStats_df2, \n",
    "    mWifi_df2, \n",
    "    wHr_df2, \n",
    "    wLight_df2, \n",
    "    wPedo_df2 \n",
    "] \n",
    "\n",
    "merged_df = reduce(lambda left, right: pd.merge(left, right, on=['subject_id', 'date'], how='outer'), df_list) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5335404",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# metrics_trainì˜ lifelog_date â†’ datetime.date í˜•ìœ¼ë¡œ ë³€í™˜ \n",
    "metrics_train['lifelog_date'] = pd.to_datetime(metrics_train['lifelog_date']).dt.date \n",
    "\n",
    "# merged_dfì˜ dateë„ ë³€í™˜ \n",
    "merged_df['date'] = pd.to_datetime(merged_df['date']).dt.date \n",
    "\n",
    "# 1. date ê¸°ì¤€ ì •ë ¬ì„ ìœ„í•´ metrics_trainì˜ lifelog_date -> dateë¡œ ë§ì¶”ê¸° \n",
    "metrics_train_renamed = metrics_train.rename(columns={'lifelog_date': 'date'}) \n",
    "\n",
    "# 2. train_df: metrics_trainê³¼ ì¼ì¹˜í•˜ëŠ” (subject_id, date) â†’ ë¼ë²¨ í¬í•¨ \n",
    "train_df = pd.merge(metrics_train_renamed, merged_df, on=['subject_id', 'date'], how='inner') \n",
    "\n",
    "# 3. test_df: metrics_trainì— ì—†ëŠ” (subject_id, date) \n",
    "merged_keys = merged_df[['subject_id', 'date']] \n",
    "train_keys = metrics_train_renamed[['subject_id', 'date']] \n",
    "test_keys = pd.merge(merged_keys, train_keys, on=['subject_id', 'date'], how='left', indicator=True) \n",
    "test_keys = test_keys[test_keys['_merge'] == 'left_only'].drop(columns=['_merge']) \n",
    "test_df = pd.merge(test_keys, merged_df, on=['subject_id', 'date'], how='left') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c98b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# âœ… íƒ€ê²Ÿ ë¦¬ìŠ¤íŠ¸ \n",
    "targets_binary = ['Q1', 'Q2', 'Q3', 'S2', 'S3'] \n",
    "target_multiclass = 'S1' \n",
    "\n",
    "# âœ… feature ì¤€ë¹„ \n",
    "X = train_df.drop(columns=['subject_id', 'sleep_date', 'date', 'Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']) \n",
    "X.fillna(0, inplace=True) # ê²°ì¸¡ê°’ ì²˜ë¦¬ \n",
    "\n",
    "test_X = test_df.drop(columns=['subject_id', 'date']) \n",
    "test_X.fillna(0, inplace=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d497e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ì»¬ëŸ¼ ì´ë¦„ì—ì„œ íŠ¹ìˆ˜ ë¬¸ì ì œê±°/ë³€í™˜ \n",
    "def sanitize_column_names(df): \n",
    "    df.columns = ( \n",
    "        df.columns \n",
    "        .str.replace(r\"[^\\w]\", \"_\", regex=True) # íŠ¹ìˆ˜ë¬¸ì â†’ _ \n",
    "        .str.replace(r\"__+\", \"_\", regex=True) # ì—°ì†ëœ _ ì œê±° \n",
    "        .str.strip(\"_\") # ì•ë’¤ _ ì œê±° \n",
    "    ) \n",
    "    return df \n",
    "\n",
    "# ëª¨ë“  ì…ë ¥ì— ì ìš© \n",
    "X = sanitize_column_names(X) \n",
    "test_X = sanitize_column_names(test_X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3ca789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ ì €ì¥ \n",
    "# ê·¸ë£¹ë³„ ì¤‘ìš” íŠ¹ì„± ì„ íƒ\n",
    "important_features = [\n",
    "    # ìŠ¤í¬ë¦° ê´€ë ¨ íŠ¹ì„±\n",
    "    'screen_on_ratio', 'screen_on_duration_avg', 'screen_on_duration_max',\n",
    "    \n",
    "    # ì™€ì´íŒŒì´ ì‹ í˜¸ ê´€ë ¨ íŠ¹ì„±\n",
    "    'wifi_rssi_max', 'wifi_rssi_mean', 'wifi_detected_cnt',\n",
    "    \n",
    "    # í™œë™ ê´€ë ¨ íŠ¹ì„±\n",
    "    'activity_3_ratio', 'activity_4_ratio',\n",
    "    \n",
    "    # ì¶©ì „ ê´€ë ¨ íŠ¹ì„±\n",
    "    'charging_ratio', 'max_charging_duration', 'avg_charging_duration',\n",
    "    \n",
    "    # ì‹ í˜¸ ê´€ë ¨ íŠ¹ì„±\n",
    "    'rssi_mean',\n",
    "    \n",
    "    # ë¹› ê´€ë ¨ íŠ¹ì„±\n",
    "    'light_night_mean', 'light_max', 'light_std',\n",
    "    \n",
    "    # ìœ„ì¹˜/ì›€ì§ì„ ê´€ë ¨ íŠ¹ì„±\n",
    "    'altitude_mean', 'speed_max_x',\n",
    "    \n",
    "    # ì•± ì‚¬ìš© ê´€ë ¨ íŠ¹ì„±\n",
    "    'ë©”ì‹œì§€_time', 'others_time', 'Narration_monologue'\n",
    "]\n",
    "\n",
    "# ì„ íƒëœ íŠ¹ì„±ë§Œìœ¼ë¡œ ë°ì´í„°ì…‹ êµ¬ì„±\n",
    "X_selected = X[important_features]\n",
    "test_X_selected = test_X[important_features]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d1d732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping \n",
    "\n",
    "# GridSearchë¥¼ ìœ„í•œ êµì°¨ ê²€ì¦ ì„¤ì •\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# LightGBM ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "lgbm_params = {\n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate': 0.03,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "# ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ê·¸ë¦¬ë“œ ì„œì¹˜ íŒŒë¼ë¯¸í„°\n",
    "binary_param_grid = {\n",
    "    'learning_rate': [0.01, 0.03],\n",
    "    'n_estimators': [500, 1000],\n",
    "    'num_leaves': [50, 100],\n",
    "    'max_depth': [-1, 5],\n",
    "    'min_child_samples': [10, 30],\n",
    "    'reg_alpha': [0.01, 0.1],\n",
    "    'reg_lambda': [0.01, 0.1],\n",
    "}\n",
    "\n",
    "# ë‹¤ì¤‘ ë¶„ë¥˜ ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ê·¸ë¦¬ë“œ ì„œì¹˜ íŒŒë¼ë¯¸í„°\n",
    "multi_param_grid = {\n",
    "    'learning_rate': [0.01, 0.03],\n",
    "    'n_estimators': [500, 1000],\n",
    "    'num_leaves': [50, 100],\n",
    "    'max_depth': [-1, 5],\n",
    "    'min_child_samples': [10, 30],\n",
    "    'reg_alpha': [0.01, 0.1],\n",
    "    'reg_lambda': [0.01, 0.1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcb2c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n===== ì˜¤í† ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•œ ê³ ê¸‰ íŠ¹ì„± ì¶”ì¶œ =====\")\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "encoding_dim = min(64, X_selected.shape[1])  # ì¸ì½”ë”© ì°¨ì› (ì›ë³¸ íŠ¹ì„± ìˆ˜ë³´ë‹¤ ì‘ê²Œ ì„¤ì •)\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "\n",
    "# ì…ë ¥ ì°¨ì›\n",
    "input_dim = X_selected.shape[1]\n",
    "\n",
    "# ì˜¤í† ì¸ì½”ë” ëª¨ë¸ êµ¬ì¶•\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "\n",
    "# ì¸ì½”ë” ë¶€ë¶„\n",
    "encoded = Dense(128, activation='relu')(input_layer)\n",
    "encoded = BatchNormalization()(encoded)\n",
    "encoded = Dropout(0.3)(encoded)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "encoded = BatchNormalization()(encoded)\n",
    "encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "\n",
    "# ë””ì½”ë” ë¶€ë¶„\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = BatchNormalization()(decoded)\n",
    "decoded = Dropout(0.3)(decoded)\n",
    "decoded = Dense(128, activation='relu')(decoded)\n",
    "decoded = BatchNormalization()(decoded)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "# ì „ì²´ ì˜¤í† ì¸ì½”ë” ëª¨ë¸\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# ì¸ì½”ë” ë¶€ë¶„ë§Œ ì¶”ì¶œ (íŠ¹ì„± ì¶”ì¶œìš©)\n",
    "encoder = Model(input_layer, encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea730fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í›ˆë ¨/ê²€ì¦ ì„¸íŠ¸ ë¶„ë¦¬\n",
    "X_train_auto, X_val_auto = train_test_split(X_selected, test_size=0.2, random_state=42)\n",
    "\n",
    "# ì¡°ê¸° ì¢…ë£Œ ì„¤ì •\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# ì˜¤í† ì¸ì½”ë” ëª¨ë¸ í•™ìŠµ\n",
    "print(\"ì˜¤í† ì¸ì½”ë” í•™ìŠµ ì¤‘...\")\n",
    "autoencoder.fit(\n",
    "    X_train_auto, X_train_auto,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_val_auto, X_val_auto),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ì¸ì½”ë”©ëœ íŠ¹ì„± ì¶”ì¶œ\n",
    "print(\"ì¸ì½”ë”©ëœ íŠ¹ì„± ì¶”ì¶œ ì¤‘...\")\n",
    "encoded_features_train = encoder.predict(X_selected)\n",
    "encoded_features_test = encoder.predict(test_X_selected)\n",
    "\n",
    "# ì›ë³¸ íŠ¹ì„±ê³¼ ì¸ì½”ë”©ëœ íŠ¹ì„± ê²°í•©\n",
    "X_combined = np.hstack([X_selected, encoded_features_train])\n",
    "test_X_combined = np.hstack([test_X_selected, encoded_features_test])\n",
    "\n",
    "print(f\"ì›ë³¸ íŠ¹ì„± ìˆ˜: {X_selected.shape[1]}\")\n",
    "print(f\"ì¸ì½”ë”©ëœ íŠ¹ì„± ìˆ˜: {encoded_features_train.shape[1]}\")\n",
    "print(f\"ê²°í•©ëœ íŠ¹ì„± ìˆ˜: {X_combined.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5fdf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ------------------ ê²°í•©ëœ íŠ¹ì„±ìœ¼ë¡œ ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ------------------\n",
    "\n",
    "# # ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ ìµœì í™” ë° í•™ìŠµ\n",
    "# binary_preds_combined = {}\n",
    "# for col in targets_binary:\n",
    "#     y = train_df[col]\n",
    "    \n",
    "#     # ì´ì§„ ë¶„ë¥˜ìš© ëª¨ë¸ ì„¤ì •\n",
    "#     binary_model = LGBMClassifier(objective='binary', **lgbm_params)\n",
    "    \n",
    "#     # ê·¸ë¦¬ë“œ ì„œì¹˜ë¡œ ìµœì  íŒŒë¼ë¯¸í„° ì°¾ê¸°\n",
    "#     grid_search = GridSearchCV(\n",
    "#         estimator=binary_model,\n",
    "#         param_grid=binary_param_grid,\n",
    "#         scoring='f1',  # ë˜ëŠ” 'roc_auc', 'precision', 'recall' ë“± ë¬¸ì œì— ì í•©í•œ ì§€í‘œ ì„ íƒ\n",
    "#         cv=cv,\n",
    "#         verbose=1,\n",
    "#         n_jobs=-1\n",
    "#     )\n",
    "    \n",
    "#     print(f\"\\n{col} ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ ìµœì í™” ì¤‘ (ê²°í•©ëœ íŠ¹ì„± ì‚¬ìš©)...\")\n",
    "#     grid_search.fit(X_combined, y)\n",
    "    \n",
    "#     print(f\"ìµœì  íŒŒë¼ë¯¸í„°: {grid_search.best_params_}\")\n",
    "#     print(f\"ìµœê³  ì ìˆ˜: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "#     # ìµœì  ëª¨ë¸ë¡œ ì˜ˆì¸¡\n",
    "#     best_model = grid_search.best_estimator_\n",
    "#     binary_preds_combined[col] = best_model.predict(test_X_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c15cb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ ê²°í•©ëœ íŠ¹ì„±ìœ¼ë¡œ ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ (SMOTE ì ìš©) ------------------\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ğŸ¯ ê²°í•©ëœ íŠ¹ì„±ìœ¼ë¡œ ì´ì§„ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ (SMOTE ì ìš©)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ ìµœì í™” ë° í•™ìŠµ\n",
    "binary_preds_combined = {}\n",
    "binary_models_combined = {}\n",
    "smote_info_combined = {}\n",
    "\n",
    "for i, col in enumerate(targets_binary):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"ğŸ”„ ì§„í–‰: {i+1}/{len(targets_binary)} - {col} ì´ì§„ë¶„ë¥˜\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    y = train_df[col]\n",
    "    \n",
    "    print(f\"   - íƒ€ê²Ÿ: {col}\")\n",
    "    print(f\"   - ì‚¬ìš© íŠ¹ì„±: ê²°í•©ëœ íŠ¹ì„± (ì˜¤í† ì¸ì½”ë” + ì›ë³¸)\")\n",
    "    print(f\"   - í›ˆë ¨ ìƒ˜í”Œ ìˆ˜: {X_combined.shape[0]}ê°œ\")\n",
    "    print(f\"   - íŠ¹ì„± ìˆ˜: {X_combined.shape[1]}ê°œ\")\n",
    "    \n",
    "    # SMOTE ì ìš© ì „ í´ë˜ìŠ¤ ë¶„í¬\n",
    "    print(f\"\\nğŸ“Š SMOTE ì ìš© ì „ í´ë˜ìŠ¤ ë¶„í¬:\")\n",
    "    original_counts = y.value_counts().sort_index()\n",
    "    print(f\"   {dict(original_counts)}\")\n",
    "    \n",
    "    # í´ë˜ìŠ¤ ë¶ˆê· í˜• í™•ì¸\n",
    "    if len(original_counts) == 2:\n",
    "        ratio = original_counts.max() / original_counts.min()\n",
    "        print(f\"   ë¶ˆê· í˜• ë¹„ìœ¨: {ratio:.2f}:1\")\n",
    "    \n",
    "    # SMOTE ì ìš©\n",
    "    print(f\"\\nğŸ”§ SMOTE ì ìš© ì¤‘...\")\n",
    "    try:\n",
    "        # k_neighbors ìë™ ì¡°ì • (ì†Œìˆ˜ í´ë˜ìŠ¤ ìƒ˜í”Œ ìˆ˜ ê³ ë ¤)\n",
    "        min_class_count = min(y.value_counts())\n",
    "        if min_class_count <= 3:\n",
    "            k_neighbors = max(1, min_class_count - 1)\n",
    "            print(f\"   âš ï¸  ì†Œìˆ˜ í´ë˜ìŠ¤ ìƒ˜í”Œì´ ì ì–´ k_neighborsë¥¼ {k_neighbors}ë¡œ ì¡°ì •\")\n",
    "        else:\n",
    "            k_neighbors = 3\n",
    "        \n",
    "        # SMOTE ì ìš© í›„ í´ë˜ìŠ¤ ë¶„í¬ ë¯¸ë¦¬ë³´ê¸°\n",
    "        smote_preview = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
    "        X_combined_smote, y_smote = smote_preview.fit_resample(X_combined, y)\n",
    "        smote_success = True\n",
    "        \n",
    "        # SMOTE ì ìš© í›„ í´ë˜ìŠ¤ ë¶„í¬\n",
    "        print(f\"ğŸ“Š SMOTE ì ìš© í›„ í´ë˜ìŠ¤ ë¶„í¬:\")\n",
    "        smote_counts = pd.Series(y_smote).value_counts().sort_index()\n",
    "        print(f\"   {dict(smote_counts)}\")\n",
    "        print(f\"   ì´ ìƒ˜í”Œ ìˆ˜: {X_combined_smote.shape[0]}ê°œ (ì¦ê°€: +{X_combined_smote.shape[0] - X_combined.shape[0]}ê°œ)\")\n",
    "        \n",
    "        # SMOTE ì •ë³´ ì €ì¥\n",
    "        smote_info_combined[col] = {\n",
    "            'original_counts': dict(original_counts),\n",
    "            'smote_counts': dict(smote_counts),\n",
    "            'original_samples': X_combined.shape[0],\n",
    "            'smote_samples': X_combined_smote.shape[0],\n",
    "            'increase': X_combined_smote.shape[0] - X_combined.shape[0]\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ SMOTE ì ìš© ì‹¤íŒ¨: {e}\")\n",
    "        print(f\"   ğŸ”„ ì›ë³¸ ë°ì´í„°ë¡œ ì§„í–‰...\")\n",
    "        X_combined_smote, y_smote = X_combined, y\n",
    "        smote_success = False\n",
    "        \n",
    "        smote_info_combined[col] = {\n",
    "            'original_counts': dict(original_counts),\n",
    "            'smote_counts': dict(original_counts),\n",
    "            'original_samples': X_combined.shape[0],\n",
    "            'smote_samples': X_combined.shape[0],\n",
    "            'increase': 0,\n",
    "            'smote_failed': True\n",
    "        }\n",
    "    \n",
    "    # ì´ì§„ ë¶„ë¥˜ìš© ëª¨ë¸ ì„¤ì •\n",
    "    binary_model = LGBMClassifier(objective='binary', **lgbm_params)\n",
    "    \n",
    "    # ê·¸ë¦¬ë“œ ì„œì¹˜ë¡œ ìµœì  íŒŒë¼ë¯¸í„° ì°¾ê¸°\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=binary_model,\n",
    "        param_grid=binary_param_grid,\n",
    "        scoring='f1',  # ë˜ëŠ” 'roc_auc', 'precision', 'recall' ë“± ë¬¸ì œì— ì í•©í•œ ì§€í‘œ ì„ íƒ\n",
    "        cv=cv,\n",
    "        verbose=1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    if smote_success:\n",
    "        print(f\"\\nğŸ” {col} ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ ìµœì í™” ì¤‘ (ê²°í•©ëœ íŠ¹ì„± + SMOTE ì‚¬ìš©)...\")\n",
    "        grid_search.fit(X_combined_smote, y_smote)\n",
    "    else:\n",
    "        print(f\"\\nğŸ” {col} ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ ìµœì í™” ì¤‘ (ê²°í•©ëœ íŠ¹ì„± ì‚¬ìš©)...\")\n",
    "        grid_search.fit(X_combined, y)\n",
    "    \n",
    "    print(f\"   âœ… ìµœì í™” ì™„ë£Œ!\")\n",
    "    print(f\"      - ìµœì  íŒŒë¼ë¯¸í„°: {grid_search.best_params_}\")\n",
    "    print(f\"      - ìµœê³  F1 ì ìˆ˜: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # ìµœì  ëª¨ë¸ë¡œ ì˜ˆì¸¡\n",
    "    best_model = grid_search.best_estimator_\n",
    "    binary_preds_combined[col] = best_model.predict(test_X_combined)\n",
    "    \n",
    "    # ëª¨ë¸ ì €ì¥\n",
    "    binary_models_combined[col] = {\n",
    "        'model': best_model,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'smote_applied': smote_success\n",
    "    }\n",
    "    \n",
    "    print(f\"      - í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ì™„ë£Œ: {len(binary_preds_combined[col])}ê°œ ìƒ˜í”Œ\")\n",
    "    pred_counts = pd.Series(binary_preds_combined[col]).value_counts().sort_index()\n",
    "    print(f\"      - ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ ë¶„í¬: {dict(pred_counts)}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ğŸ‰ ëª¨ë“  ê²°í•©ëœ íŠ¹ì„± ì´ì§„ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# ğŸ“Š ì „ì²´ ê²°ê³¼ ìš”ì•½\n",
    "print(f\"\\nğŸ“Š ê²°í•©ëœ íŠ¹ì„± ì´ì§„ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ìš”ì•½:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"   ì´ í•™ìŠµëœ ëª¨ë¸: {len(binary_models_combined)}ê°œ\")\n",
    "\n",
    "for col in targets_binary:\n",
    "    if col in binary_models_combined:\n",
    "        model_info = binary_models_combined[col]\n",
    "        smote_status = \"âœ… SMOTE ì ìš©\" if model_info['smote_applied'] else \"âŒ SMOTE ë¯¸ì ìš©\"\n",
    "        print(f\"   - {col}: F1={model_info['best_score']:.4f} ({smote_status})\")\n",
    "\n",
    "# ğŸ“ˆ SMOTE ì ìš© íš¨ê³¼ ë¶„ì„\n",
    "print(f\"\\nğŸ“ˆ SMOTE ì ìš© íš¨ê³¼ ë¶„ì„:\")\n",
    "print(\"-\" * 60)\n",
    "for col in targets_binary:\n",
    "    if col in smote_info_combined:\n",
    "        info = smote_info_combined[col]\n",
    "        if not info.get('smote_failed', False):\n",
    "            print(f\"\\nğŸ”¹ {col}:\")\n",
    "            print(f\"      ì›ë³¸ ìƒ˜í”Œ: {info['original_samples']}ê°œ\")\n",
    "            print(f\"      SMOTE í›„: {info['smote_samples']}ê°œ (+{info['increase']}ê°œ)\")\n",
    "            print(f\"      ì›ë³¸ ë¶„í¬: {info['original_counts']}\")\n",
    "            print(f\"      SMOTE ë¶„í¬: {info['smote_counts']}\")\n",
    "\n",
    "# ğŸ“ ê²°í•©ëœ íŠ¹ì„± ì´ì§„ë¶„ë¥˜ ì˜ˆì¸¡ ê²°ê³¼ DataFrame ìƒì„±\n",
    "def create_combined_binary_predictions_df():\n",
    "    \"\"\"ê²°í•©ëœ íŠ¹ì„± ì´ì§„ë¶„ë¥˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ìƒì„±\"\"\"\n",
    "    return pd.DataFrame(binary_preds_combined)\n",
    "\n",
    "print(f\"\\nğŸ“Œ ê²°í•©ëœ íŠ¹ì„± ì´ì§„ë¶„ë¥˜ ê²°ê³¼ ì ‘ê·¼ ë°©ë²•:\")\n",
    "print(f\"   - ì˜ˆì¸¡ ê²°ê³¼: binary_preds_combined\")\n",
    "print(f\"   - ëª¨ë¸ ì •ë³´: binary_models_combined\")\n",
    "print(f\"   - SMOTE ì •ë³´: smote_info_combined\")\n",
    "print(f\"   - DataFrame ìƒì„±: create_combined_binary_predictions_df()\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ìµœì¢… ê²°ê³¼:\")\n",
    "print(f\"   - ê²°í•©ëœ íŠ¹ì„±ìœ¼ë¡œ ëª¨ë“  ì´ì§„ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ (SMOTE ì ìš©) âœ…\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3171ae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ------------------ ê²°í•©ëœ íŠ¹ì„±ìœ¼ë¡œ ë‹¤ì¤‘ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ------------------\n",
    "\n",
    "# # ë‹¤ì¤‘ ë¶„ë¥˜ (S1) ëª¨ë¸ ìµœì í™” ë° í•™ìŠµ\n",
    "# y_multi = train_df['S1']\n",
    "# n_classes = len(y_multi.unique())\n",
    "\n",
    "# # ë‹¤ì¤‘ ë¶„ë¥˜ìš© ëª¨ë¸ ì„¤ì •\n",
    "# multiclass_model = LGBMClassifier(objective='multiclass', num_class=n_classes, **lgbm_params)\n",
    "\n",
    "# # ê·¸ë¦¬ë“œ ì„œì¹˜ë¡œ ìµœì  íŒŒë¼ë¯¸í„° ì°¾ê¸°\n",
    "# grid_search_multi = GridSearchCV(\n",
    "#     estimator=multiclass_model,\n",
    "#     param_grid=multi_param_grid,\n",
    "#     scoring='f1_macro',  # ë‹¤ì¤‘ ë¶„ë¥˜ì—ì„œëŠ” macro í‰ê·  ì‚¬ìš©\n",
    "#     cv=cv,\n",
    "#     verbose=1,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# print(\"\\nS1 ë‹¤ì¤‘ ë¶„ë¥˜ ëª¨ë¸ ìµœì í™” ì¤‘ (ê²°í•©ëœ íŠ¹ì„± ì‚¬ìš©)...\")\n",
    "# grid_search_multi.fit(X_combined, y_multi)\n",
    "\n",
    "# print(f\"ìµœì  íŒŒë¼ë¯¸í„°: {grid_search_multi.best_params_}\")\n",
    "# print(f\"ìµœê³  ì ìˆ˜: {grid_search_multi.best_score_:.4f}\")\n",
    "\n",
    "# # ìµœì  ëª¨ë¸ë¡œ ì˜ˆì¸¡\n",
    "# best_multi_model = grid_search_multi.best_estimator_\n",
    "# multiclass_pred_combined = best_multi_model.predict(test_X_combined)\n",
    "\n",
    "# # ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ ì„±ëŠ¥ í‰ê°€\n",
    "# X_train_comb, X_val_comb, y_train, y_val = train_test_split(\n",
    "#     X_combined, y_multi, test_size=0.2, random_state=42, stratify=y_multi\n",
    "# )\n",
    "\n",
    "# # ìµœì  íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ì¦ìš© ëª¨ë¸ í•™ìŠµ\n",
    "# eval_model_comb = LGBMClassifier(**grid_search_multi.best_params_)\n",
    "# eval_model_comb.fit(X_train_comb, y_train)\n",
    "\n",
    "# # ì˜ˆì¸¡ ë° í‰ê°€\n",
    "# y_pred_comb = eval_model_comb.predict(X_val_comb)\n",
    "\n",
    "# print(\"\\n===== ì˜¤í† ì¸ì½”ë” + LightGBM ê²°í•© ëª¨ë¸ í‰ê°€ (S1) =====\")\n",
    "# print(classification_report(y_val, y_pred_comb))\n",
    "# print(\"\\ní˜¼ë™ í–‰ë ¬:\")\n",
    "# print(confusion_matrix(y_val, y_pred_comb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256f05cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ ê²°í•©ëœ íŠ¹ì„±ìœ¼ë¡œ ë‹¤ì¤‘ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ------------------\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "\n",
    "# ë‹¤ì¤‘ ë¶„ë¥˜ (S1) ëª¨ë¸ ìµœì í™” ë° í•™ìŠµ\n",
    "y_multi = train_df['S1']\n",
    "n_classes = len(y_multi.unique())\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ğŸ¯ S1 ë‹¤ì¤‘ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ (ê²°í•©ëœ íŠ¹ì„± + SMOTE)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"   - íƒ€ê²Ÿ: S1 (ë‹¤ì¤‘ë¶„ë¥˜)\")\n",
    "print(f\"   - í´ë˜ìŠ¤ ìˆ˜: {n_classes}ê°œ\")\n",
    "print(f\"   - ì‚¬ìš© íŠ¹ì„±: ê²°í•©ëœ íŠ¹ì„± (ì˜¤í† ì¸ì½”ë” + ì›ë³¸)\")\n",
    "print(f\"   - í›ˆë ¨ ìƒ˜í”Œ ìˆ˜: {X_combined.shape[0]}ê°œ\")\n",
    "print(f\"   - íŠ¹ì„± ìˆ˜: {X_combined.shape[1]}ê°œ\")\n",
    "\n",
    "# SMOTE ì ìš© ì „ í´ë˜ìŠ¤ ë¶„í¬\n",
    "print(f\"\\nğŸ“Š SMOTE ì ìš© ì „ í´ë˜ìŠ¤ ë¶„í¬:\")\n",
    "print(f\"   {dict(y_multi.value_counts().sort_index())}\")\n",
    "\n",
    "# SMOTE ì ìš©\n",
    "print(f\"\\nğŸ”§ SMOTE ì ìš© ì¤‘...\")\n",
    "smote = SMOTE(random_state=42, k_neighbors=3)\n",
    "X_combined_smote, y_multi_smote = smote.fit_resample(X_combined, y_multi)\n",
    "\n",
    "# SMOTE ì ìš© í›„ í´ë˜ìŠ¤ ë¶„í¬\n",
    "print(f\"ğŸ“Š SMOTE ì ìš© í›„ í´ë˜ìŠ¤ ë¶„í¬:\")\n",
    "print(f\"   {dict(pd.Series(y_multi_smote).value_counts().sort_index())}\")\n",
    "print(f\"   ì´ ìƒ˜í”Œ ìˆ˜: {X_combined_smote.shape[0]}ê°œ (ì¦ê°€: +{X_combined_smote.shape[0] - X_combined.shape[0]}ê°œ)\")\n",
    "\n",
    "# ë‹¤ì¤‘ ë¶„ë¥˜ìš© ëª¨ë¸ ì„¤ì •\n",
    "multiclass_model = LGBMClassifier(\n",
    "    objective='multiclass',\n",
    "    num_class=n_classes,\n",
    "    **lgbm_params\n",
    ")\n",
    "\n",
    "# ê·¸ë¦¬ë“œ ì„œì¹˜ë¡œ ìµœì  íŒŒë¼ë¯¸í„° ì°¾ê¸°\n",
    "grid_search_multi = GridSearchCV(\n",
    "    estimator=multiclass_model,\n",
    "    param_grid=multi_param_grid,\n",
    "    scoring='f1_macro',  # ë‹¤ì¤‘ ë¶„ë¥˜ì—ì„œëŠ” macro í‰ê·  ì‚¬ìš©\n",
    "    cv=cv,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ” S1 ë‹¤ì¤‘ ë¶„ë¥˜ ëª¨ë¸ ìµœì í™” ì¤‘ (ê²°í•©ëœ íŠ¹ì„± + SMOTE ì‚¬ìš©)...\")\n",
    "grid_search_multi.fit(X_combined_smote, y_multi_smote)\n",
    "\n",
    "print(f\"âœ… ìµœì í™” ì™„ë£Œ!\")\n",
    "print(f\"   - ìµœì  íŒŒë¼ë¯¸í„°: {grid_search_multi.best_params_}\")\n",
    "print(f\"   - ìµœê³  F1-Macro ì ìˆ˜: {grid_search_multi.best_score_:.4f}\")\n",
    "\n",
    "# ìµœì  ëª¨ë¸ë¡œ ì˜ˆì¸¡\n",
    "best_multi_model = grid_search_multi.best_estimator_\n",
    "multiclass_pred_combined = best_multi_model.predict(test_X_combined)\n",
    "\n",
    "print(f\"   - í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ì™„ë£Œ: {len(multiclass_pred_combined)}ê°œ ìƒ˜í”Œ\")\n",
    "print(f\"   - ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ ë¶„í¬: {dict(pd.Series(multiclass_pred_combined).value_counts().sort_index())}\")\n",
    "\n",
    "# ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ ì„±ëŠ¥ í‰ê°€ (SMOTE ì ìš©ëœ ë°ì´í„° ì‚¬ìš©)\n",
    "print(f\"\\nğŸ“Š êµì°¨ ê²€ì¦ ì„±ëŠ¥ í‰ê°€ ì¤‘...\")\n",
    "X_train_comb, X_val_comb, y_train, y_val = train_test_split(\n",
    "    X_combined_smote, y_multi_smote, test_size=0.2, random_state=42, stratify=y_multi_smote\n",
    ")\n",
    "\n",
    "print(f\"   - í›ˆë ¨ ì„¸íŠ¸ (SMOTE): {X_train_comb.shape[0]}ê°œ ìƒ˜í”Œ\")\n",
    "print(f\"   - ê²€ì¦ ì„¸íŠ¸ (SMOTE): {X_val_comb.shape[0]}ê°œ ìƒ˜í”Œ\")\n",
    "\n",
    "# ìµœì  íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ì¦ìš© ëª¨ë¸ í•™ìŠµ\n",
    "eval_model_comb = LGBMClassifier(**grid_search_multi.best_params_)\n",
    "eval_model_comb.fit(X_train_comb, y_train)\n",
    "\n",
    "# ì˜ˆì¸¡ ë° í‰ê°€\n",
    "y_pred_comb = eval_model_comb.predict(X_val_comb)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ğŸ“ˆ ì˜¤í† ì¸ì½”ë” + LightGBM ê²°í•© ëª¨ë¸ í‰ê°€ (S1 + SMOTE)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(\"\\nğŸ¯ ë¶„ë¥˜ ë³´ê³ ì„œ:\")\n",
    "print(classification_report(y_val, y_pred_comb))\n",
    "\n",
    "print(\"\\nğŸ” í˜¼ë™ í–‰ë ¬:\")\n",
    "print(confusion_matrix(y_val, y_pred_comb))\n",
    "\n",
    "# í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„\n",
    "print(f\"\\nğŸ“Š í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ì •í™•ë„:\")\n",
    "unique_classes = sorted(y_multi.unique())\n",
    "for class_label in unique_classes:\n",
    "    class_mask = (y_val == class_label)\n",
    "    if class_mask.sum() > 0:\n",
    "        class_accuracy = (y_pred_comb[class_mask] == class_label).mean()\n",
    "        print(f\"   - í´ë˜ìŠ¤ {class_label}: {class_accuracy:.4f} ({class_mask.sum()}ê°œ ìƒ˜í”Œ)\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ğŸ‰ ê²°í•©ëœ íŠ¹ì„± + SMOTE ë‹¤ì¤‘ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95750df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ ì„±ëŠ¥ ë¹„êµ ë¶„ì„ ------------------\n",
    "\n",
    "# ì›ë³¸ íŠ¹ì„±ë§Œ ì‚¬ìš©í•œ ëª¨ë¸ê³¼ ê²°í•© íŠ¹ì„± ëª¨ë¸ ë¹„êµ\n",
    "print(\"\\n===== ì›ë³¸ íŠ¹ì„± vs ê²°í•© íŠ¹ì„± ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ =====\")\n",
    "\n",
    "# ì›ë³¸ íŠ¹ì„± ëª¨ë¸ ì¬í•™ìŠµ (ë¹„êµìš©)\n",
    "X_train_orig, X_val_orig, y_train, y_val = train_test_split(\n",
    "    X_selected, y_multi, test_size=0.2, random_state=42, stratify=y_multi\n",
    ")\n",
    "\n",
    "eval_model_orig = LGBMClassifier(**grid_search_multi.best_params_)\n",
    "eval_model_orig.fit(X_train_orig, y_train)\n",
    "y_pred_orig = eval_model_orig.predict(X_val_orig)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "# ì •í™•ë„ ë¹„êµ\n",
    "acc_orig = accuracy_score(y_val, y_pred_orig)\n",
    "acc_comb = accuracy_score(y_val, y_pred_comb)\n",
    "print(f\"ì •í™•ë„ - ì›ë³¸ íŠ¹ì„±: {acc_orig:.4f}, ê²°í•© íŠ¹ì„±: {acc_comb:.4f}, í–¥ìƒ: {(acc_comb-acc_orig)*100:.2f}%\")\n",
    "\n",
    "# F1 ì ìˆ˜ ë¹„êµ (ë‹¤ì¤‘ ë¶„ë¥˜ëŠ” macro í‰ê· )\n",
    "f1_orig = f1_score(y_val, y_pred_orig, average='macro')\n",
    "f1_comb = f1_score(y_val, y_pred_comb, average='macro')\n",
    "print(f\"F1 ì ìˆ˜ - ì›ë³¸ íŠ¹ì„±: {f1_orig:.4f}, ê²°í•© íŠ¹ì„±: {f1_comb:.4f}, í–¥ìƒ: {(f1_comb-f1_orig)*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa052704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ ì„±ëŠ¥ í‰ê°€\n",
    "X_train_sel, X_val_sel, y_train, y_val = train_test_split(\n",
    "    X_selected, y_multi, test_size=0.2, random_state=42, stratify=y_multi\n",
    ")\n",
    "\n",
    "# ìµœì  íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ì¦ìš© ëª¨ë¸ í•™ìŠµ\n",
    "eval_model_sel = LGBMClassifier(**grid_search_multi.best_params_)\n",
    "eval_model_sel.fit(X_train_sel, y_train)\n",
    "\n",
    "# ì˜ˆì¸¡ ë° í‰ê°€\n",
    "y_pred_sel = eval_model_sel.predict(X_val_sel)\n",
    "\n",
    "print(\"\\n===== LightGBM ìµœì í™” ëª¨ë¸ í‰ê°€ (S1) =====\")\n",
    "print(classification_report(y_val, y_pred_sel))\n",
    "print(\"\\ní˜¼ë™ í–‰ë ¬:\")\n",
    "print(confusion_matrix(y_val, y_pred_sel))\n",
    "\n",
    "# # ì„±ëŠ¥ í‰ê°€ (ì›ë³¸ ëª¨ë¸ê³¼ ë¹„êµ)\n",
    "# # ê²€ì¦ ë°ì´í„° ë¶„í• \n",
    "# X_train_sel, X_val_sel, y_train, y_val = train_test_split(\n",
    "#     X_selected, y_multi, test_size=0.2, random_state=42, stratify=y_multi\n",
    "# )\n",
    "\n",
    "# # í‰ê°€ìš© ëª¨ë¸ í•™ìŠµ\n",
    "# eval_model_sel = LGBMClassifier(**multiclass_params)\n",
    "# eval_model_sel.fit(X_train_sel, y_train)\n",
    "\n",
    "# # ì˜ˆì¸¡ ë° í‰ê°€\n",
    "# y_pred_sel = eval_model_sel.predict(X_val_sel)\n",
    "\n",
    "# print(\"\\n===== LightGBM ê·¸ë£¹ ê¸°ë°˜ íŠ¹ì„± ì„ íƒ ëª¨ë¸ í‰ê°€ (S1) =====\")\n",
    "# print(classification_report(y_val, y_pred_sel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be857a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample ê¸°ë°˜ ì œì¶œ í¬ë§· ê°€ì ¸ì˜¤ê¸°\n",
    "submission_final = sample_submission[['subject_id', 'sleep_date', 'lifelog_date']].copy()\n",
    "\n",
    "# lifelog_date ê¸°ì¤€ìœ¼ë¡œ string â†’ date í˜•ì‹ í†µì¼\n",
    "submission_final['lifelog_date'] = pd.to_datetime(submission_final['lifelog_date']).dt.date\n",
    "\n",
    "# ID ë§Œë“¤ê¸° (submissionì—ì„œ ì˜ˆì¸¡í•œ ê²°ê³¼ì™€ ì—°ê²°í•˜ê¸° ìœ„í•´)\n",
    "submission_final['ID'] = submission_final['subject_id'] + '_' + submission_final['lifelog_date'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f142a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ì˜ˆì¸¡ ê²°ê³¼ ì—°ê²°í•  ìˆ˜ ìˆë„ë¡ ë™ì¼í•œ ìˆœì„œë¡œ ì •ë ¬\n",
    "# ë³´í†µ ì˜ˆì¸¡ ê²°ê³¼ëŠ” test_df ê¸°ì¤€ì´ë¯€ë¡œ ì •ë ¬ ë³´ì¥ë˜ì–´ì•¼ í•¨\n",
    "assert len(submission_final) == len(multiclass_pred_combined) # shape ì²´í¬\n",
    "\n",
    "# ë‹¤ì¤‘ ë¶„ë¥˜ ì˜ˆì¸¡ ë¶™ì´ê¸°\n",
    "submission_final['S1'] = multiclass_pred_combined\n",
    "\n",
    "# ì´ì§„ ë¶„ë¥˜ ê²°ê³¼ ë¶™ì´ê¸°\n",
    "for col in ['Q1', 'Q2', 'Q3', 'S2', 'S3']:\n",
    "    submission_final[col] = binary_preds_combined[col].astype(int) # í™•ë¥  ì•„ë‹Œ class ì˜ˆì¸¡\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ac68e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ìµœì¢… ì œì¶œ í˜•ì‹ ì •ë ¬\n",
    "submission_final = submission_final[['subject_id', 'sleep_date', 'lifelog_date', 'Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']]\n",
    "\n",
    "# ì €ì¥\n",
    "submission_final.to_csv(submission_folder + submission_file, index=False)\n",
    "\n",
    "# VSCodeì—ì„œëŠ” files.download()ê°€ ì‘ë™í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ëŒ€ì²´\n",
    "\n",
    "print(f\"âœ… ì œì¶œ íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {os.path.abspath(submission_file)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba047669",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
