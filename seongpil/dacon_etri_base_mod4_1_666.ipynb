{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91b1009",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# ê°„ë‹¨í•˜ê³  ì•ˆì „í•œ ê³ ê¸‰ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
    "# ê¸°ì¡´ êµ¬ì¡° + ìƒˆë¡œìš´ íŠ¹ì„± (ì—ëŸ¬ ë°©ì§€)\n",
    "# =============================================================================\n",
    "\n",
    "from datetime import datetime\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import glob \n",
    "import random \n",
    "import os \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import ast \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks\n",
    "import torch\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd437db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ê¸°ì¡´ ê¸°ë³¸ ì„¤ì • ìœ ì§€\n",
    "# =============================================================================\n",
    "\n",
    "# í˜„ì¬ ë‚ ì§œ ë° ì‹œê°„ ê°€ì ¸ì˜¤ê¸°\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"%m%d_%H%M\")  # ì˜ˆ: 0517_1530\n",
    "\n",
    "submission_folder = '/users/KTL/Desktop/dacon/submission/'\n",
    "submission_file = f'submission_new_advanced_{timestamp}.csv'\n",
    "\n",
    "# seed ê³ ì • \n",
    "SD = 42 \n",
    "random.seed(SD) \n",
    "np.random.seed(SD) \n",
    "os.environ['PYTHONHASHSEED'] = str(SD)\n",
    "tf.random.set_seed(SD)  # TensorFlow ì‹œë“œ ì„¤ì •\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ ì„¤ì • - VSCode ìƒëŒ€ê²½ë¡œë¡œ ë³€ê²½\n",
    "base_folder =  '/users/KTL/Desktop/ETRI_lifelog_dataset'\n",
    "folder = '/ch2025_data_items'\n",
    "data_dir = base_folder + folder \n",
    "\n",
    "# Parquet íŒŒì¼ ì „ì²´ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ \n",
    "parquet_files = glob.glob(os.path.join(data_dir, 'ch2025_*.parquet')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4d7b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ê¸°ì¡´ ë°ì´í„° ë¡œë”© ë°©ì‹ ìœ ì§€\n",
    "# =============================================================================\n",
    "\n",
    "# íŒŒì¼ ì´ë¦„ì„ í‚¤ë¡œ, DataFrameì„ ê°’ìœ¼ë¡œ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ \n",
    "lifelog_data = {} \n",
    "\n",
    "# íŒŒì¼ë³„ë¡œ ì½ê¸° \n",
    "for file_path in parquet_files: \n",
    "    name = os.path.basename(file_path).replace('.parquet', '').replace('ch2025_', '') \n",
    "    lifelog_data[name] = pd.read_parquet(file_path) \n",
    "    print(f\"âœ… Loaded: {name}, shape = {lifelog_data[name].shape}\") \n",
    "\n",
    "# ë”•ì…”ë„ˆë¦¬ì— ìˆëŠ” ëª¨ë“  í•­ëª©ì„ ë…ë¦½ì ì¸ ë³€ìˆ˜ë¡œ í• ë‹¹ \n",
    "for key, df in lifelog_data.items(): \n",
    "    globals()[f\"{key}_df\"] = df \n",
    "\n",
    "# ë©”íŠ¸ë¦­ìŠ¤ íŒŒì¼ ì½ê¸°\n",
    "metrics_train = pd.read_csv(base_folder + '/ch2025_metrics_train.csv')\n",
    "sample_submission = pd.read_csv(base_folder+'/ch2025_submission_sample.csv')\n",
    "\n",
    "# âœ… ê¸°ì¤€ ìŒ (subject_id, lifelog_date) \n",
    "sample_submission['lifelog_date'] = pd.to_datetime(sample_submission['lifelog_date']) \n",
    "test_keys = set(zip(sample_submission['subject_id'], sample_submission['lifelog_date'].dt.date)) \n",
    "\n",
    "# âœ… DataFrame ë³„ timestamp ì»¬ëŸ¼ ìˆ˜ë™ ì§€ì • \n",
    "dataframes = { \n",
    "    'mACStatus': (mACStatus_df, 'timestamp'), \n",
    "    'mActivity': (mActivity_df, 'timestamp'), \n",
    "    'mAmbience': (mAmbience_df, 'timestamp'), \n",
    "    'mBle': (mBle_df, 'timestamp'), \n",
    "    'mGps': (mGps_df, 'timestamp'), \n",
    "    'mLight': (mLight_df, 'timestamp'), \n",
    "    'mScreenStatus': (mScreenStatus_df, 'timestamp'), \n",
    "    'mUsageStats': (mUsageStats_df, 'timestamp'), \n",
    "    'mWifi': (mWifi_df, 'timestamp'), \n",
    "    'wHr': (wHr_df, 'timestamp'), \n",
    "    'wLight': (wLight_df, 'timestamp'), \n",
    "    'wPedo': (wPedo_df, 'timestamp'), \n",
    "} \n",
    "\n",
    "# âœ… ê¸°ì¡´ ë¶„ë¦¬ í•¨ìˆ˜ ìœ ì§€\n",
    "def split_test_train(df, subject_col='subject_id', timestamp_col='timestamp'): \n",
    "    df[timestamp_col] = pd.to_datetime(df[timestamp_col], errors='coerce') \n",
    "    df = df.dropna(subset=[timestamp_col]) \n",
    "    df['date_only'] = df[timestamp_col].dt.date \n",
    "    df['key'] = list(zip(df[subject_col], df['date_only'])) \n",
    "    test_df = df[df['key'].isin(test_keys)].drop(columns=['date_only', 'key']) \n",
    "    train_df = df[~df['key'].isin(test_keys)].drop(columns=['date_only', 'key']) \n",
    "    return test_df, train_df \n",
    "\n",
    "# âœ… ê¸°ì¡´ ë¶„í•  ê²°ê³¼ ì €ì¥ \n",
    "for name, (df, ts_col) in dataframes.items(): \n",
    "    print(f\"â³ {name} ë¶„ë¦¬ ì¤‘...\") \n",
    "    test_df, train_df = split_test_train(df.copy(), subject_col='subject_id', timestamp_col=ts_col) \n",
    "    globals()[f\"{name}_test\"] = test_df \n",
    "    globals()[f\"{name}_train\"] = train_df \n",
    "    print(f\"âœ… {name}_test â†’ {test_df.shape}, {name}_train â†’ {train_df.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fbe046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸš€ ì•ˆì „í•œ ê³ ê¸‰ ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ë°©ì‹ ê°œì„ )\n",
    "# =============================================================================\n",
    "\n",
    "def safe_process_mACStatus(df):\n",
    "    \"\"\"ì¶©ì „ íŒ¨í„´ ì•ˆì „í•œ ê³ ê¸‰ ë¶„ì„\"\"\"\n",
    "    print(\"ğŸ”‹ ì¶©ì „ íŒ¨í„´ ì•ˆì „ ë¶„ì„ ì¤‘...\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['date'] = df['timestamp'].dt.date\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df = df.sort_values(['subject_id', 'timestamp'])\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']):\n",
    "        status = group['m_charging'].values\n",
    "        hours = group['hour'].values\n",
    "        \n",
    "        # ê¸°ë³¸ í†µê³„\n",
    "        ratio_charging = status.mean()\n",
    "        transitions = (status[1:] != status[:-1]).sum() if len(status) > 1 else 0\n",
    "        \n",
    "        # ì—°ì† ì¶©ì „ ê¸¸ì´\n",
    "        charging_sessions = []\n",
    "        current_session = 0\n",
    "        for s in status:\n",
    "            if s == 1:\n",
    "                current_session += 1\n",
    "            else:\n",
    "                if current_session > 0:\n",
    "                    charging_sessions.append(current_session)\n",
    "                    current_session = 0\n",
    "        if current_session > 0:\n",
    "            charging_sessions.append(current_session)\n",
    "        \n",
    "        # ì‹œê°„ëŒ€ë³„ ì¶©ì „ íŒ¨í„´\n",
    "        charging_hours = hours[status == 1]\n",
    "        \n",
    "        # ìƒˆë¡œìš´ ê³ ê¸‰ íŠ¹ì„±ë“¤\n",
    "        features = {\n",
    "            'subject_id': subj,\n",
    "            'date': date,\n",
    "            # ê¸°ë³¸ íŠ¹ì„±\n",
    "            'charging_ratio': ratio_charging,\n",
    "            'charging_transitions': transitions,\n",
    "            'avg_charging_duration': np.mean(charging_sessions) if charging_sessions else 0,\n",
    "            'max_charging_duration': np.max(charging_sessions) if charging_sessions else 0,\n",
    "            \n",
    "            # ğŸ†• ê³ ê¸‰ íŠ¹ì„±\n",
    "            'charging_sessions_count': len(charging_sessions),\n",
    "            'charging_regularity': 1 / (np.std(charging_sessions) + 1) if charging_sessions else 0,\n",
    "            'preferred_charging_hour': stats.mode(charging_hours)[0][0] if len(charging_hours) > 0 else 12,\n",
    "            'charging_hour_diversity': len(np.unique(charging_hours)) if len(charging_hours) > 0 else 0,\n",
    "            'night_charging_ratio': (charging_hours < 6).sum() / len(charging_hours) if len(charging_hours) > 0 else 0,\n",
    "            'day_charging_ratio': ((charging_hours >= 6) & (charging_hours <= 18)).sum() / len(charging_hours) if len(charging_hours) > 0 else 0,\n",
    "            'evening_charging_ratio': (charging_hours > 18).sum() / len(charging_hours) if len(charging_hours) > 0 else 0,\n",
    "        }\n",
    "        \n",
    "        results.append(features)\n",
    "    \n",
    "    result_df = pd.DataFrame(results)\n",
    "    print(f\"   âœ… ì¶©ì „ íŠ¹ì„± ìƒì„±: {result_df.shape}\")\n",
    "    return result_df\n",
    "\n",
    "def safe_process_mActivity(df):\n",
    "    \"\"\"í™œë™ íŒ¨í„´ ì•ˆì „í•œ ê³ ê¸‰ ë¶„ì„\"\"\"\n",
    "    print(\"ğŸƒ í™œë™ íŒ¨í„´ ì•ˆì „ ë¶„ì„ ì¤‘...\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['date'] = df['timestamp'].dt.date\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df = df.sort_values(['subject_id', 'timestamp'])\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']):\n",
    "        activities = group['m_activity'].values\n",
    "        hours = group['hour'].values\n",
    "        \n",
    "        if len(activities) == 0:\n",
    "            continue\n",
    "        \n",
    "        # ê¸°ë³¸ í†µê³„\n",
    "        activity_counts = pd.Series(activities).value_counts()\n",
    "        dominant_activity = activity_counts.index[0] if len(activity_counts) > 0 else 0\n",
    "        \n",
    "        # ì „ì´ ë¶„ì„\n",
    "        transitions = (activities[1:] != activities[:-1]).sum() if len(activities) > 1 else 0\n",
    "        \n",
    "        # N-gram íŒ¨í„´ (ê°„ë‹¨í•œ 2-gram)\n",
    "        bigrams = []\n",
    "        if len(activities) >= 2:\n",
    "            bigrams = [(activities[i], activities[i+1]) for i in range(len(activities)-1)]\n",
    "            bigram_counts = pd.Series(bigrams).value_counts()\n",
    "            top_bigram_freq = bigram_counts.iloc[0] / len(bigrams) if len(bigram_counts) > 0 else 0\n",
    "        else:\n",
    "            top_bigram_freq = 0\n",
    "        \n",
    "        # ì‹œê°„ëŒ€ë³„ ì£¼ìš” í™œë™\n",
    "        morning_activities = activities[(hours >= 6) & (hours < 12)]\n",
    "        afternoon_activities = activities[(hours >= 12) & (hours < 18)]\n",
    "        evening_activities = activities[(hours >= 18) & (hours < 24)]\n",
    "        \n",
    "        features = {\n",
    "            'subject_id': subj,\n",
    "            'date': date,\n",
    "            # ê¸°ë³¸ íŠ¹ì„±\n",
    "            'activity_diversity': len(activity_counts),\n",
    "            'dominant_activity': dominant_activity,\n",
    "            'activity_entropy': stats.entropy(activity_counts.values + 1e-10),\n",
    "            \n",
    "            # ğŸ†• ê³ ê¸‰ íŠ¹ì„±\n",
    "            'activity_transitions': transitions,\n",
    "            'activity_transition_rate': transitions / len(activities) if len(activities) > 0 else 0,\n",
    "            'activity_stability': (activities == dominant_activity).mean(),\n",
    "            'top_bigram_frequency': top_bigram_freq,\n",
    "            \n",
    "            # ì‹œê°„ëŒ€ë³„ íŒ¨í„´\n",
    "            'morning_main_activity': stats.mode(morning_activities)[0][0] if len(morning_activities) > 0 else -1,\n",
    "            'afternoon_main_activity': stats.mode(afternoon_activities)[0][0] if len(afternoon_activities) > 0 else -1,\n",
    "            'evening_main_activity': stats.mode(evening_activities)[0][0] if len(evening_activities) > 0 else -1,\n",
    "            'morning_activity_diversity': len(np.unique(morning_activities)) if len(morning_activities) > 0 else 0,\n",
    "            'afternoon_activity_diversity': len(np.unique(afternoon_activities)) if len(afternoon_activities) > 0 else 0,\n",
    "            'evening_activity_diversity': len(np.unique(evening_activities)) if len(evening_activities) > 0 else 0,\n",
    "        }\n",
    "        \n",
    "        results.append(features)\n",
    "    \n",
    "    result_df = pd.DataFrame(results)\n",
    "    print(f\"   âœ… í™œë™ íŠ¹ì„± ìƒì„±: {result_df.shape}\")\n",
    "    return result_df\n",
    "\n",
    "def safe_process_mLight(df):\n",
    "    \"\"\"ì¡°ë„ ì„¼ì„œ ì•ˆì „í•œ ê³ ê¸‰ ë¶„ì„\"\"\"\n",
    "    print(\"ğŸ’¡ ì¡°ë„ íŒ¨í„´ ì•ˆì „ ë¶„ì„ ì¤‘...\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['date'] = df['timestamp'].dt.date\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']):\n",
    "        light_values = group['m_light'].dropna().values\n",
    "        hours = group['hour'].values\n",
    "        \n",
    "        if len(light_values) < 3:\n",
    "            continue\n",
    "        \n",
    "        # ê¸°ë³¸ í†µê³„\n",
    "        light_mean = np.mean(light_values)\n",
    "        light_std = np.std(light_values)\n",
    "        light_range = np.max(light_values) - np.min(light_values)\n",
    "        \n",
    "        # ì‹œê°„ëŒ€ë³„ ë¶„ì„\n",
    "        morning_light = group[(group['hour'] >= 6) & (group['hour'] < 12)]['m_light'].mean()\n",
    "        afternoon_light = group[(group['hour'] >= 12) & (group['hour'] < 18)]['m_light'].mean()\n",
    "        evening_light = group[(group['hour'] >= 18) & (group['hour'] < 24)]['m_light'].mean()\n",
    "        night_light = group[(group['hour'] >= 0) & (group['hour'] < 6)]['m_light'].mean()\n",
    "        \n",
    "        # í”¼í¬ ë¶„ì„ (ì•ˆì „í•œ ë°©ì‹)\n",
    "        try:\n",
    "            if len(light_values) > 5:\n",
    "                peaks, _ = find_peaks(light_values, height=np.percentile(light_values, 70))\n",
    "                valleys, _ = find_peaks(-light_values, height=-np.percentile(light_values, 30))\n",
    "                peak_count = len(peaks)\n",
    "                valley_count = len(valleys)\n",
    "            else:\n",
    "                peak_count = 0\n",
    "                valley_count = 0\n",
    "        except:\n",
    "            peak_count = 0\n",
    "            valley_count = 0\n",
    "        \n",
    "        features = {\n",
    "            'subject_id': subj,\n",
    "            'date': date,\n",
    "            # ê¸°ë³¸ íŠ¹ì„±\n",
    "            'light_mean': light_mean,\n",
    "            'light_std': light_std,\n",
    "            'light_min': np.min(light_values),\n",
    "            'light_max': np.max(light_values),\n",
    "            'light_range': light_range,\n",
    "            \n",
    "            # ğŸ†• ê³ ê¸‰ íŠ¹ì„±\n",
    "            'light_cv': light_std / (light_mean + 1e-10),\n",
    "            'light_peaks_count': peak_count,\n",
    "            'light_valleys_count': valley_count,\n",
    "            'light_variability': np.var(light_values),\n",
    "            \n",
    "            # ì‹œê°„ëŒ€ë³„ íŒ¨í„´\n",
    "            'morning_light': morning_light if not np.isnan(morning_light) else 0,\n",
    "            'afternoon_light': afternoon_light if not np.isnan(afternoon_light) else 0,\n",
    "            'evening_light': evening_light if not np.isnan(evening_light) else 0,\n",
    "            'night_light': night_light if not np.isnan(night_light) else 0,\n",
    "            \n",
    "            # ì¼ì£¼ê¸° íŠ¹ì„±\n",
    "            'circadian_amplitude': (afternoon_light - night_light) if not (np.isnan(afternoon_light) or np.isnan(night_light)) else 0,\n",
    "            'dark_ratio': (light_values < np.percentile(light_values, 25)).mean(),\n",
    "            'bright_ratio': (light_values > np.percentile(light_values, 75)).mean(),\n",
    "        }\n",
    "        \n",
    "        results.append(features)\n",
    "    \n",
    "    result_df = pd.DataFrame(results)\n",
    "    print(f\"   âœ… ì¡°ë„ íŠ¹ì„± ìƒì„±: {result_df.shape}\")\n",
    "    return result_df\n",
    "\n",
    "def safe_process_wHr(df):\n",
    "    \"\"\"ì‹¬ë°•ìˆ˜ ì•ˆì „í•œ ê³ ê¸‰ ë¶„ì„ (HRV í¬í•¨)\"\"\"\n",
    "    print(\"ğŸ’“ ì‹¬ë°•ìˆ˜ ì•ˆì „ ë¶„ì„ ì¤‘...\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['date'] = df['timestamp'].dt.date\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']):\n",
    "        all_hr = []\n",
    "        \n",
    "        # ì‹¬ë°•ìˆ˜ ë°ì´í„° íŒŒì‹± (ì•ˆì „í•œ ë°©ì‹)\n",
    "        for hr_data in group['heart_rate']:\n",
    "            try:\n",
    "                if isinstance(hr_data, str):\n",
    "                    parsed = ast.literal_eval(hr_data)\n",
    "                    hr_values = [int(hr) for hr in parsed if hr is not None and 40 <= int(hr) <= 200]\n",
    "                    all_hr.extend(hr_values)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if len(all_hr) < 5:\n",
    "            continue\n",
    "        \n",
    "        hr_array = np.array(all_hr)\n",
    "        \n",
    "        # ê¸°ë³¸ ì‹¬ë°•ìˆ˜ í†µê³„\n",
    "        hr_mean = np.mean(hr_array)\n",
    "        hr_std = np.std(hr_array)\n",
    "        hr_range = np.max(hr_array) - np.min(hr_array)\n",
    "        \n",
    "        # HRV ì§€í‘œ (ì•ˆì „í•œ ë°©ì‹)\n",
    "        try:\n",
    "            rr_intervals = 60000 / hr_array  # RR intervals in ms\n",
    "            rr_diff = np.diff(rr_intervals)\n",
    "            \n",
    "            rmssd = np.sqrt(np.mean(rr_diff ** 2))  # RMSSD\n",
    "            pnn50 = np.sum(np.abs(rr_diff) > 50) / len(rr_diff) * 100 if len(rr_diff) > 0 else 0  # pNN50\n",
    "        except:\n",
    "            rmssd = 0\n",
    "            pnn50 = 0\n",
    "        \n",
    "        features = {\n",
    "            'subject_id': subj,\n",
    "            'date': date,\n",
    "            # ê¸°ë³¸ íŠ¹ì„±\n",
    "            'hr_mean': hr_mean,\n",
    "            'hr_std': hr_std,\n",
    "            'hr_min': np.min(hr_array),\n",
    "            'hr_max': np.max(hr_array),\n",
    "            'hr_range': hr_range,\n",
    "            \n",
    "            # ğŸ†• HRV íŠ¹ì„±\n",
    "            'hr_rmssd': rmssd,\n",
    "            'hr_pnn50': pnn50,\n",
    "            'hr_cv': hr_std / hr_mean if hr_mean > 0 else 0,\n",
    "            \n",
    "            # ìƒë¦¬í•™ì  ì§€í‘œ\n",
    "            'hr_stress_load': (hr_array > 100).mean(),\n",
    "            'hr_recovery_ratio': (hr_array < 70).mean(),\n",
    "            'hr_variability_index': hr_std / hr_mean if hr_mean > 0 else 0,\n",
    "            \n",
    "            # ë¶„í¬ íŠ¹ì„±\n",
    "            'hr_median': np.median(hr_array),\n",
    "            'hr_q75': np.percentile(hr_array, 75),\n",
    "            'hr_q25': np.percentile(hr_array, 25),\n",
    "            'hr_iqr': np.percentile(hr_array, 75) - np.percentile(hr_array, 25),\n",
    "        }\n",
    "        \n",
    "        results.append(features)\n",
    "    \n",
    "    result_df = pd.DataFrame(results)\n",
    "    print(f\"   âœ… ì‹¬ë°•ìˆ˜ íŠ¹ì„± ìƒì„±: {result_df.shape}\")\n",
    "    return result_df\n",
    "\n",
    "def safe_process_wPedo(df):\n",
    "    \"\"\"ë³´í–‰ íŒ¨í„´ ì•ˆì „í•œ ê³ ê¸‰ ë¶„ì„\"\"\"\n",
    "    print(\"ğŸ‘Ÿ ë³´í–‰ íŒ¨í„´ ì•ˆì „ ë¶„ì„ ì¤‘...\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['date'] = df['timestamp'].dt.date\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']):\n",
    "        steps = group['step'].values\n",
    "        speeds = group['speed'].dropna().values\n",
    "        distances = group['distance'].values\n",
    "        hours = group['hour'].values\n",
    "        \n",
    "        if len(steps) == 0:\n",
    "            continue\n",
    "        \n",
    "        # ê¸°ë³¸ í†µê³„\n",
    "        total_steps = np.sum(steps)\n",
    "        total_distance = np.sum(distances)\n",
    "        avg_speed = np.mean(speeds) if len(speeds) > 0 else 0\n",
    "        max_speed = np.max(speeds) if len(speeds) > 0 else 0\n",
    "        \n",
    "        # í™œë™ ì„¸ì…˜ ë¶„ì„\n",
    "        active_threshold = 10\n",
    "        active_periods = steps > active_threshold\n",
    "        \n",
    "        # ì—°ì† í™œë™ êµ¬ê°„\n",
    "        activity_sessions = []\n",
    "        current_session = 0\n",
    "        for is_active in active_periods:\n",
    "            if is_active:\n",
    "                current_session += 1\n",
    "            else:\n",
    "                if current_session > 0:\n",
    "                    activity_sessions.append(current_session)\n",
    "                    current_session = 0\n",
    "        if current_session > 0:\n",
    "            activity_sessions.append(current_session)\n",
    "        \n",
    "        # ì‹œê°„ëŒ€ë³„ í™œë™\n",
    "        morning_steps = steps[(hours >= 6) & (hours < 12)].sum()\n",
    "        afternoon_steps = steps[(hours >= 12) & (hours < 18)].sum()\n",
    "        evening_steps = steps[(hours >= 18) & (hours < 24)].sum()\n",
    "        \n",
    "        features = {\n",
    "            'subject_id': subj,\n",
    "            'date': date,\n",
    "            # ê¸°ë³¸ íŠ¹ì„±\n",
    "            'total_steps': total_steps,\n",
    "            'total_distance': total_distance,\n",
    "            'avg_speed': avg_speed,\n",
    "            'max_speed': max_speed,\n",
    "            'speed_variability': np.std(speeds) if len(speeds) > 0 else 0,\n",
    "            \n",
    "            # ğŸ†• ê³ ê¸‰ íŠ¹ì„±\n",
    "            'active_sessions_count': len(activity_sessions),\n",
    "            'avg_session_length': np.mean(activity_sessions) if activity_sessions else 0,\n",
    "            'max_session_length': np.max(activity_sessions) if activity_sessions else 0,\n",
    "            'activity_fragmentation': len(activity_sessions) / (np.sum(activity_sessions) + 1),\n",
    "            'step_regularity': 1 / (np.std(steps) + 1),\n",
    "            'step_consistency': (steps > 0).mean(),\n",
    "            \n",
    "            # ì‹œê°„ëŒ€ë³„ íŒ¨í„´\n",
    "            'morning_steps': morning_steps,\n",
    "            'afternoon_steps': afternoon_steps,\n",
    "            'evening_steps': evening_steps,\n",
    "            'morning_steps_ratio': morning_steps / (total_steps + 1),\n",
    "            'afternoon_steps_ratio': afternoon_steps / (total_steps + 1),\n",
    "            'evening_steps_ratio': evening_steps / (total_steps + 1),\n",
    "            \n",
    "            # ê°•ë„ ë¶„ì„\n",
    "            'high_intensity_ratio': (steps > np.percentile(steps, 80)).mean() if len(steps) > 0 else 0,\n",
    "            'low_activity_ratio': (steps < np.percentile(steps, 20)).mean() if len(steps) > 0 else 0,\n",
    "        }\n",
    "        \n",
    "        results.append(features)\n",
    "    \n",
    "    result_df = pd.DataFrame(results)\n",
    "    print(f\"   âœ… ë³´í–‰ íŠ¹ì„± ìƒì„±: {result_df.shape}\")\n",
    "    return result_df\n",
    "\n",
    "def safe_process_mScreenStatus(df):\n",
    "    \"\"\"ìŠ¤í¬ë¦° ì‚¬ìš© íŒ¨í„´ ì•ˆì „í•œ ê³ ê¸‰ ë¶„ì„\"\"\"\n",
    "    print(\"ğŸ“± ìŠ¤í¬ë¦° íŒ¨í„´ ì•ˆì „ ë¶„ì„ ì¤‘...\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['date'] = df['timestamp'].dt.date\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']):\n",
    "        screen_states = group['m_screen_use'].values\n",
    "        hours = group['hour'].values\n",
    "        \n",
    "        if len(screen_states) == 0:\n",
    "            continue\n",
    "        \n",
    "        # ê¸°ë³¸ í†µê³„\n",
    "        usage_ratio = np.mean(screen_states)\n",
    "        transitions = (screen_states[1:] != screen_states[:-1]).sum() if len(screen_states) > 1 else 0\n",
    "        \n",
    "        # ì‚¬ìš© ì„¸ì…˜ ë¶„ì„\n",
    "        usage_sessions = []\n",
    "        current_session = 0\n",
    "        for state in screen_states:\n",
    "            if state == 1:\n",
    "                current_session += 1\n",
    "            else:\n",
    "                if current_session > 0:\n",
    "                    usage_sessions.append(current_session)\n",
    "                    current_session = 0\n",
    "        if current_session > 0:\n",
    "            usage_sessions.append(current_session)\n",
    "        \n",
    "        # ì‹œê°„ëŒ€ë³„ ì‚¬ìš© íŒ¨í„´\n",
    "        usage_hours = hours[screen_states == 1]\n",
    "        morning_usage = ((usage_hours >= 6) & (usage_hours < 12)).sum() / len(usage_hours) if len(usage_hours) > 0 else 0\n",
    "        afternoon_usage = ((usage_hours >= 12) & (usage_hours < 18)).sum() / len(usage_hours) if len(usage_hours) > 0 else 0\n",
    "        evening_usage = ((usage_hours >= 18) & (usage_hours < 24)).sum() / len(usage_hours) if len(usage_hours) > 0 else 0\n",
    "        night_usage = ((usage_hours >= 0) & (usage_hours < 6)).sum() / len(usage_hours) if len(usage_hours) > 0 else 0\n",
    "        \n",
    "        features = {\n",
    "            'subject_id': subj,\n",
    "            'date': date,\n",
    "            # ê¸°ë³¸ íŠ¹ì„±\n",
    "            'screen_usage_ratio': usage_ratio,\n",
    "            'screen_transitions': transitions,\n",
    "            \n",
    "            # ğŸ†• ê³ ê¸‰ íŠ¹ì„±\n",
    "            'screen_sessions_count': len(usage_sessions),\n",
    "            'avg_session_duration': np.mean(usage_sessions) if usage_sessions else 0,\n",
    "            'max_session_duration': np.max(usage_sessions) if usage_sessions else 0,\n",
    "            'session_duration_std': np.std(usage_sessions) if usage_sessions else 0,\n",
    "            'usage_intensity': np.sum(usage_sessions) / len(screen_states),\n",
    "            'usage_fragmentation': len(usage_sessions) / (np.sum(usage_sessions) + 1),\n",
    "            \n",
    "            # ì‹œê°„ëŒ€ë³„ íŒ¨í„´\n",
    "            'morning_usage_rate': morning_usage,\n",
    "            'afternoon_usage_rate': afternoon_usage,\n",
    "            'evening_usage_rate': evening_usage,\n",
    "            'night_usage_rate': night_usage,\n",
    "            \n",
    "            # ì‚¬ìš© íŒ¨í„´ íŠ¹ì„±\n",
    "            'peak_usage_hour': hours[np.argmax(screen_states)] if len(screen_states) > 0 else 12,\n",
    "            'usage_regularity': 1 / (transitions + 1),\n",
    "            'screen_addiction_score': usage_ratio * len(usage_sessions),  # ì‚¬ìš©ë¥  Ã— ì„¸ì…˜ ìˆ˜\n",
    "        }\n",
    "        \n",
    "        results.append(features)\n",
    "    \n",
    "    result_df = pd.DataFrame(results)\n",
    "    print(f\"   âœ… ìŠ¤í¬ë¦° íŠ¹ì„± ìƒì„±: {result_df.shape}\")\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b02056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ”§ ê°œì¸í™” íŠ¹ì„± ì¶”ì¶œê¸° (ì•ˆì „í•œ ë²„ì „)\n",
    "# =============================================================================\n",
    "\n",
    "class SafePersonalizedExtractor:\n",
    "    \"\"\"ì•ˆì „í•œ ê°œì¸í™” íŠ¹ì„± ì¶”ì¶œ\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.user_baselines = {}\n",
    "    \n",
    "    def compute_baselines(self, df, value_cols):\n",
    "        \"\"\"ì‚¬ìš©ìë³„ ê¸°ì¤€ì„  ì•ˆì „í•˜ê²Œ ê³„ì‚°\"\"\"\n",
    "        for subject_id, group in df.groupby('subject_id'):\n",
    "            baseline = {}\n",
    "            for col in value_cols:\n",
    "                if col in group.columns and group[col].notna().sum() > 2:  # ìµœì†Œ 3ê°œ ê°’ í•„ìš”\n",
    "                    try:\n",
    "                        baseline[f'{col}_personal_mean'] = group[col].mean()\n",
    "                        baseline[f'{col}_personal_std'] = group[col].std()\n",
    "                        baseline[f'{col}_personal_median'] = group[col].median()\n",
    "                    except:\n",
    "                        continue\n",
    "            \n",
    "            if baseline:  # ë¹ˆ dictê°€ ì•„ë‹ ë•Œë§Œ ì €ì¥\n",
    "                self.user_baselines[subject_id] = baseline\n",
    "    \n",
    "    def extract_personal_deviations(self, df, value_cols):\n",
    "        \"\"\"ê°œì¸ ê¸°ì¤€ í¸ì°¨ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        for col in value_cols:\n",
    "            if col not in df.columns:\n",
    "                continue\n",
    "            \n",
    "            # ê°œì¸ë³„ Z-score (ì•ˆì „í•œ ë°©ì‹)\n",
    "            personal_zscores = []\n",
    "            above_medians = []\n",
    "            \n",
    "            for _, row in df.iterrows():\n",
    "                subject_id = row['subject_id']\n",
    "                if subject_id in self.user_baselines:\n",
    "                    baseline = self.user_baselines[subject_id]\n",
    "                    \n",
    "                    # Z-score ê³„ì‚°\n",
    "                    if f'{col}_personal_mean' in baseline and f'{col}_personal_std' in baseline:\n",
    "                        mean_val = baseline[f'{col}_personal_mean']\n",
    "                        std_val = baseline[f'{col}_personal_std']\n",
    "                        if std_val > 1e-10:\n",
    "                            zscore = (row[col] - mean_val) / std_val\n",
    "                        else:\n",
    "                            zscore = 0\n",
    "                    else:\n",
    "                        zscore = 0\n",
    "                    \n",
    "                    # ì¤‘ìœ„ìˆ˜ ì´ìƒ/ì´í•˜\n",
    "                    if f'{col}_personal_median' in baseline:\n",
    "                        median_val = baseline[f'{col}_personal_median']\n",
    "                        above_median = 1 if row[col] > median_val else 0\n",
    "                    else:\n",
    "                        above_median = 0\n",
    "                else:\n",
    "                    zscore = 0\n",
    "                    above_median = 0\n",
    "                \n",
    "                personal_zscores.append(zscore)\n",
    "                above_medians.append(above_median)\n",
    "            \n",
    "            df[f'{col}_personal_zscore'] = personal_zscores\n",
    "            df[f'{col}_above_personal_median'] = above_medians\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c819e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸš€ ë©”ì¸ ì•ˆì „í•œ ì „ì²˜ë¦¬ ì‹¤í–‰\n",
    "# =============================================================================\n",
    "\n",
    "def run_safe_advanced_preprocessing():\n",
    "    \"\"\"ì•ˆì „í•œ ê³ ê¸‰ ì „ì²˜ë¦¬ ì‹¤í–‰\"\"\"\n",
    "    print(\"ğŸš€ ì•ˆì „í•œ ê³ ê¸‰ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘...\")\n",
    "    \n",
    "    processed_dfs = []\n",
    "    \n",
    "    # ê° ì„¼ì„œë³„ ì•ˆì „í•œ ê³ ê¸‰ ì „ì²˜ë¦¬ ì‹¤í–‰\n",
    "    sensor_processors = [\n",
    "        ('mACStatus_df', safe_process_mACStatus),\n",
    "        ('mActivity_df', safe_process_mActivity),\n",
    "        ('mLight_df', safe_process_mLight),\n",
    "        ('wHr_df', safe_process_wHr),\n",
    "        ('wPedo_df', safe_process_wPedo),\n",
    "        ('mScreenStatus_df', safe_process_mScreenStatus),\n",
    "    ]\n",
    "    \n",
    "    for sensor_name, processor_func in sensor_processors:\n",
    "        if sensor_name in globals() and len(globals()[sensor_name]) > 0:\n",
    "            try:\n",
    "                result_df = processor_func(globals()[sensor_name])\n",
    "                if len(result_df) > 0 and 'date' in result_df.columns:\n",
    "                    processed_dfs.append(result_df)\n",
    "                    print(f\"   âœ… {sensor_name} ì²˜ë¦¬ ì™„ë£Œ: {result_df.shape}\")\n",
    "                else:\n",
    "                    print(f\"   âš ï¸ {sensor_name} ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ\")\n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ {sensor_name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
    "        else:\n",
    "            print(f\"   âš ï¸ {sensor_name} ë°ì´í„° ì—†ìŒ\")\n",
    "    \n",
    "    # íŠ¹ì„± ë³‘í•©\n",
    "    print(\"ğŸ”— ëª¨ë“  ì„¼ì„œ íŠ¹ì„± ì•ˆì „í•˜ê²Œ ë³‘í•© ì¤‘...\")\n",
    "    if processed_dfs:\n",
    "        merged_df = processed_dfs[0].copy()\n",
    "        print(f\"   ğŸ“Š ì²« ë²ˆì§¸ DF: {merged_df.shape}\")\n",
    "        \n",
    "        for i, df in enumerate(processed_dfs[1:], 2):\n",
    "            print(f\"   ğŸ“Š {i}ë²ˆì§¸ DF ë³‘í•©: {df.shape}\")\n",
    "            merged_df = pd.merge(merged_df, df, on=['subject_id', 'date'], how='outer')\n",
    "            print(f\"      â†’ ë³‘í•© í›„: {merged_df.shape}\")\n",
    "    else:\n",
    "        print(\"   âŒ ì²˜ë¦¬ëœ íŠ¹ì„±ì´ ì—†ì–´ ë¹ˆ DataFrame ìƒì„±\")\n",
    "        merged_df = pd.DataFrame(columns=['subject_id', 'date'])\n",
    "        return pd.DataFrame(), pd.DataFrame(), merged_df\n",
    "    \n",
    "    # ê°œì¸í™” íŠ¹ì„± ì¶”ê°€\n",
    "    print(\"ğŸ‘¤ ê°œì¸í™” íŠ¹ì„± ì•ˆì „í•˜ê²Œ ì¶”ì¶œ ì¤‘...\")\n",
    "    if len(merged_df) > 5:  # ìµœì†Œ 5ê°œ í–‰ í•„ìš”\n",
    "        try:\n",
    "            personal_extractor = SafePersonalizedExtractor()\n",
    "            numeric_cols = merged_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            if 'subject_id' in numeric_cols:\n",
    "                numeric_cols.remove('subject_id')\n",
    "            \n",
    "            if len(numeric_cols) > 0:\n",
    "                personal_extractor.compute_baselines(merged_df, numeric_cols[:10])  # ìµœëŒ€ 10ê°œ ì»¬ëŸ¼ë§Œ\n",
    "                merged_df = personal_extractor.extract_personal_deviations(merged_df, numeric_cols[:10])\n",
    "                personal_features_count = len([col for col in merged_df.columns if 'personal' in col])\n",
    "                print(f\"   âœ… ê°œì¸í™” íŠ¹ì„± {personal_features_count}ê°œ ì¶”ê°€\")\n",
    "            else:\n",
    "                print(\"   âš ï¸ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì—†ì–´ ê°œì¸í™” íŠ¹ì„± ìƒëµ\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ ê°œì¸í™” íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    # Train/Test ë¶„í• \n",
    "    print(\"âœ‚ï¸ Train/Test ì•ˆì „í•˜ê²Œ ë¶„í•  ì¤‘...\")\n",
    "    try:\n",
    "        # date ì»¬ëŸ¼ í™•ì¸ ë° ë³€í™˜\n",
    "        if 'date' not in merged_df.columns:\n",
    "            print(\"   âŒ 'date' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return pd.DataFrame(), pd.DataFrame(), merged_df\n",
    "        \n",
    "        merged_df['date'] = pd.to_datetime(merged_df['date']).dt.date\n",
    "        metrics_train['lifelog_date'] = pd.to_datetime(metrics_train['lifelog_date']).dt.date\n",
    "        \n",
    "        # Train ë°ì´í„°\n",
    "        train_df = pd.merge(\n",
    "            metrics_train.rename(columns={'lifelog_date': 'date'}),\n",
    "            merged_df,\n",
    "            on=['subject_id', 'date'],\n",
    "            how='inner'\n",
    "        )\n",
    "        \n",
    "        # Test ë°ì´í„°\n",
    "        sample_submission['lifelog_date'] = pd.to_datetime(sample_submission['lifelog_date']).dt.date\n",
    "        test_df = pd.merge(\n",
    "            sample_submission[['subject_id', 'lifelog_date']].rename(columns={'lifelog_date': 'date'}),\n",
    "            merged_df,\n",
    "            on=['subject_id', 'date'],\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… ì•ˆì „í•œ ê³ ê¸‰ ì „ì²˜ë¦¬ ì™„ë£Œ!\")\n",
    "        print(f\"   - Train ë°ì´í„°: {train_df.shape}\")\n",
    "        print(f\"   - Test ë°ì´í„°: {test_df.shape}\")\n",
    "        print(f\"   - ì¶”ì¶œëœ íŠ¹ì„± ìˆ˜: {len([col for col in merged_df.columns if col not in ['subject_id', 'date']])}\")\n",
    "        \n",
    "        return train_df, test_df, merged_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Train/Test ë¶„í•  ì‹¤íŒ¨: {e}\")\n",
    "        return pd.DataFrame(), pd.DataFrame(), merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefc83d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ì‹¤í–‰\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_df_advanced, test_df_advanced, features_df_advanced = run_safe_advanced_preprocessing()\n",
    "    \n",
    "    print(\"\\nğŸ‰ ì•ˆì „í•œ ê³ ê¸‰ ì „ì²˜ë¦¬ ì™„ë£Œ!\")\n",
    "    print(f\"ğŸ“Š ê²°ê³¼ ìš”ì•½:\")\n",
    "    print(f\"   - Train: {train_df_advanced.shape}\")\n",
    "    print(f\"   - Test: {test_df_advanced.shape}\")\n",
    "    print(f\"   - Features: {features_df_advanced.shape}\")\n",
    "    \n",
    "    if len(features_df_advanced) > 0:\n",
    "        feature_examples = [col for col in features_df_advanced.columns \n",
    "                           if col not in ['subject_id', 'date']][:15]\n",
    "        \n",
    "        print(f\"\\nğŸ”¬ ìƒì„±ëœ ê³ ê¸‰ íŠ¹ì„± ì˜ˆì‹œ (ìƒìœ„ 15ê°œ):\")\n",
    "        for i, feature in enumerate(feature_examples, 1):\n",
    "            print(f\"   {i:2d}. {feature}\")\n",
    "        \n",
    "        # íŠ¹ì„± ì¹´í…Œê³ ë¦¬ë³„ ê°œìˆ˜\n",
    "        personal_features = len([col for col in features_df_advanced.columns if 'personal' in col])\n",
    "        session_features = len([col for col in features_df_advanced.columns if 'session' in col])\n",
    "        ratio_features = len([col for col in features_df_advanced.columns if 'ratio' in col])\n",
    "        advanced_features = len([col for col in features_df_advanced.columns if any(keyword in col for keyword in ['rmssd', 'pnn50', 'fragmentation', 'regularity'])])\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ íŠ¹ì„± ì¹´í…Œê³ ë¦¬ë³„ í†µê³„:\")\n",
    "        print(f\"   - ê°œì¸í™” íŠ¹ì„±: {personal_features}ê°œ\")\n",
    "        print(f\"   - ì„¸ì…˜ ê¸°ë°˜: {session_features}ê°œ\") \n",
    "        print(f\"   - ë¹„ìœ¨ íŠ¹ì„±: {ratio_features}ê°œ\")\n",
    "        print(f\"   - ê³ ê¸‰ ìƒì²´ì‹ í˜¸: {advanced_features}ê°œ\")\n",
    "        print(f\"   - ì´ íŠ¹ì„± ìˆ˜: {len(feature_examples)}ê°œ\")\n",
    "    \n",
    "    else:\n",
    "        print(\"   âš ï¸ íŠ¹ì„±ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a41b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ê³ ê¸‰ ì „ì²˜ë¦¬ ë°ì´í„° + ê¸°ì¡´ LightGBM ëª¨ë¸ë§ íŒŒì´í”„ë¼ì¸\n",
    "# =============================================================================\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "print(\"ğŸ¯ ê³ ê¸‰ ì „ì²˜ë¦¬ ë°ì´í„°ë¡œ ê¸°ì¡´ LightGBM ëª¨ë¸ë§ ì‹œì‘...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226da9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. ë°ì´í„° ì¤€ë¹„\n",
    "# =============================================================================\n",
    "\n",
    "# âœ… íƒ€ê²Ÿ ë¦¬ìŠ¤íŠ¸ \n",
    "targets_binary = ['Q1', 'Q2', 'Q3', 'S2', 'S3'] \n",
    "target_multiclass = 'S1' \n",
    "\n",
    "print(f\"ğŸ“Š ë°ì´í„° í˜„í™©:\")\n",
    "print(f\"   - Train ë°ì´í„°: {train_df_advanced.shape}\")\n",
    "print(f\"   - Test ë°ì´í„°: {test_df_advanced.shape}\")\n",
    "\n",
    "# âœ… feature ì¤€ë¹„ \n",
    "feature_cols = [col for col in train_df_advanced.columns \n",
    "                if col not in ['subject_id', 'sleep_date', 'date', 'Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']]\n",
    "\n",
    "X_advanced = train_df_advanced[feature_cols].copy()\n",
    "X_advanced.fillna(0, inplace=True)  # ê²°ì¸¡ê°’ ì²˜ë¦¬ \n",
    "\n",
    "test_X_advanced = test_df_advanced[feature_cols].copy()\n",
    "test_X_advanced.fillna(0, inplace=True) \n",
    "\n",
    "print(f\"ğŸ“ˆ íŠ¹ì„± ì •ë³´:\")\n",
    "print(f\"   - ì‚¬ìš© íŠ¹ì„± ìˆ˜: {len(feature_cols)}\")\n",
    "print(f\"   - Train ìƒ˜í”Œ: {X_advanced.shape[0]}\")\n",
    "print(f\"   - Test ìƒ˜í”Œ: {test_X_advanced.shape[0]}\")\n",
    "\n",
    "# ì»¬ëŸ¼ ì´ë¦„ì—ì„œ íŠ¹ìˆ˜ ë¬¸ì ì œê±°/ë³€í™˜ \n",
    "def sanitize_column_names(df): \n",
    "    df.columns = ( \n",
    "        df.columns \n",
    "        .str.replace(r\"[^\\w]\", \"_\", regex=True)  # íŠ¹ìˆ˜ë¬¸ì â†’ _ \n",
    "        .str.replace(r\"__+\", \"_\", regex=True)  # ì—°ì†ëœ _ ì œê±° \n",
    "        .str.strip(\"_\")  # ì•ë’¤ _ ì œê±° \n",
    "    ) \n",
    "    return df \n",
    "\n",
    "# ëª¨ë“  ì…ë ¥ì— ì ìš© \n",
    "X_advanced = sanitize_column_names(X_advanced) \n",
    "test_X_advanced = sanitize_column_names(test_X_advanced)\n",
    "\n",
    "print(f\"âœ… ì»¬ëŸ¼ëª… ì •ê·œí™” ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178d5c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. LightGBM ê¸°ë³¸ ì„¤ì •\n",
    "# =============================================================================\n",
    "\n",
    "# GridSearchë¥¼ ìœ„í•œ êµì°¨ ê²€ì¦ ì„¤ì •\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# LightGBM ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "lgbm_params = {\n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate': 0.03,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "# ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ê·¸ë¦¬ë“œ ì„œì¹˜ íŒŒë¼ë¯¸í„°\n",
    "binary_param_grid = {\n",
    "    'learning_rate': [0.01, 0.03, 0.1],\n",
    "    'n_estimators': [1000, 1500],\n",
    "    'num_leaves': [50, 100],\n",
    "    'max_depth': [-1, 5],\n",
    "    'min_child_samples': [10, 30],\n",
    "}\n",
    "\n",
    "# ë‹¤ì¤‘ ë¶„ë¥˜ ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ê·¸ë¦¬ë“œ ì„œì¹˜ íŒŒë¼ë¯¸í„°\n",
    "multi_param_grid = {\n",
    "    'learning_rate': [0.01, 0.03, 0.1],\n",
    "    'n_estimators': [1000, 1500],\n",
    "    'num_leaves': [50, 100],\n",
    "    'max_depth': [-1, 5],\n",
    "    'min_child_samples': [10, 30],\n",
    "}\n",
    "\n",
    "print(f\"âš™ï¸ LightGBM íŒŒë¼ë¯¸í„° ì„¤ì • ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911510b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. ê¸°ë³¸ ëª¨ë¸ í•™ìŠµ (Feature Importance ì¶”ì¶œìš©)\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ğŸ”¥ 1ë‹¨ê³„: ê¸°ë³¸ ëª¨ë¸ í•™ìŠµ (Feature Importance ì¶”ì¶œ)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# ì´ì§„ ë¶„ë¥˜ ê¸°ë³¸ ëª¨ë¸\n",
    "binary_models_basic = {}\n",
    "basic_importance_dict = {}\n",
    "\n",
    "for col in targets_binary:\n",
    "    print(f\"   ğŸ¯ {col} ê¸°ë³¸ ëª¨ë¸ í•™ìŠµ ì¤‘...\")\n",
    "    y = train_df_advanced[col]\n",
    "    \n",
    "    # ê¸°ë³¸ LightGBM ëª¨ë¸\n",
    "    model = LGBMClassifier(objective='binary', **lgbm_params)\n",
    "    model.fit(X_advanced, y)\n",
    "    \n",
    "    # Feature Importance ì €ì¥\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': X_advanced.columns,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False).head(20)  # 20ê°œ\n",
    "    \n",
    "    binary_models_basic[col] = model\n",
    "    basic_importance_dict[col] = importance_df\n",
    "    \n",
    "    print(f\"      âœ… ì™„ë£Œ - ìµœê³  ì¤‘ìš”ë„: {importance_df.iloc[0]['importance']:.4f} ({importance_df.iloc[0]['feature']})\")\n",
    "\n",
    "# S1 ë‹¤ì¤‘ë¶„ë¥˜ ê¸°ë³¸ ëª¨ë¸\n",
    "print(f\"   ğŸ¯ S1 ë‹¤ì¤‘ë¶„ë¥˜ ê¸°ë³¸ ëª¨ë¸ í•™ìŠµ ì¤‘...\")\n",
    "y_multi = train_df_advanced['S1']\n",
    "model_s1_basic = LGBMClassifier(objective='multiclass', num_class=len(y_multi.unique()), **lgbm_params)\n",
    "model_s1_basic.fit(X_advanced, y_multi)\n",
    "\n",
    "# S1 Feature Importance ì €ì¥\n",
    "s1_importance = pd.DataFrame({\n",
    "    'feature': X_advanced.columns,\n",
    "    'importance': model_s1_basic.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(20)\n",
    "\n",
    "basic_importance_dict['S1'] = s1_importance\n",
    "print(f\"      âœ… ì™„ë£Œ - ìµœê³  ì¤‘ìš”ë„: {s1_importance.iloc[0]['importance']:.4f} ({s1_importance.iloc[0]['feature']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4ca9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4. Feature Importance ì‹œê°í™”\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nğŸ“Š Feature Importance ì‹œê°í™”...\")\n",
    "\n",
    "# ì´ì§„ë¶„ë¥˜ ëª¨ë¸ë“¤ì˜ Feature Importance ì‹œê°í™”\n",
    "n_models = len(binary_models_basic)\n",
    "n_cols = 2\n",
    "n_rows = math.ceil(n_models / n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 8 * n_rows))\n",
    "axes = axes.flatten() if n_models > 1 else [axes]\n",
    "\n",
    "for idx, (col_name, importance_df) in enumerate(basic_importance_dict.items()):\n",
    "    if col_name == 'S1':  # S1ì€ ë”°ë¡œ ì²˜ë¦¬\n",
    "        continue\n",
    "        \n",
    "    sns.barplot(\n",
    "        x='importance', y='feature', \n",
    "        data=importance_df.head(10),  # ìƒìœ„ 10ê°œë§Œ í‘œì‹œ\n",
    "        ax=axes[idx], \n",
    "        palette='viridis'\n",
    "    )\n",
    "    axes[idx].set_title(f'{col_name} - Top 10 Feature Importance', fontsize=14, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Importance Score', fontsize=12)\n",
    "    axes[idx].set_ylabel('Features', fontsize=12)\n",
    "    axes[idx].tick_params(axis='y', labelsize=10)\n",
    "    axes[idx].tick_params(axis='x', labelsize=10)\n",
    "\n",
    "# ë‚¨ì€ ë¹ˆ subplot ì œê±°\n",
    "for j in range(len(binary_models_basic), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# S1 ë‹¤ì¤‘ë¶„ë¥˜ Feature Importance ì‹œê°í™”\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.barplot(x='importance', y='feature', data=s1_importance.head(15), palette='viridis')\n",
    "plt.title('S1 ë‹¤ì¤‘ë¶„ë¥˜ ëª¨ë¸ - Top 15 Feature Importance', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Š Feature Importance ì‹œê°í™” ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0538dadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5. ê°œë³„ ëª¨ë¸ í•™ìŠµ (ê° íƒ€ê²Ÿë³„ ìƒìœ„ íŠ¹ì„± ì‚¬ìš© + SMOTE + GridSearch)\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ğŸš€ 2ë‹¨ê³„: ê°œë³„ ëª¨ë¸ ìµœì í™” í•™ìŠµ (SMOTE + GridSearch)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# ê° ëª¨ë¸ë³„ ì´ì§„ë¶„ë¥˜ ê²°ê³¼ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬\n",
    "binary_models_optimized = {}\n",
    "binary_preds_optimized = {}\n",
    "\n",
    "# ì´ì§„ë¶„ë¥˜ ëª¨ë¸ë“¤ ìµœì í™”\n",
    "for model_name in targets_binary:\n",
    "    target_col = model_name\n",
    "    \n",
    "    # í•´ë‹¹ ëª¨ë¸ì˜ ìƒìœ„ 20ê°œ ì¤‘ìš” í”¼ì²˜ ì„ íƒ\n",
    "    top_features = basic_importance_dict[model_name]['feature'].tolist()\n",
    "    \n",
    "    # ì¤‘ìš” í”¼ì²˜ë“¤ë¡œ ë°ì´í„° í•„í„°ë§\n",
    "    X_model_selected = X_advanced[top_features]\n",
    "    test_X_model_selected = test_X_advanced[top_features]\n",
    "    \n",
    "    y = train_df_advanced[target_col]\n",
    "    \n",
    "    print(f\"\\nğŸ¯ {model_name} ëª¨ë¸ ìµœì í™”\")\n",
    "    print(f\"   - ì‚¬ìš© íŠ¹ì„± ìˆ˜: {X_model_selected.shape[1]}ê°œ\")\n",
    "    print(f\"   - í›ˆë ¨ ìƒ˜í”Œ ìˆ˜: {X_model_selected.shape[0]}ê°œ\")\n",
    "    print(f\"   - í´ë˜ìŠ¤ ë¶„í¬: {dict(y.value_counts().sort_index())}\")\n",
    "    \n",
    "    # k_neighbors ìë™ ì¡°ì •\n",
    "    min_class_count = min(y.value_counts())\n",
    "    if min_class_count <= 3:\n",
    "        k_neighbors = max(1, min_class_count - 1)\n",
    "        print(f\"   âš ï¸  ì†Œìˆ˜ í´ë˜ìŠ¤ ìƒ˜í”Œì´ ì ì–´ k_neighborsë¥¼ {k_neighbors}ë¡œ ì¡°ì •\")\n",
    "    else:\n",
    "        k_neighbors = 3\n",
    "    \n",
    "    # SMOTE + LightGBM Pipeline\n",
    "    pipeline = ImbPipeline([\n",
    "        ('smote', SMOTE(random_state=42, k_neighbors=k_neighbors)),\n",
    "        ('classifier', LGBMClassifier(objective='binary', **lgbm_params))\n",
    "    ])\n",
    "    \n",
    "    # íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ (classifier__ ì ‘ë‘ì‚¬ ì¶”ê°€)\n",
    "    binary_param_grid_mod = {}\n",
    "    for key, value in binary_param_grid.items():\n",
    "        binary_param_grid_mod[f'classifier__{key}'] = value\n",
    "    \n",
    "    # ê·¸ë¦¬ë“œ ì„œì¹˜ë¡œ ìµœì  íŒŒë¼ë¯¸í„° ì°¾ê¸°\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=binary_param_grid_mod,\n",
    "        scoring='f1_macro',\n",
    "        cv=cv,\n",
    "        verbose=0,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print(f\"   â³ ëª¨ë¸ ìµœì í™” ì¤‘... (SMOTE k_neighbors={k_neighbors})\")\n",
    "    grid_search.fit(X_model_selected, y)\n",
    "    \n",
    "    print(f\"   âœ… ì™„ë£Œ!\")\n",
    "    print(f\"      - ìµœì  íŒŒë¼ë¯¸í„°: {grid_search.best_params_}\")\n",
    "    print(f\"      - ìµœê³  F1 ì ìˆ˜: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # ìµœì  ëª¨ë¸ê³¼ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥\n",
    "    best_model = grid_search.best_estimator_\n",
    "    predictions = best_model.predict(test_X_model_selected)\n",
    "    \n",
    "    binary_models_optimized[model_name] = best_model\n",
    "    binary_preds_optimized[model_name] = predictions\n",
    "    \n",
    "    print(f\"      - ì˜ˆì¸¡ ì™„ë£Œ: {len(predictions)}ê°œ ìƒ˜í”Œ\")\n",
    "    print(f\"      - ì˜ˆì¸¡ í´ë˜ìŠ¤ ë¶„í¬: {dict(pd.Series(predictions).value_counts().sort_index())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be636e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6. S1 ë‹¤ì¤‘ë¶„ë¥˜ ëª¨ë¸ ìµœì í™”\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nğŸ¯ S1 ë‹¤ì¤‘ë¶„ë¥˜ ëª¨ë¸ ìµœì í™”\")\n",
    "\n",
    "target_col = 'S1'\n",
    "y_multiclass = train_df_advanced[target_col]\n",
    "\n",
    "# S1 ìƒìœ„ 20ê°œ ì¤‘ìš” í”¼ì²˜ ì„ íƒ\n",
    "top_features_s1 = basic_importance_dict[target_col]['feature'].tolist()\n",
    "X_s1_selected = X_advanced[top_features_s1]\n",
    "test_X_s1_selected = test_X_advanced[top_features_s1]\n",
    "\n",
    "print(f\"   - ì‚¬ìš© íŠ¹ì„± ìˆ˜: {X_s1_selected.shape[1]}ê°œ\")\n",
    "print(f\"   - í›ˆë ¨ ìƒ˜í”Œ ìˆ˜: {X_s1_selected.shape[0]}ê°œ\")\n",
    "print(f\"   - í´ë˜ìŠ¤ ìˆ˜: {len(y_multiclass.unique())}ê°œ\")\n",
    "print(f\"   - í´ë˜ìŠ¤ ë¶„í¬: {dict(y_multiclass.value_counts().sort_index())}\")\n",
    "\n",
    "# SMOTE ì ìš©\n",
    "smote = SMOTE(random_state=42, k_neighbors=3)\n",
    "X_s1_smote, y_multiclass_smote = smote.fit_resample(X_s1_selected, y_multiclass)\n",
    "\n",
    "print(f\"   - SMOTE ì ìš© í›„ ìƒ˜í”Œ ìˆ˜: {X_s1_smote.shape[0]}ê°œ\")\n",
    "print(f\"   - SMOTE ì ìš© í›„ í´ë˜ìŠ¤ ë¶„í¬: {dict(pd.Series(y_multiclass_smote).value_counts().sort_index())}\")\n",
    "\n",
    "# ë‹¤ì¤‘ë¶„ë¥˜ìš© ëª¨ë¸ ì„¤ì •\n",
    "multiclass_model = LGBMClassifier(\n",
    "    objective='multiclass',\n",
    "    metric='multi_logloss',\n",
    "    **lgbm_params\n",
    ")\n",
    "\n",
    "# ê·¸ë¦¬ë“œ ì„œì¹˜ë¡œ ìµœì  íŒŒë¼ë¯¸í„° ì°¾ê¸°\n",
    "grid_search_multiclass = GridSearchCV(\n",
    "    estimator=multiclass_model,\n",
    "    param_grid=multi_param_grid,\n",
    "    scoring='f1_macro',\n",
    "    cv=cv,\n",
    "    verbose=0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"   â³ S1 ë‹¤ì¤‘ë¶„ë¥˜ ëª¨ë¸ ìµœì í™” ì¤‘...\")\n",
    "grid_search_multiclass.fit(X_s1_smote, y_multiclass_smote)\n",
    "\n",
    "print(f\"   âœ… ì™„ë£Œ!\")\n",
    "print(f\"      - ìµœì  íŒŒë¼ë¯¸í„°: {grid_search_multiclass.best_params_}\")\n",
    "print(f\"      - ìµœê³  F1-Macro ì ìˆ˜: {grid_search_multiclass.best_score_:.4f}\")\n",
    "\n",
    "# ìµœì  ëª¨ë¸ê³¼ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥\n",
    "best_multiclass_model = grid_search_multiclass.best_estimator_\n",
    "multiclass_predictions = best_multiclass_model.predict(test_X_s1_selected)\n",
    "\n",
    "print(f\"      - ì˜ˆì¸¡ ì™„ë£Œ: {len(multiclass_predictions)}ê°œ ìƒ˜í”Œ\")\n",
    "print(f\"      - ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ ë¶„í¬: {dict(pd.Series(multiclass_predictions).value_counts().sort_index())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965467ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7. ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ğŸ“‹ ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„±\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# sample ê¸°ë°˜ ì œì¶œ í¬ë§· ê°€ì ¸ì˜¤ê¸°\n",
    "submission_final = sample_submission[['subject_id', 'sleep_date', 'lifelog_date']].copy()\n",
    "\n",
    "# lifelog_date ê¸°ì¤€ìœ¼ë¡œ string â†’ date í˜•ì‹ í†µì¼\n",
    "submission_final['lifelog_date'] = pd.to_datetime(submission_final['lifelog_date']).dt.date\n",
    "\n",
    "print(f\"ğŸ“ ì œì¶œ ë°ì´í„° ì¤€ë¹„:\")\n",
    "print(f\"   - ì œì¶œ ìƒ˜í”Œ ìˆ˜: {len(submission_final)}\")\n",
    "\n",
    "# S1 ë‹¤ì¤‘ë¶„ë¥˜ ì˜ˆì¸¡ ê²°ê³¼ ì‚¬ìš©\n",
    "assert len(submission_final) == len(multiclass_predictions), f\"ê¸¸ì´ ë¶ˆì¼ì¹˜: submission={len(submission_final)}, S1 predictions={len(multiclass_predictions)}\"\n",
    "\n",
    "# ë‹¤ì¤‘ ë¶„ë¥˜ ì˜ˆì¸¡ ë¶™ì´ê¸°\n",
    "submission_final['S1'] = multiclass_predictions\n",
    "\n",
    "# ì´ì§„ ë¶„ë¥˜ ê²°ê³¼ ë¶™ì´ê¸°\n",
    "for col in targets_binary:\n",
    "    individual_predictions = binary_preds_optimized[col]\n",
    "    \n",
    "    # shape ì²´í¬\n",
    "    assert len(submission_final) == len(individual_predictions), f\"ê¸¸ì´ ë¶ˆì¼ì¹˜: submission={len(submission_final)}, {col} predictions={len(individual_predictions)}\"\n",
    "    \n",
    "    submission_final[col] = individual_predictions.astype(int)\n",
    "\n",
    "# ğŸ“Š ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½ ì¶œë ¥\n",
    "print(f\"\\nğŸ“Š ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for col in ['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']:\n",
    "    value_counts = submission_final[col].value_counts().sort_index()\n",
    "    total = len(submission_final)\n",
    "    print(f\"ğŸ”¹ {col}:\")\n",
    "    for value, count in value_counts.items():\n",
    "        percentage = (count / total) * 100\n",
    "        print(f\"   í´ë˜ìŠ¤ {value}: {count:,}ê°œ ({percentage:.1f}%)\")\n",
    "\n",
    "# ìµœì¢… ì œì¶œ í˜•ì‹ ì •ë ¬\n",
    "submission_final = submission_final[['subject_id', 'sleep_date', 'lifelog_date', 'Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']]\n",
    "\n",
    "# ì €ì¥\n",
    "submission_final.to_csv(submission_folder + submission_file, index=False)\n",
    "\n",
    "print(f\"\\nâœ… ì œì¶œ íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {submission_folder + submission_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d6b5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 8. ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ğŸ‰ ê³ ê¸‰ ì „ì²˜ë¦¬ + LightGBM ëª¨ë¸ë§ ì™„ë£Œ!\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(f\"ğŸ”¹ ì´ì§„ë¶„ë¥˜ ëª¨ë¸ë“¤:\")\n",
    "for model_name in targets_binary:\n",
    "    if model_name in binary_models_optimized:\n",
    "        print(f\"   - {model_name}: âœ… í•™ìŠµ ì™„ë£Œ\")\n",
    "    else:\n",
    "        print(f\"   - {model_name}: âŒ í•™ìŠµ ì‹¤íŒ¨\")\n",
    "\n",
    "print(f\"\\nğŸ”¹ ë‹¤ì¤‘ë¶„ë¥˜ ëª¨ë¸:\")\n",
    "print(f\"   - S1 (ë‹¤ì¤‘ë¶„ë¥˜): âœ… í•™ìŠµ ì™„ë£Œ\")\n",
    "print(f\"   - ìµœê³  F1-Macro ì ìˆ˜: {grid_search_multiclass.best_score_:.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š ì‚¬ìš©ëœ íŠ¹ì„± ì •ë³´:\")\n",
    "print(f\"   - ì „ì²´ íŠ¹ì„± ìˆ˜: {len(feature_cols)}\")\n",
    "print(f\"   - ê³ ê¸‰ ì‹œê³„ì—´ íŠ¹ì„±: {len([col for col in feature_cols if 'rolling' in col or 'lag' in col])}\")\n",
    "print(f\"   - ê°œì¸í™” íŠ¹ì„±: {len([col for col in feature_cols if 'personal' in col])}\")\n",
    "print(f\"   - ìƒì²´ì‹ í˜¸ íŠ¹ì„±: {len([col for col in feature_cols if any(bio in col for bio in ['hr_', 'rmssd', 'pnn50'])])}\")\n",
    "print(f\"   - í–‰ë™íŒ¨í„´ íŠ¹ì„±: {len([col for col in feature_cols if any(pattern in col for pattern in ['session', 'pattern', 'transition'])])}\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ ì €ì¥ëœ ê²°ê³¼:\")\n",
    "print(f\"   - ì œì¶œ íŒŒì¼: {submission_file}\")\n",
    "print(f\"   - ìœ„ì¹˜: {submission_folder}\")\n",
    "\n",
    "# ëª¨ë¸ ê°ì²´ë“¤ë„ ì €ì¥ë˜ì–´ ì¶”í›„ ë¶„ì„ ê°€ëŠ¥\n",
    "print(f\"\\nğŸ”§ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ê°ì²´ë“¤:\")\n",
    "print(f\"   - binary_models_optimized: ìµœì í™”ëœ ì´ì§„ë¶„ë¥˜ ëª¨ë¸ë“¤\")\n",
    "print(f\"   - best_multiclass_model: ìµœì í™”ëœ S1 ë‹¤ì¤‘ë¶„ë¥˜ ëª¨ë¸\")\n",
    "print(f\"   - basic_importance_dict: ê° ëª¨ë¸ë³„ Feature Importance\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ:\")\n",
    "print(f\"   1. êµì°¨ê²€ì¦ìœ¼ë¡œ ëª¨ë¸ ì„±ëŠ¥ ê²€ì¦\")\n",
    "print(f\"   2. ì•™ìƒë¸” ëª¨ë¸ ì ìš© (Voting, Stacking)\")\n",
    "print(f\"   3. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶”ê°€ íŠœë‹\")\n",
    "print(f\"   4. íŠ¹ì„± ì„ íƒ ìµœì í™”\")\n",
    "\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
