{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1f085c0",
   "metadata": {},
   "source": [
    "### feature ëŒ€ìƒìœ¼ë¡œ LGBM í•™ìŠµ  \n",
    "(Developed from dacon_etri_base_mod1.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0593c4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# í˜„ì¬ ë‚ ì§œ ë° ì‹œê°„ ê°€ì ¸ì˜¤ê¸°\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"%m%d_%H%M\")  # ì˜ˆ: 0517_1530\n",
    "\n",
    "\n",
    "submission_folder = '/users/KTL/Desktop/dacon/submission/'\n",
    "submission_file = f'submission_final_catboost_1_{timestamp}.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bead988",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import glob \n",
    "import random \n",
    "import os \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import ast \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "903c2113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed ê³ ì • \n",
    "SD = 42 \n",
    "random.seed(SD) \n",
    "np.random.seed(SD) \n",
    "os.environ['PYTHONHASHSEED'] = str(SD)\n",
    "tf.random.set_seed(SD)  # TensorFlow ì‹œë“œ ì„¤ì •\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ ì„¤ì • - VSCode ìƒëŒ€ê²½ë¡œë¡œ ë³€ê²½\n",
    "# ì‹¤ì œ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì • í•„ìš”\n",
    "base_folder =  '/users/KTL/Desktop/ETRI_lifelog_dataset'\n",
    "folder = '/ch2025_data_items'\n",
    "\n",
    "data_dir = base_folder + folder \n",
    "\n",
    "\n",
    "# Parquet íŒŒì¼ ì „ì²´ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ \n",
    "parquet_files = glob.glob(os.path.join(data_dir, 'ch2025_*.parquet')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9028dc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded: mACStatus, shape = (939896, 3)\n",
      "âœ… Loaded: mActivity, shape = (961062, 3)\n",
      "âœ… Loaded: mAmbience, shape = (476577, 3)\n",
      "âœ… Loaded: mBle, shape = (21830, 3)\n",
      "âœ… Loaded: mGps, shape = (800611, 3)\n",
      "âœ… Loaded: mLight, shape = (96258, 3)\n",
      "âœ… Loaded: mScreenStatus, shape = (939653, 3)\n",
      "âœ… Loaded: mUsageStats, shape = (45197, 3)\n",
      "âœ… Loaded: mWifi, shape = (76336, 3)\n",
      "âœ… Loaded: wHr, shape = (382918, 3)\n",
      "âœ… Loaded: wLight, shape = (633741, 3)\n",
      "âœ… Loaded: wPedo, shape = (748100, 9)\n"
     ]
    }
   ],
   "source": [
    "# íŒŒì¼ ì´ë¦„ì„ í‚¤ë¡œ, DataFrameì„ ê°’ìœ¼ë¡œ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ \n",
    "lifelog_data = {} \n",
    "\n",
    "# íŒŒì¼ë³„ë¡œ ì½ê¸° \n",
    "for file_path in parquet_files: \n",
    "    name = os.path.basename(file_path).replace('.parquet', '').replace('ch2025_', '') \n",
    "    lifelog_data[name] = pd.read_parquet(file_path) \n",
    "    print(f\"âœ… Loaded: {name}, shape = {lifelog_data[name].shape}\") \n",
    "\n",
    "# ë”•ì…”ë„ˆë¦¬ì— ìˆëŠ” ëª¨ë“  í•­ëª©ì„ ë…ë¦½ì ì¸ ë³€ìˆ˜ë¡œ í• ë‹¹ \n",
    "for key, df in lifelog_data.items(): \n",
    "    globals()[f\"{key}_df\"] = df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cbd9424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”íŠ¸ë¦­ìŠ¤ íŒŒì¼ ì½ê¸°\n",
    "metrics_train = pd.read_csv(base_folder + '/ch2025_metrics_train.csv')\n",
    "sample_submission = pd.read_csv(base_folder+'/ch2025_submission_sample.csv')\n",
    "\n",
    "# âœ… ê¸°ì¤€ ìŒ (subject_id, lifelog_date) \n",
    "sample_submission['lifelog_date'] = pd.to_datetime(sample_submission['lifelog_date']) \n",
    "test_keys = set(zip(sample_submission['subject_id'], sample_submission['lifelog_date'].dt.date)) \n",
    "\n",
    "# âœ… DataFrame ë³„ timestamp ì»¬ëŸ¼ ìˆ˜ë™ ì§€ì • \n",
    "dataframes = { \n",
    "    'mACStatus': (mACStatus_df, 'timestamp'), \n",
    "    'mActivity': (mActivity_df, 'timestamp'), \n",
    "    'mAmbience': (mAmbience_df, 'timestamp'), \n",
    "    'mBle': (mBle_df, 'timestamp'), \n",
    "    'mGps': (mGps_df, 'timestamp'), \n",
    "    'mLight': (mLight_df, 'timestamp'), \n",
    "    'mScreenStatus': (mScreenStatus_df, 'timestamp'), \n",
    "    'mUsageStats': (mUsageStats_df, 'timestamp'), \n",
    "    'mWifi': (mWifi_df, 'timestamp'), \n",
    "    'wHr': (wHr_df, 'timestamp'), \n",
    "    'wLight': (wLight_df, 'timestamp'), \n",
    "    'wPedo': (wPedo_df, 'timestamp'), \n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b77b6fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… ë¶„ë¦¬ í•¨ìˆ˜ \n",
    "def split_test_train(df, subject_col='subject_id', timestamp_col='timestamp'): \n",
    "    df[timestamp_col] = pd.to_datetime(df[timestamp_col], errors='coerce') \n",
    "    df = df.dropna(subset=[timestamp_col]) \n",
    "    df['date_only'] = df[timestamp_col].dt.date \n",
    "    df['key'] = list(zip(df[subject_col], df['date_only'])) \n",
    "    test_df = df[df['key'].isin(test_keys)].drop(columns=['date_only', 'key']) \n",
    "    train_df = df[~df['key'].isin(test_keys)].drop(columns=['date_only', 'key']) \n",
    "    return test_df, train_df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3416230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ mACStatus ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mACStatus_test â†’ (335849, 3), mACStatus_train â†’ (604047, 3)\n",
      "â³ mActivity ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mActivity_test â†’ (343579, 3), mActivity_train â†’ (617483, 3)\n",
      "â³ mAmbience ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mAmbience_test â†’ (170453, 3), mAmbience_train â†’ (306124, 3)\n",
      "â³ mBle ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mBle_test â†’ (8140, 3), mBle_train â†’ (13690, 3)\n",
      "â³ mGps ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mGps_test â†’ (287386, 3), mGps_train â†’ (513225, 3)\n",
      "â³ mLight ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mLight_test â†’ (34439, 3), mLight_train â†’ (61819, 3)\n",
      "â³ mScreenStatus ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mScreenStatus_test â†’ (336160, 3), mScreenStatus_train â†’ (603493, 3)\n",
      "â³ mUsageStats ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mUsageStats_test â†’ (16499, 3), mUsageStats_train â†’ (28698, 3)\n",
      "â³ mWifi ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mWifi_test â†’ (27467, 3), mWifi_train â†’ (48869, 3)\n",
      "â³ wHr ë¶„ë¦¬ ì¤‘...\n",
      "âœ… wHr_test â†’ (143311, 3), wHr_train â†’ (239607, 3)\n",
      "â³ wLight ë¶„ë¦¬ ì¤‘...\n",
      "âœ… wLight_test â†’ (233809, 3), wLight_train â†’ (399932, 3)\n",
      "â³ wPedo ë¶„ë¦¬ ì¤‘...\n",
      "âœ… wPedo_test â†’ (288832, 9), wPedo_train â†’ (459268, 9)\n"
     ]
    }
   ],
   "source": [
    "# âœ… ê²°ê³¼ ì €ì¥ \n",
    "for name, (df, ts_col) in dataframes.items(): \n",
    "    print(f\"â³ {name} ë¶„ë¦¬ ì¤‘...\") \n",
    "    test_df, train_df = split_test_train(df.copy(), subject_col='subject_id', timestamp_col=ts_col) \n",
    "    globals()[f\"{name}_test\"] = test_df \n",
    "    globals()[f\"{name}_train\"] = train_df \n",
    "    print(f\"âœ… {name}_test â†’ {test_df.shape}, {name}_train â†’ {train_df.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7f34d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mACStatus(df): \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    df = df.sort_values(['subject_id', 'timestamp']) \n",
    "    results = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        status = group['m_charging'].values # 0/1 ìƒíƒœ \n",
    "        times = group['timestamp'].values # ì¶©ì „ ìƒíƒœ ë¹„ìœ¨ \n",
    "        ratio_charging = status.mean() \n",
    "        # ìƒíƒœ ì „ì´ íšŸìˆ˜ \n",
    "        transitions = (status[1:] != status[:-1]).sum() \n",
    "        # ì—°ì†ëœ 1 ìƒíƒœ ê¸¸ì´ë“¤ \n",
    "        lengths = [] \n",
    "        current_len = 0 \n",
    "        for val in status: \n",
    "            if val == 1: \n",
    "                current_len += 1 \n",
    "            elif current_len > 0: \n",
    "                lengths.append(current_len) \n",
    "                current_len = 0 \n",
    "        if current_len > 0: \n",
    "            lengths.append(current_len) \n",
    "        avg_charging_duration = np.mean(lengths) if lengths else 0 \n",
    "        max_charging_duration = np.max(lengths) if lengths else 0 \n",
    "        results.append({ \n",
    "            'subject_id': subj, \n",
    "            'date': date, \n",
    "            'charging_ratio': ratio_charging, \n",
    "            'charging_transitions': transitions, \n",
    "            'avg_charging_duration': avg_charging_duration, \n",
    "            'max_charging_duration': max_charging_duration, \n",
    "        }) \n",
    "    return pd.DataFrame(results) \n",
    "\n",
    "mACStatus_df2 = process_mACStatus(mACStatus_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7c3c0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mActivity(df): \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    summary = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        counts = group['m_activity'].value_counts(normalize=True) # ë¹„ìœ¨ \n",
    "        row = {'subject_id': subj, 'date': date} \n",
    "        # 0~8 ë¹„ìœ¨ ì €ì¥ \n",
    "        for i in range(9): \n",
    "            row[f'activity_{i}_ratio'] = counts.get(i, 0) \n",
    "        # ì£¼ìš” í™œë™ ì •ë³´ \n",
    "        row['dominant_activity'] = group['m_activity'].mode()[0] \n",
    "        row['num_unique_activities'] = group['m_activity'].nunique() \n",
    "        summary.append(row) \n",
    "    return pd.DataFrame(summary) \n",
    "\n",
    "mActivity_df2 = process_mActivity(mActivity_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ddb753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì§€ì •ëœ 10ê°œ ë¼ë²¨ \n",
    "top_10_labels = [ \n",
    "    \"Inside, small room\", \"Speech\", \"Silence\", \"Music\", \"Narration, monologue\", \n",
    "    \"Child speech, kid speaking\", \"Conversation\", \"Speech synthesizer\", \"Shout\", \"Babbling\" \n",
    "] \n",
    "\n",
    "def process_mAmbience_top10(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    # ì´ˆê¸°í™” \n",
    "    for label in top_10_labels + ['others']: \n",
    "        df[label] = 0.0 \n",
    "    for idx, row in df.iterrows(): \n",
    "        parsed = ast.literal_eval(row['m_ambience']) if isinstance(row['m_ambience'], str) else row['m_ambience'] \n",
    "        others_prob = 0.0 \n",
    "        for label, prob in parsed: \n",
    "            prob = float(prob) \n",
    "            if label in top_10_labels: \n",
    "                df.at[idx, label] = prob \n",
    "            else: \n",
    "                others_prob += prob \n",
    "        df.at[idx, 'others'] = others_prob \n",
    "    return df.drop(columns=['m_ambience']) \n",
    "\n",
    "mAmbience_df2= process_mAmbience_top10(mAmbience_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7cc98b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_mAmbience_daily(df): \n",
    "    prob_cols = [col for col in df.columns if col not in ['subject_id', 'timestamp', 'date']] \n",
    "    # í•˜ë£¨ ë‹¨ìœ„ë¡œ í‰ê· ê°’ ìš”ì•½ \n",
    "    daily_summary = df.groupby(['subject_id', 'date'])[prob_cols].mean().reset_index() \n",
    "    return daily_summary \n",
    "\n",
    "mAmbience_df2 = summarize_mAmbience_daily(mAmbience_df2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "763e541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mBle(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    features = [] \n",
    "    for idx, row in df.iterrows(): \n",
    "        entry = ast.literal_eval(row['m_ble']) if isinstance(row['m_ble'], str) else row['m_ble'] \n",
    "        rssi_list = [] \n",
    "        class_0_cnt = 0 \n",
    "        class_other_cnt = 0 \n",
    "        for device in entry: \n",
    "            try: \n",
    "                rssi = int(device['rssi']) \n",
    "                rssi_list.append(rssi) \n",
    "                if str(device['device_class']) == '0': \n",
    "                    class_0_cnt += 1 \n",
    "                else: \n",
    "                    class_other_cnt += 1 \n",
    "            except: \n",
    "                continue # malformed record \n",
    "        feature = { \n",
    "            'subject_id': row['subject_id'], \n",
    "            'date': row['date'], \n",
    "            'device_class_0_cnt': class_0_cnt, \n",
    "            'device_class_others_cnt': class_other_cnt, \n",
    "            'device_count': len(rssi_list), \n",
    "            'rssi_mean': np.mean(rssi_list) if rssi_list else np.nan, \n",
    "            'rssi_min': np.min(rssi_list) if rssi_list else np.nan, \n",
    "            'rssi_max': np.max(rssi_list) if rssi_list else np.nan, \n",
    "        } \n",
    "        features.append(feature) \n",
    "    return pd.DataFrame(features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e38ac08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_mBle_daily(df): \n",
    "    # row ë‹¨ìœ„ BLE feature ì¶”ì¶œ \n",
    "    df = process_mBle(df) \n",
    "    # í•˜ë£¨ ë‹¨ìœ„ë¡œ cnt í•©ì¹˜ê¸° \n",
    "    grouped = df.groupby(['subject_id', 'date']).agg({ \n",
    "        'device_class_0_cnt': 'sum', \n",
    "        'device_class_others_cnt': 'sum', \n",
    "        'rssi_mean': 'mean', \n",
    "        'rssi_min': 'min', \n",
    "        'rssi_max': 'max', \n",
    "    }).reset_index() \n",
    "    # ì´í•© êµ¬í•´ì„œ ë¹„ìœ¨ ê³„ì‚° \n",
    "    total_cnt = grouped['device_class_0_cnt'] + grouped['device_class_others_cnt'] \n",
    "    grouped['device_class_0_ratio'] = grouped['device_class_0_cnt'] / total_cnt.replace(0, np.nan) \n",
    "    grouped['device_class_others_ratio'] = grouped['device_class_others_cnt'] / total_cnt.replace(0, np.nan) \n",
    "    # í•„ìš” ì—†ëŠ” ì›ë˜ cnt ì»¬ëŸ¼ ì œê±° \n",
    "    grouped.drop(columns=['device_class_0_cnt', 'device_class_others_cnt'], inplace=True) \n",
    "    return grouped \n",
    "\n",
    "mBle_df2 = summarize_mBle_daily(mBle_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "618d366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mGps(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    features = [] \n",
    "    for idx, row in df.iterrows(): \n",
    "        gps_list = ast.literal_eval(row['m_gps']) if isinstance(row['m_gps'], str) else row['m_gps'] \n",
    "        altitudes = [] \n",
    "        latitudes = [] \n",
    "        longitudes = [] \n",
    "        speeds = [] \n",
    "        for entry in gps_list: \n",
    "            try: \n",
    "                altitudes.append(float(entry['altitude'])) \n",
    "                latitudes.append(float(entry['latitude'])) \n",
    "                longitudes.append(float(entry['longitude'])) \n",
    "                speeds.append(float(entry['speed'])) \n",
    "            except: \n",
    "                continue \n",
    "        features.append({ \n",
    "            'subject_id': row['subject_id'], \n",
    "            'date': row['date'], \n",
    "            'altitude_mean': np.mean(altitudes) if altitudes else np.nan, \n",
    "            'latitude_std': np.std(latitudes) if latitudes else np.nan, \n",
    "            'longitude_std': np.std(longitudes) if longitudes else np.nan, \n",
    "            'speed_mean': np.mean(speeds) if speeds else np.nan, \n",
    "            'speed_max': np.max(speeds) if speeds else np.nan, \n",
    "            'speed_std': np.std(speeds) if speeds else np.nan, \n",
    "        }) \n",
    "    return pd.DataFrame(features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14a0be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_Gps_df2 = process_mGps(mGps_df) \n",
    "m_Gps_df2 = m_Gps_df2.groupby(['subject_id', 'date']).agg({ \n",
    "    'altitude_mean': 'mean', \n",
    "    'latitude_std': 'mean', \n",
    "    'longitude_std': 'mean', \n",
    "    'speed_mean': 'mean', \n",
    "    'speed_max': 'max', \n",
    "    'speed_std': 'mean' \n",
    "}).reset_index() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8389d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mLight(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    df['hour'] = df['timestamp'].dt.hour \n",
    "    # ë°¤(22~05ì‹œ), ë‚®(06~21ì‹œ) êµ¬ë¶„ \n",
    "    df['is_night'] = df['hour'].apply(lambda h: h >= 22 or h < 6) \n",
    "    # í•˜ë£¨ ë‹¨ìœ„ ìš”ì•½ \n",
    "    daily = df.groupby(['subject_id', 'date']).agg( \n",
    "        light_mean=('m_light', 'mean'), \n",
    "        light_std=('m_light', 'std'), \n",
    "        light_max=('m_light', 'max'), \n",
    "        light_min=('m_light', 'min'), \n",
    "        light_night_mean=('m_light', lambda x: x[df.loc[x.index, 'is_night']].mean()), \n",
    "        light_day_mean=('m_light', lambda x: x[~df.loc[x.index, 'is_night']].mean()), \n",
    "        light_night_ratio=('is_night', 'mean') # ë°¤ ì‹œê°„ ì¸¡ì • ë¹„ìœ¨ \n",
    "    ).reset_index() \n",
    "    return daily \n",
    "\n",
    "mLight_df2 = process_mLight(mLight_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16ad526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mScreenStatus(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    features = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        status = group['m_screen_use'].values \n",
    "        ratio_on = status.mean() \n",
    "        transitions = (status[1:] != status[:-1]).sum() \n",
    "        # ì—°ì†ëœ 1 ìƒíƒœ ê¸¸ì´ë“¤ \n",
    "        durations = [] \n",
    "        current = 0 \n",
    "        for val in status: \n",
    "            if val == 1: \n",
    "                current += 1 \n",
    "            elif current > 0: \n",
    "                durations.append(current) \n",
    "                current = 0 \n",
    "        if current > 0: \n",
    "            durations.append(current) \n",
    "        features.append({ \n",
    "            'subject_id': subj, \n",
    "            'date': date, \n",
    "            'screen_on_ratio': ratio_on, \n",
    "            'screen_on_transitions': transitions, \n",
    "            'screen_on_duration_avg': np.mean(durations) if durations else 0, \n",
    "            'screen_on_duration_max': np.max(durations) if durations else 0, \n",
    "        }) \n",
    "    return pd.DataFrame(features) \n",
    "\n",
    "mScreenStatus_df2 = process_mScreenStatus(mScreenStatus_df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f04edbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_apps = [ \n",
    "    'One UI í™ˆ', 'ì¹´ì¹´ì˜¤í†¡', 'ì‹œìŠ¤í…œ UI', 'NAVER', 'ìºì‹œì›Œí¬', \n",
    "    'ì„±ê²½ì¼ë…Q', 'YouTube', 'í†µí™”', 'ë©”ì‹œì§€', 'íƒ€ì„ìŠ¤í”„ë ˆë“œ', 'Instagram'\n",
    "] \n",
    "\n",
    "def process_mUsageStats(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    features = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        app_time = {app: 0 for app in top_apps} \n",
    "        others_time = 0 \n",
    "        for row in group['m_usage_stats']: \n",
    "            parsed = ast.literal_eval(row) if isinstance(row, str) else row \n",
    "            for entry in parsed: \n",
    "                app = entry.get('app_name') \n",
    "                time = entry.get('total_time', 0) \n",
    "                if app in top_apps: \n",
    "                    app_time[app] += int(time) \n",
    "                else: \n",
    "                    others_time += int(time) \n",
    "        feature = { \n",
    "            'subject_id': subj, \n",
    "            'date': date, \n",
    "            'others_time': others_time \n",
    "        } \n",
    "        # ê° ì•±ë³„ ì»¬ëŸ¼ ì¶”ê°€ \n",
    "        feature.update({f'{app}_time': app_time[app] for app in top_apps}) \n",
    "        features.append(feature) \n",
    "    return pd.DataFrame(features) \n",
    "\n",
    "mUsageStats_df2 = process_mUsageStats(mUsageStats_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73bcd6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_mWifi(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    results = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        rssi_all = [] \n",
    "        for row in group['m_wifi']: \n",
    "            parsed = ast.literal_eval(row) if isinstance(row, str) else row \n",
    "            for ap in parsed: \n",
    "                try: \n",
    "                    rssi = int(ap['rssi']) \n",
    "                    rssi_all.append(rssi) \n",
    "                except: \n",
    "                    continue \n",
    "        results.append({ \n",
    "            'subject_id': subj, \n",
    "            'date': date, \n",
    "            'wifi_rssi_mean': np.mean(rssi_all) if rssi_all else np.nan, \n",
    "            'wifi_rssi_min': np.min(rssi_all) if rssi_all else np.nan, \n",
    "            'wifi_rssi_max': np.max(rssi_all) if rssi_all else np.nan, \n",
    "            'wifi_detected_cnt': len(rssi_all) \n",
    "        }) \n",
    "    return pd.DataFrame(results) \n",
    "\n",
    "mWifi_df2 = process_mWifi(mWifi_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83782374",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_time_block(hour): \n",
    "    if 0 <= hour < 6: \n",
    "        return 'early_morning' \n",
    "    elif 6 <= hour < 12: \n",
    "        return 'morning' \n",
    "    elif 12 <= hour < 18: \n",
    "        return 'afternoon' \n",
    "    else: \n",
    "        return 'evening' \n",
    "\n",
    "def process_wHr_by_timeblock(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    df['block'] = df['timestamp'].dt.hour.map(get_time_block) \n",
    "    results = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        block_stats = {'subject_id': subj, 'date': date} \n",
    "        for block, block_group in group.groupby('block'): \n",
    "            hr_all = [] \n",
    "            for row in block_group['heart_rate']: \n",
    "                parsed = ast.literal_eval(row) if isinstance(row, str) else row \n",
    "                hr_all.extend([int(h) for h in parsed if h is not None]) \n",
    "            if not hr_all: \n",
    "                continue \n",
    "            above_100 = [hr for hr in hr_all if hr > 100] \n",
    "            block_stats[f'hr_{block}_mean'] = np.mean(hr_all) \n",
    "            block_stats[f'hr_{block}_std'] = np.std(hr_all) \n",
    "            block_stats[f'hr_{block}_max'] = np.max(hr_all) \n",
    "            block_stats[f'hr_{block}_min'] = np.min(hr_all) \n",
    "            block_stats[f'hr_{block}_above_100_ratio'] = len(above_100) / len(hr_all) \n",
    "        results.append(block_stats) \n",
    "    return pd.DataFrame(results) \n",
    "\n",
    "wHr_df2 = process_wHr_by_timeblock(wHr_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f39b5000",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_wLight_by_timeblock(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    df['block'] = df['timestamp'].dt.hour.map(get_time_block) \n",
    "    results = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        block_stats = {'subject_id': subj, 'date': date} \n",
    "        for block, block_group in group.groupby('block'): \n",
    "            lux = block_group['w_light'].dropna().values \n",
    "            if len(lux) == 0: \n",
    "                continue \n",
    "            block_stats[f'wlight_{block}_mean'] = np.mean(lux) \n",
    "            block_stats[f'wlight_{block}_std'] = np.std(lux) \n",
    "            block_stats[f'wlight_{block}_max'] = np.max(lux) \n",
    "            block_stats[f'wlight_{block}_min'] = np.min(lux) \n",
    "        results.append(block_stats) \n",
    "    return pd.DataFrame(results) \n",
    "\n",
    "wLight_df2 = process_wLight_by_timeblock(wLight_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a55ce49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_wPedo(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    summary = df.groupby(['subject_id', 'date']).agg({ \n",
    "        'step': 'sum', \n",
    "        'step_frequency': 'mean', \n",
    "        'distance': 'sum', \n",
    "        'speed': ['mean', 'max'], \n",
    "        'burned_calories': 'sum' \n",
    "    }).reset_index() \n",
    "    # ì»¬ëŸ¼ ì´ë¦„ ì •ë¦¬ \n",
    "    summary.columns = ['subject_id', 'date', 'step_sum', 'step_frequency_mean', 'distance_sum', 'speed_mean', 'speed_max', 'burned_calories_sum'] \n",
    "    return summary \n",
    "\n",
    "wPedo_df2 = process_wPedo(wPedo_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50c9a3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from functools import reduce \n",
    "df_list = [ \n",
    "    mACStatus_df2, \n",
    "    mActivity_df2,\n",
    "    mAmbience_df2, \n",
    "    mBle_df2, \n",
    "    m_Gps_df2, \n",
    "    mLight_df2, \n",
    "    mScreenStatus_df2, \n",
    "    mUsageStats_df2, \n",
    "    mWifi_df2, \n",
    "    wHr_df2, \n",
    "    wLight_df2, \n",
    "    wPedo_df2 \n",
    "] \n",
    "\n",
    "merged_df = reduce(lambda left, right: pd.merge(left, right, on=['subject_id', 'date'], how='outer'), df_list) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5335404",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# metrics_trainì˜ lifelog_date â†’ datetime.date í˜•ìœ¼ë¡œ ë³€í™˜ \n",
    "metrics_train['lifelog_date'] = pd.to_datetime(metrics_train['lifelog_date']).dt.date \n",
    "\n",
    "# merged_dfì˜ dateë„ ë³€í™˜ \n",
    "merged_df['date'] = pd.to_datetime(merged_df['date']).dt.date \n",
    "\n",
    "# 1. date ê¸°ì¤€ ì •ë ¬ì„ ìœ„í•´ metrics_trainì˜ lifelog_date -> dateë¡œ ë§ì¶”ê¸° \n",
    "metrics_train_renamed = metrics_train.rename(columns={'lifelog_date': 'date'}) \n",
    "\n",
    "# 2. train_df: metrics_trainê³¼ ì¼ì¹˜í•˜ëŠ” (subject_id, date) â†’ ë¼ë²¨ í¬í•¨ \n",
    "train_df = pd.merge(metrics_train_renamed, merged_df, on=['subject_id', 'date'], how='inner') \n",
    "\n",
    "# 3. test_df: metrics_trainì— ì—†ëŠ” (subject_id, date) \n",
    "merged_keys = merged_df[['subject_id', 'date']] \n",
    "train_keys = metrics_train_renamed[['subject_id', 'date']] \n",
    "test_keys = pd.merge(merged_keys, train_keys, on=['subject_id', 'date'], how='left', indicator=True) \n",
    "test_keys = test_keys[test_keys['_merge'] == 'left_only'].drop(columns=['_merge']) \n",
    "test_df = pd.merge(test_keys, merged_df, on=['subject_id', 'date'], how='left') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08c98b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# âœ… íƒ€ê²Ÿ ë¦¬ìŠ¤íŠ¸ \n",
    "targets_binary = ['Q1', 'Q2', 'Q3', 'S2', 'S3'] \n",
    "target_multiclass = 'S1' \n",
    "\n",
    "# âœ… feature ì¤€ë¹„ \n",
    "X = train_df.drop(columns=['subject_id', 'sleep_date', 'date', 'Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']) \n",
    "X.fillna(0, inplace=True) # ê²°ì¸¡ê°’ ì²˜ë¦¬ \n",
    "\n",
    "test_X = test_df.drop(columns=['subject_id', 'date']) \n",
    "test_X.fillna(0, inplace=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12d497e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ì»¬ëŸ¼ ì´ë¦„ì—ì„œ íŠ¹ìˆ˜ ë¬¸ì ì œê±°/ë³€í™˜ \n",
    "def sanitize_column_names(df): \n",
    "    df.columns = ( \n",
    "        df.columns \n",
    "        .str.replace(r\"[^\\w]\", \"_\", regex=True) # íŠ¹ìˆ˜ë¬¸ì â†’ _ \n",
    "        .str.replace(r\"__+\", \"_\", regex=True) # ì—°ì†ëœ _ ì œê±° \n",
    "        .str.strip(\"_\") # ì•ë’¤ _ ì œê±° \n",
    "    ) \n",
    "    return df \n",
    "\n",
    "# ëª¨ë“  ì…ë ¥ì— ì ìš© \n",
    "X = sanitize_column_names(X) \n",
    "test_X = sanitize_column_names(test_X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f78cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë“  íŠ¹ì„±ì— ëŒ€í•´ ëª¨ë¸ë§ ì§„í–‰ \n",
    "X_selected = X.copy()\n",
    "test_X_selected = test_X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4655dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸš€ GPU CatBoost ëª¨ë¸ ì¤€ë¹„ ì¤‘... (Macro-F1 ìµœì í™”)\n",
      "============================================================\n",
      "ğŸ” GPU ì„¤ì • í™•ì¸ ì¤‘...\n",
      "âš ï¸ GPU ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ CatBoost GPUëŠ” ê³„ì† ì‹œë„í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸš€ GPU CatBoost ëª¨ë¸ ì¤€ë¹„ ì¤‘... (Macro-F1 ìµœì í™”)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# GPU CatBoost ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, make_scorer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "# GPU ì„¤ì • í™•ì¸\n",
    "print(\"ğŸ” GPU ì„¤ì • í™•ì¸ ì¤‘...\")\n",
    "try:\n",
    "    import pynvml\n",
    "    pynvml.nvmlInit()\n",
    "    gpu_count = pynvml.nvmlDeviceGetCount()\n",
    "    print(f\"âœ… GPU ê°ì§€: {gpu_count}ê°œ\")\n",
    "    for i in range(gpu_count):\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "        name = pynvml.nvmlDeviceGetName(handle).decode('utf-8')\n",
    "        print(f\"   - GPU {i}: {name}\")\n",
    "except:\n",
    "    print(\"âš ï¸ GPU ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ CatBoost GPUëŠ” ê³„ì† ì‹œë„í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# GPU CatBoost ê³ ì„±ëŠ¥ íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "gpu_catboost_params = {\n",
    "    'task_type': 'GPU',              # ğŸ”¥ GPU ì‚¬ìš© í™œì„±í™”\n",
    "    'devices': '0',                  # GPU ë””ë°”ì´ìŠ¤ ID (ê¸°ë³¸ê°’)\n",
    "    'iterations': 2000,              # GPUë¡œ ë” ë§ì€ iterations\n",
    "    'learning_rate': 0.02,           # ë” ë‚®ì€ í•™ìŠµë¥ ë¡œ ì •ë°€ í•™ìŠµ\n",
    "    'random_seed': 42,\n",
    "    'verbose': 100,                  # 100 iterationë§ˆë‹¤ ì§„í–‰ìƒí™© ì¶œë ¥\n",
    "    'allow_writing_files': False,\n",
    "    'train_dir': None,\n",
    "    'gpu_ram_part': 0.8,            # ğŸ”¥ GPU ë©”ëª¨ë¦¬ 80% ì‚¬ìš©\n",
    "    'depth': 8,                      # ë” ê¹Šì€ íŠ¸ë¦¬ (GPUì—ì„œ íš¨ìœ¨ì )\n",
    "    'border_count': 254,             # ìµœëŒ€ê°’ìœ¼ë¡œ ì„¤ì • (GPU ìµœì í™”)\n",
    "    'feature_border_type': 'GreedyLogSum',  # GPU ìµœì í™” ì„¤ì •\n",
    "}\n",
    "\n",
    "# âœ… íƒ€ê²Ÿ ë¦¬ìŠ¤íŠ¸ \n",
    "targets_binary = ['Q1', 'Q2', 'Q3', 'S2', 'S3'] \n",
    "target_multiclass = 'S1'\n",
    "\n",
    "# Macro-F1 Scoreë¥¼ ìœ„í•œ ì»¤ìŠ¤í…€ ìŠ¤ì½”ì–´ë§ í•¨ìˆ˜\n",
    "def macro_f1_scorer(y_true, y_pred):\n",
    "    \"\"\"Macro-F1 Score ê³„ì‚°\"\"\"\n",
    "    return f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "macro_f1_score = make_scorer(macro_f1_scorer, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1ee74fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š 1ë‹¨ê³„: ê³ ê¸‰ í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„\n",
      "\n",
      "ğŸ”¹ Q1 í´ë˜ìŠ¤ ë¶„ì„:\n",
      "   ë¶„í¬: {0: 227, 1: 223}\n",
      "   í´ë˜ìŠ¤ 0: 227ê°œ (50.44%)\n",
      "   í´ë˜ìŠ¤ 1: 223ê°œ (49.56%)\n",
      "   ë¶ˆê· í˜• ë¹„ìœ¨: 1.02:1\n",
      "   í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {0: 0.9911894273127754, 1: 1.0089686098654709}\n",
      "\n",
      "ğŸ”¹ Q2 í´ë˜ìŠ¤ ë¶„ì„:\n",
      "   ë¶„í¬: {0: 197, 1: 253}\n",
      "   í´ë˜ìŠ¤ 0: 197ê°œ (43.78%)\n",
      "   í´ë˜ìŠ¤ 1: 253ê°œ (56.22%)\n",
      "   ë¶ˆê· í˜• ë¹„ìœ¨: 1.28:1\n",
      "   í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {0: 1.1421319796954315, 1: 0.8893280632411067}\n",
      "\n",
      "ğŸ”¹ Q3 í´ë˜ìŠ¤ ë¶„ì„:\n",
      "   ë¶„í¬: {0: 180, 1: 270}\n",
      "   í´ë˜ìŠ¤ 0: 180ê°œ (40.00%)\n",
      "   í´ë˜ìŠ¤ 1: 270ê°œ (60.00%)\n",
      "   ë¶ˆê· í˜• ë¹„ìœ¨: 1.50:1\n",
      "   í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {0: 1.25, 1: 0.8333333333333334}\n",
      "\n",
      "ğŸ”¹ S2 í´ë˜ìŠ¤ ë¶„ì„:\n",
      "   ë¶„í¬: {0: 157, 1: 293}\n",
      "   í´ë˜ìŠ¤ 0: 157ê°œ (34.89%)\n",
      "   í´ë˜ìŠ¤ 1: 293ê°œ (65.11%)\n",
      "   ë¶ˆê· í˜• ë¹„ìœ¨: 1.87:1\n",
      "   í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {0: 1.4331210191082802, 1: 0.7679180887372014}\n",
      "\n",
      "ğŸ”¹ S3 í´ë˜ìŠ¤ ë¶„ì„:\n",
      "   ë¶„í¬: {0: 152, 1: 298}\n",
      "   í´ë˜ìŠ¤ 0: 152ê°œ (33.78%)\n",
      "   í´ë˜ìŠ¤ 1: 298ê°œ (66.22%)\n",
      "   ë¶ˆê· í˜• ë¹„ìœ¨: 1.96:1\n",
      "   í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {1: 0.7550335570469798, 0: 1.480263157894737}\n",
      "\n",
      "ğŸ”¹ S1 í´ë˜ìŠ¤ ë¶„ì„:\n",
      "   ë¶„í¬: {0: 143, 1: 224, 2: 83}\n",
      "   í´ë˜ìŠ¤ 0: 143ê°œ (31.78%)\n",
      "   í´ë˜ìŠ¤ 1: 224ê°œ (49.78%)\n",
      "   í´ë˜ìŠ¤ 2: 83ê°œ (18.44%)\n",
      "   ë¶ˆê· í˜• ë¹„ìœ¨: 2.70:1\n",
      "   í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {0: 1.048951048951049, 1: 0.6696428571428571, 2: 1.8072289156626506}\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ğŸ“Š 1ë‹¨ê³„: í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„ ë° ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_class_distribution_advanced(train_df):\n",
    "    \"\"\"ê³ ê¸‰ í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„ ë° ê°€ì¤‘ì¹˜ ê³„ì‚°\"\"\"\n",
    "    print(\"\\nğŸ“Š 1ë‹¨ê³„: ê³ ê¸‰ í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„\")\n",
    "    \n",
    "    class_weights = {}\n",
    "    class_info = {}\n",
    "    \n",
    "    for col in targets_binary + [target_multiclass]:\n",
    "        if col in train_df.columns:\n",
    "            distribution = train_df[col].value_counts().sort_index()\n",
    "            total_samples = len(train_df[col])\n",
    "            \n",
    "            print(f\"\\nğŸ”¹ {col} í´ë˜ìŠ¤ ë¶„ì„:\")\n",
    "            print(f\"   ë¶„í¬: {dict(distribution)}\")\n",
    "            \n",
    "            # í´ë˜ìŠ¤ë³„ ë¹„ìœ¨ ê³„ì‚°\n",
    "            class_ratios = {}\n",
    "            for class_val, count in distribution.items():\n",
    "                ratio = count / total_samples\n",
    "                class_ratios[class_val] = ratio\n",
    "                print(f\"   í´ë˜ìŠ¤ {class_val}: {count:,}ê°œ ({ratio:.2%})\")\n",
    "            \n",
    "            # ë¶ˆê· í˜• ì •ë„ ê³„ì‚°\n",
    "            max_ratio = max(class_ratios.values())\n",
    "            min_ratio = min(class_ratios.values())\n",
    "            imbalance_ratio = max_ratio / min_ratio\n",
    "            print(f\"   ë¶ˆê· í˜• ë¹„ìœ¨: {imbalance_ratio:.2f}:1\")\n",
    "            \n",
    "            # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (balanced ë°©ì‹)\n",
    "            if col == target_multiclass:  # ë‹¤ì¤‘ë¶„ë¥˜\n",
    "                classes = train_df[col].unique()\n",
    "                weights = compute_class_weight('balanced', classes=classes, y=train_df[col])\n",
    "                class_weights[col] = dict(zip(classes, weights))\n",
    "            else:  # ì´ì§„ë¶„ë¥˜\n",
    "                classes = train_df[col].unique()\n",
    "                if len(classes) > 1:\n",
    "                    weights = compute_class_weight('balanced', classes=classes, y=train_df[col])\n",
    "                    class_weights[col] = dict(zip(classes, weights))\n",
    "                else:\n",
    "                    class_weights[col] = {classes[0]: 1.0}\n",
    "            \n",
    "            print(f\"   í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {class_weights[col]}\")\n",
    "            \n",
    "            # ì¶”ê°€ ì •ë³´ ì €ì¥\n",
    "            class_info[col] = {\n",
    "                'distribution': dict(distribution),\n",
    "                'imbalance_ratio': imbalance_ratio,\n",
    "                'total_samples': total_samples,\n",
    "                'num_classes': len(classes)\n",
    "            }\n",
    "    \n",
    "    return class_weights, class_info\n",
    "\n",
    "# í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„ ì‹¤í–‰\n",
    "class_weights, class_info = analyze_class_distribution_advanced(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "931b67e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ“Š 2ë‹¨ê³„: ê³ ê¸‰ ë°ì´í„° ì¦ê°• (Multiple SMOTE ê¸°ë²•)\n",
    "# =============================================================================\n",
    "\n",
    "def apply_advanced_data_augmentation(X, y, strategy='auto'):\n",
    "    \"\"\"ê³ ê¸‰ ë°ì´í„° ì¦ê°• ê¸°ë²• ì ìš©\"\"\"\n",
    "    print(f\"\\nğŸ“Š 2ë‹¨ê³„: ê³ ê¸‰ ë°ì´í„° ì¦ê°• ({strategy})\")\n",
    "    \n",
    "    original_distribution = dict(pd.Series(y).value_counts().sort_index())\n",
    "    print(f\"   ì›ë³¸ ë¶„í¬: {original_distribution}\")\n",
    "    \n",
    "    # í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜ í™•ì¸\n",
    "    min_samples = min(pd.Series(y).value_counts())\n",
    "    \n",
    "    # k_neighbors ë™ì  ì„¤ì •\n",
    "    if min_samples <= 5:\n",
    "        k_neighbors = max(1, min_samples - 1)\n",
    "    else:\n",
    "        k_neighbors = 5\n",
    "    \n",
    "    try:\n",
    "        if strategy == 'smote':\n",
    "            sampler = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
    "        elif strategy == 'adasyn':\n",
    "            sampler = ADASYN(random_state=42, n_neighbors=k_neighbors)\n",
    "        elif strategy == 'borderline':\n",
    "            sampler = BorderlineSMOTE(random_state=42, k_neighbors=k_neighbors)\n",
    "        else:  # auto\n",
    "            # ë¶ˆê· í˜• ì •ë„ì— ë”°ë¼ ìë™ ì„ íƒ\n",
    "            class_counts = pd.Series(y).value_counts()\n",
    "            imbalance_ratio = max(class_counts) / min(class_counts)\n",
    "            \n",
    "            if imbalance_ratio > 10:\n",
    "                sampler = ADASYN(random_state=42, n_neighbors=k_neighbors)\n",
    "                strategy = 'ADASYN (ì‹¬í•œ ë¶ˆê· í˜•)'\n",
    "            elif imbalance_ratio > 5:\n",
    "                sampler = BorderlineSMOTE(random_state=42, k_neighbors=k_neighbors)\n",
    "                strategy = 'BorderlineSMOTE (ì¤‘ê°„ ë¶ˆê· í˜•)'\n",
    "            else:\n",
    "                sampler = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
    "                strategy = 'SMOTE (ê°€ë²¼ìš´ ë¶ˆê· í˜•)'\n",
    "        \n",
    "        X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "        \n",
    "        new_distribution = dict(pd.Series(y_resampled).value_counts().sort_index())\n",
    "        print(f\"   ì‚¬ìš© ê¸°ë²•: {strategy}\")\n",
    "        print(f\"   ì¦ê°• í›„ ë¶„í¬: {new_distribution}\")\n",
    "        print(f\"   ìƒ˜í”Œ ìˆ˜ ë³€í™”: {len(y)} â†’ {len(y_resampled)}\")\n",
    "        \n",
    "        return X_resampled, y_resampled\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ ë°ì´í„° ì¦ê°• ì‹¤íŒ¨: {str(e)}\")\n",
    "        print(f\"   ì›ë³¸ ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e34c519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š 3ë‹¨ê³„: GPU CatBoost ê¸°ë³¸ ëª¨ë¸ í•™ìŠµ (ê³¼ì í•© ë°©ì§€ ë²„ì „)\n",
      "\n",
      "ğŸ”„ Q1 GPU CatBoost ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
      "0:\tlearn: 0.7367461\ttest: 0.4581809\tbest: 0.4581809 (0)\ttotal: 56.8ms\tremaining: 1m 53s\n",
      "50:\tlearn: 1.0000000\ttest: 0.4960716\tbest: 0.5801580 (2)\ttotal: 2.82s\tremaining: 1m 47s\n",
      "100:\tlearn: 1.0000000\ttest: 0.5307782\tbest: 0.5801580 (2)\ttotal: 5.57s\tremaining: 1m 44s\n",
      "bestTest = 0.5801579918\n",
      "bestIteration = 2\n",
      "Shrink model to first 3 iterations.\n",
      "   ğŸ“ˆ í›ˆë ¨ ì •í™•ë„: 0.8250\n",
      "   ğŸ“Š ê²€ì¦ ì •í™•ë„: 0.6444\n",
      "   ğŸ“‰ ê³¼ì í•© ì •ë„: 0.1806\n",
      "   ğŸ”´ ê³¼ì í•© ìƒíƒœ: ì£¼ì˜ (> 0.1)\n",
      "   âœ… Q1 ì™„ë£Œ!\n",
      "\n",
      "ğŸ”„ Q2 GPU CatBoost ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
      "0:\tlearn: 0.6391483\ttest: 0.4449067\tbest: 0.4449067 (0)\ttotal: 54.5ms\tremaining: 1m 48s\n",
      "50:\tlearn: 0.9975186\ttest: 0.4957014\tbest: 0.5702493 (17)\ttotal: 2.78s\tremaining: 1m 46s\n",
      "100:\tlearn: 1.0000000\ttest: 0.5128154\tbest: 0.5702493 (17)\ttotal: 5.53s\tremaining: 1m 43s\n",
      "bestTest = 0.5702492677\n",
      "bestIteration = 17\n",
      "Shrink model to first 18 iterations.\n",
      "   ğŸ“ˆ í›ˆë ¨ ì •í™•ë„: 0.9750\n",
      "   ğŸ“Š ê²€ì¦ ì •í™•ë„: 0.5889\n",
      "   ğŸ“‰ ê³¼ì í•© ì •ë„: 0.3861\n",
      "   ğŸ”´ ê³¼ì í•© ìƒíƒœ: ì£¼ì˜ (> 0.1)\n",
      "   âœ… Q2 ì™„ë£Œ!\n",
      "\n",
      "ğŸ”„ Q3 GPU CatBoost ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
      "0:\tlearn: 0.7770701\ttest: 0.5925926\tbest: 0.5925926 (0)\ttotal: 55.5ms\tremaining: 1m 50s\n",
      "50:\tlearn: 1.0000000\ttest: 0.6225681\tbest: 0.6419753 (18)\ttotal: 2.8s\tremaining: 1m 46s\n",
      "100:\tlearn: 1.0000000\ttest: 0.5937500\tbest: 0.6436782 (53)\ttotal: 5.52s\tremaining: 1m 43s\n",
      "150:\tlearn: 1.0000000\ttest: 0.6117647\tbest: 0.6436782 (53)\ttotal: 8.25s\tremaining: 1m 41s\n",
      "bestTest = 0.6436781569\n",
      "bestIteration = 53\n",
      "Shrink model to first 54 iterations.\n",
      "   ğŸ“ˆ í›ˆë ¨ ì •í™•ë„: 1.0000\n",
      "   ğŸ“Š ê²€ì¦ ì •í™•ë„: 0.6111\n",
      "   ğŸ“‰ ê³¼ì í•© ì •ë„: 0.3889\n",
      "   ğŸ”´ ê³¼ì í•© ìƒíƒœ: ì£¼ì˜ (> 0.1)\n",
      "   âœ… Q3 ì™„ë£Œ!\n",
      "\n",
      "ğŸ”„ S2 GPU CatBoost ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
      "0:\tlearn: 0.7194450\ttest: 0.5989677\tbest: 0.5989677 (0)\ttotal: 57.1ms\tremaining: 1m 54s\n",
      "50:\tlearn: 0.9957082\ttest: 0.6328822\tbest: 0.6350369 (2)\ttotal: 2.78s\tremaining: 1m 46s\n",
      "100:\tlearn: 1.0000000\ttest: 0.6072662\tbest: 0.6431055 (74)\ttotal: 5.5s\tremaining: 1m 43s\n",
      "150:\tlearn: 1.0000000\ttest: 0.6172625\tbest: 0.6453528 (110)\ttotal: 8.22s\tremaining: 1m 40s\n",
      "200:\tlearn: 1.0000000\ttest: 0.6271163\tbest: 0.6453528 (110)\ttotal: 11s\tremaining: 1m 38s\n",
      "bestTest = 0.6453528309\n",
      "bestIteration = 110\n",
      "Shrink model to first 111 iterations.\n",
      "   ğŸ“ˆ í›ˆë ¨ ì •í™•ë„: 1.0000\n",
      "   ğŸ“Š ê²€ì¦ ì •í™•ë„: 0.6333\n",
      "   ğŸ“‰ ê³¼ì í•© ì •ë„: 0.3667\n",
      "   ğŸ”´ ê³¼ì í•© ìƒíƒœ: ì£¼ì˜ (> 0.1)\n",
      "   âœ… S2 ì™„ë£Œ!\n",
      "\n",
      "ğŸ”„ S3 GPU CatBoost ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
      "0:\tlearn: 0.9203163\ttest: 0.8146528\tbest: 0.8146528 (0)\ttotal: 55.3ms\tremaining: 1m 50s\n",
      "50:\tlearn: 0.9862610\ttest: 0.8345505\tbest: 0.8890866 (30)\ttotal: 2.81s\tremaining: 1m 47s\n",
      "100:\tlearn: 1.0000000\ttest: 0.8086839\tbest: 0.8890866 (30)\ttotal: 5.53s\tremaining: 1m 44s\n",
      "bestTest = 0.8890866247\n",
      "bestIteration = 30\n",
      "Shrink model to first 31 iterations.\n",
      "   ğŸ“ˆ í›ˆë ¨ ì •í™•ë„: 0.8944\n",
      "   ğŸ“Š ê²€ì¦ ì •í™•ë„: 0.7222\n",
      "   ğŸ“‰ ê³¼ì í•© ì •ë„: 0.1722\n",
      "   ğŸ”´ ê³¼ì í•© ìƒíƒœ: ì£¼ì˜ (> 0.1)\n",
      "   âœ… S3 ì™„ë£Œ!\n",
      "\n",
      "ğŸ¯ ëª¨ë“  ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!\n",
      "ğŸ“Š í›ˆë ¨ëœ ëª¨ë¸ ìˆ˜: 5\n",
      "\n",
      "ğŸ“‹ í›ˆë ¨ ìš”ì•½ ë¦¬í¬íŠ¸:\n",
      "   Q1: 2/2000 ì´í„°ë ˆì´ì…˜ì—ì„œ ì¡°ê¸° ì¢…ë£Œ\n",
      "   Q2: 17/2000 ì´í„°ë ˆì´ì…˜ì—ì„œ ì¡°ê¸° ì¢…ë£Œ\n",
      "   Q3: 53/2000 ì´í„°ë ˆì´ì…˜ì—ì„œ ì¡°ê¸° ì¢…ë£Œ\n",
      "   S2: 110/2000 ì´í„°ë ˆì´ì…˜ì—ì„œ ì¡°ê¸° ì¢…ë£Œ\n",
      "   S3: 30/2000 ì´í„°ë ˆì´ì…˜ì—ì„œ ì¡°ê¸° ì¢…ë£Œ\n",
      "\n",
      "ğŸ“Š ì „ì²´ í†µê³„:\n",
      "   ğŸ“ˆ ì´ ëª¨ë¸ ìˆ˜: 5\n",
      "   â¹ï¸  ì¡°ê¸° ì¢…ë£Œëœ ëª¨ë¸: 5/5 (100.0%)\n",
      "   ğŸ”„ í‰ê·  ì´í„°ë ˆì´ì…˜: 42.4\n",
      "   âš ï¸  í‰ê·  ì´í„°ë ˆì´ì…˜ì´ ë‚®ìŠµë‹ˆë‹¤. íŒŒë¼ë¯¸í„° ì¡°ì •ì„ ê³ ë ¤í•˜ì„¸ìš”.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ğŸ“Š 3ë‹¨ê³„: GPU CatBoost ê¸°ë³¸ ëª¨ë¸ í•™ìŠµ (ê³¼ì í•© ë°©ì§€ ë²„ì „)\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"\\nğŸ“Š 3ë‹¨ê³„: GPU CatBoost ê¸°ë³¸ ëª¨ë¸ í•™ìŠµ (ê³¼ì í•© ë°©ì§€ ë²„ì „)\")\n",
    "\n",
    "# ì´ì§„ ë¶„ë¥˜\n",
    "binary_preds_selected = {}\n",
    "binary_models_selected = {}\n",
    "\n",
    "for col in targets_binary:\n",
    "    print(f\"\\nğŸ”„ {col} GPU CatBoost ëª¨ë¸ í›ˆë ¨ ì¤‘...\")\n",
    "    y = train_df[col]\n",
    "    \n",
    "    # í›ˆë ¨/ê²€ì¦ ë°ì´í„° ë¶„í•  (ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•´ í•„ìˆ˜!)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_selected, y, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=y  # í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€\n",
    "    )\n",
    "    \n",
    "    # ê³¼ì í•© ë°©ì§€ GPU íŒŒë¼ë¯¸í„°\n",
    "    binary_params = gpu_catboost_params.copy()\n",
    "    \n",
    "    # íŒŒë¼ë¯¸í„° ì¶©ëŒ ë°©ì§€: ë™ì˜ì–´ íŒŒë¼ë¯¸í„°ë“¤ ì •ë¦¬\n",
    "    conflict_params = {\n",
    "        'max_depth': 'depth',           # max_depth ëŒ€ì‹  depth ì‚¬ìš©\n",
    "        'max_bin': 'border_count',      # max_bin ëŒ€ì‹  border_count ì‚¬ìš©  \n",
    "        'colsample_bylevel': 'rsm'      # colsample_bylevel ëŒ€ì‹  rsm ì‚¬ìš©\n",
    "    }\n",
    "    \n",
    "    # ì¶©ëŒí•˜ëŠ” íŒŒë¼ë¯¸í„° ì œê±°\n",
    "    for new_param, old_param in conflict_params.items():\n",
    "        if new_param in binary_params:\n",
    "            del binary_params[new_param]\n",
    "    \n",
    "    # Bootstrap íƒ€ì… í˜¸í™˜ì„± í•´ê²° (ë‹¨ìˆœí™”)\n",
    "    # subsample ê´€ë ¨ íŒŒë¼ë¯¸í„°ë“¤ ì •ë¦¬\n",
    "    bootstrap_related = ['subsample', 'bagging_temperature', 'bootstrap_type']\n",
    "    for param in bootstrap_related:\n",
    "        binary_params.pop(param, None)  # ëª¨ë“  bootstrap ê´€ë ¨ íŒŒë¼ë¯¸í„° ì œê±°\n",
    "    \n",
    "    # ê· í˜•ì¡íŒ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ (bootstrap ë¬¸ì œ íšŒí”¼)\n",
    "    binary_params.update({\n",
    "        'loss_function': 'Logloss',\n",
    "        'eval_metric': 'F1',\n",
    "        'early_stopping_rounds': 100,   # 100íšŒë¡œ ì¦ê°€ (ë” ë§ì€ ê¸°íšŒ)\n",
    "        'use_best_model': True,         # ìµœì  ëª¨ë¸ ì‚¬ìš©\n",
    "        'verbose': 50,                  # 50íšŒë§ˆë‹¤ ì¶œë ¥\n",
    "        'learning_rate': 0.1,           # í•™ìŠµë¥  ì¦ê°€\n",
    "        \n",
    "        # ëœ aggressiveí•œ ì •ê·œí™”\n",
    "        'l2_leaf_reg': 3,              # 10ì—ì„œ 3ìœ¼ë¡œ ê°ì†Œ\n",
    "        'min_data_in_leaf': 5,         # 10ì—ì„œ 5ë¡œ ê°ì†Œ\n",
    "        # bootstrap_typeì€ ê¸°ë³¸ê°’ ì‚¬ìš© (Bayesian)\n",
    "    })\n",
    "    \n",
    "    # ê¸°ì¡´ íŒŒë¼ë¯¸í„° ëœ ì œí•œì ìœ¼ë¡œ ì¡°ì •\n",
    "    if 'depth' in binary_params:\n",
    "        binary_params['depth'] = min(binary_params['depth'], 8)  # 6ì—ì„œ 8ë¡œ ì¦ê°€\n",
    "    if 'border_count' in binary_params:\n",
    "        binary_params['border_count'] = min(binary_params['border_count'], 254)  # ê¸°ë³¸ê°’ ìœ ì§€\n",
    "    if 'rsm' in binary_params:\n",
    "        binary_params['rsm'] = min(binary_params['rsm'], 0.9)  # 0.8ì—ì„œ 0.9ë¡œ ì¦ê°€\n",
    "    \n",
    "    # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©\n",
    "    if col in class_weights:\n",
    "        binary_params['class_weights'] = list(class_weights[col].values())\n",
    "    \n",
    "    try:\n",
    "        model = CatBoostClassifier(**binary_params)\n",
    "        \n",
    "        # ê²€ì¦ ë°ì´í„°ì™€ í•¨ê»˜ í›ˆë ¨ (ê³¼ì í•© ë°©ì§€ì˜ í•µì‹¬!)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=(X_val, y_val),  # ê²€ì¦ ì„¸íŠ¸ í•„ìˆ˜!\n",
    "            plot=False  # Jupyterì—ì„œ ê·¸ë˜í”„ ë¹„í™œì„±í™”\n",
    "        )\n",
    "        \n",
    "        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡\n",
    "        binary_preds_selected[col] = model.predict(test_X_selected)\n",
    "        binary_models_selected[col] = model\n",
    "        \n",
    "        # ìµœì¢… í›ˆë ¨/ê²€ì¦ ì„±ëŠ¥ ì¶œë ¥\n",
    "        train_score = model.score(X_train, y_train)\n",
    "        val_score = model.score(X_val, y_val)\n",
    "        overfitting = train_score - val_score\n",
    "        \n",
    "        print(f\"   ğŸ“ˆ í›ˆë ¨ ì •í™•ë„: {train_score:.4f}\")\n",
    "        print(f\"   ğŸ“Š ê²€ì¦ ì •í™•ë„: {val_score:.4f}\")\n",
    "        print(f\"   ğŸ“‰ ê³¼ì í•© ì •ë„: {overfitting:.4f}\")\n",
    "        \n",
    "        # ê³¼ì í•© ìƒíƒœ í‰ê°€\n",
    "        if overfitting < 0.05:\n",
    "            print(f\"   ğŸŸ¢ ê³¼ì í•© ìƒíƒœ: ì–‘í˜¸ (< 0.05)\")\n",
    "        elif overfitting < 0.1:\n",
    "            print(f\"   ğŸŸ¡ ê³¼ì í•© ìƒíƒœ: ë³´í†µ (0.05-0.1)\")\n",
    "        else:\n",
    "            print(f\"   ğŸ”´ ê³¼ì í•© ìƒíƒœ: ì£¼ì˜ (> 0.1)\")\n",
    "            \n",
    "        print(f\"   âœ… {col} ì™„ë£Œ!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ {col} GPU í›ˆë ¨ ì‹¤íŒ¨: {str(e)}\")\n",
    "        # GPU ì‹¤íŒ¨ì‹œ CPUë¡œ fallback\n",
    "        print(f\"   ğŸ”„ {col} CPUë¡œ ì¬ì‹œë„...\")\n",
    "        \n",
    "        cpu_params = binary_params.copy()\n",
    "        cpu_params['task_type'] = 'CPU'\n",
    "        cpu_params.pop('devices', None)\n",
    "        cpu_params.pop('gpu_ram_part', None)\n",
    "        \n",
    "        # CPUì—ì„œë„ bootstrap ê´€ë ¨ íŒŒë¼ë¯¸í„° ì •ë¦¬\n",
    "        bootstrap_related = ['subsample', 'bagging_temperature', 'bootstrap_type']\n",
    "        for param in bootstrap_related:\n",
    "            cpu_params.pop(param, None)\n",
    "        \n",
    "        # CPUì—ì„œë„ ë™ì¼í•œ íŒŒë¼ë¯¸í„° ì¡°ì • (ëœ ì œí•œì )\n",
    "        if 'depth' in cpu_params:\n",
    "            cpu_params['depth'] = min(cpu_params['depth'], 8)  # 8ë¡œ ì¦ê°€\n",
    "        if 'border_count' in cpu_params:\n",
    "            cpu_params['border_count'] = min(cpu_params['border_count'], 254)  # ê¸°ë³¸ê°’ ìœ ì§€\n",
    "        if 'rsm' in cpu_params:\n",
    "            cpu_params['rsm'] = min(cpu_params['rsm'], 0.9)  # 0.9ë¡œ ì¦ê°€\n",
    "        \n",
    "        model = CatBoostClassifier(**cpu_params)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=(X_val, y_val),\n",
    "            plot=False\n",
    "        )\n",
    "        \n",
    "        binary_preds_selected[col] = model.predict(test_X_selected)\n",
    "        binary_models_selected[col] = model\n",
    "        \n",
    "        # CPU ì„±ëŠ¥ë„ ì¶œë ¥\n",
    "        train_score = model.score(X_train, y_train)\n",
    "        val_score = model.score(X_val, y_val)\n",
    "        overfitting = train_score - val_score\n",
    "        \n",
    "        print(f\"   ğŸ“ˆ í›ˆë ¨ ì •í™•ë„: {train_score:.4f}\")\n",
    "        print(f\"   ğŸ“Š ê²€ì¦ ì •í™•ë„: {val_score:.4f}\")\n",
    "        print(f\"   ğŸ“‰ ê³¼ì í•© ì •ë„: {overfitting:.4f}\")\n",
    "        \n",
    "        # ê³¼ì í•© ìƒíƒœ í‰ê°€\n",
    "        if overfitting < 0.05:\n",
    "            print(f\"   ğŸŸ¢ ê³¼ì í•© ìƒíƒœ: ì–‘í˜¸ (< 0.05)\")\n",
    "        elif overfitting < 0.1:\n",
    "            print(f\"   ğŸŸ¡ ê³¼ì í•© ìƒíƒœ: ë³´í†µ (0.05-0.1)\")\n",
    "        else:\n",
    "            print(f\"   ğŸ”´ ê³¼ì í•© ìƒíƒœ: ì£¼ì˜ (> 0.1)\")\n",
    "            \n",
    "        print(f\"   âœ… {col} CPUë¡œ ì™„ë£Œ!\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ëª¨ë“  ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“Š í›ˆë ¨ëœ ëª¨ë¸ ìˆ˜: {len(binary_models_selected)}\")\n",
    "\n",
    "# ê³¼ì í•© ìš”ì•½ ë¦¬í¬íŠ¸\n",
    "print(f\"\\nğŸ“‹ í›ˆë ¨ ìš”ì•½ ë¦¬í¬íŠ¸:\")\n",
    "total_models = len(binary_models_selected)\n",
    "early_stopped = 0\n",
    "avg_iterations = 0\n",
    "\n",
    "for col in binary_models_selected.keys():\n",
    "    model = binary_models_selected[col]\n",
    "    best_iteration = model.get_best_iteration()\n",
    "    total_iterations = model.get_param('iterations')\n",
    "    \n",
    "    if best_iteration < total_iterations - 1:\n",
    "        early_stopped += 1\n",
    "    avg_iterations += best_iteration\n",
    "    \n",
    "    print(f\"   {col}: {best_iteration}/{total_iterations} ì´í„°ë ˆì´ì…˜ì—ì„œ ì¡°ê¸° ì¢…ë£Œ\")\n",
    "\n",
    "avg_iterations = avg_iterations / total_models if total_models > 0 else 0\n",
    "\n",
    "print(f\"\\nğŸ“Š ì „ì²´ í†µê³„:\")\n",
    "print(f\"   ğŸ“ˆ ì´ ëª¨ë¸ ìˆ˜: {total_models}\")\n",
    "print(f\"   â¹ï¸  ì¡°ê¸° ì¢…ë£Œëœ ëª¨ë¸: {early_stopped}/{total_models} ({early_stopped/total_models*100:.1f}%)\")\n",
    "print(f\"   ğŸ”„ í‰ê·  ì´í„°ë ˆì´ì…˜: {avg_iterations:.1f}\")\n",
    "\n",
    "if avg_iterations < 100:\n",
    "    print(f\"   âš ï¸  í‰ê·  ì´í„°ë ˆì´ì…˜ì´ ë‚®ìŠµë‹ˆë‹¤. íŒŒë¼ë¯¸í„° ì¡°ì •ì„ ê³ ë ¤í•˜ì„¸ìš”.\")\n",
    "elif avg_iterations > 1000:\n",
    "    print(f\"   âš ï¸  í‰ê·  ì´í„°ë ˆì´ì…˜ì´ ë†’ìŠµë‹ˆë‹¤. ì •ê·œí™” ê°•í™”ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.\")\n",
    "else:\n",
    "    print(f\"   âœ… ì ì ˆí•œ ì´í„°ë ˆì´ì…˜ ë²”ìœ„ì…ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aeb36c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ S1 GPU CatBoost ë‹¤ì¤‘ë¶„ë¥˜ ëª¨ë¸ í›ˆë ¨ ì¤‘... (ìˆ˜ì •ëœ ë²„ì „)\n",
      "   ğŸ“Š S1 í´ë˜ìŠ¤ ë¶„í¬: {0: 143, 1: 224, 2: 83}\n",
      "   ğŸ“Š ê³ ìœ  í´ë˜ìŠ¤ ìˆ˜: 3\n",
      "   ğŸ”„ GPUë¡œ S1 ë‹¤ì¤‘ë¶„ë¥˜ ì‹œë„ ì¤‘...\n",
      "0:\tlearn: 1.0927730\ttotal: 8.63ms\tremaining: 17.2s\n",
      "100:\tlearn: 0.7104279\ttotal: 758ms\tremaining: 14.3s\n",
      "200:\tlearn: 0.5119554\ttotal: 1.48s\tremaining: 13.3s\n",
      "300:\tlearn: 0.3904311\ttotal: 2.18s\tremaining: 12.3s\n",
      "400:\tlearn: 0.3106925\ttotal: 2.87s\tremaining: 11.4s\n",
      "500:\tlearn: 0.2554644\ttotal: 3.54s\tremaining: 10.6s\n",
      "600:\tlearn: 0.2151540\ttotal: 4.2s\tremaining: 9.78s\n",
      "700:\tlearn: 0.1835583\ttotal: 4.86s\tremaining: 9s\n",
      "800:\tlearn: 0.1592402\ttotal: 5.51s\tremaining: 8.25s\n",
      "900:\tlearn: 0.1390363\ttotal: 6.18s\tremaining: 7.54s\n",
      "1000:\tlearn: 0.1231827\ttotal: 6.84s\tremaining: 6.83s\n",
      "1100:\tlearn: 0.1099216\ttotal: 7.51s\tremaining: 6.13s\n",
      "1200:\tlearn: 0.0988094\ttotal: 8.17s\tremaining: 5.43s\n",
      "1300:\tlearn: 0.0895743\ttotal: 8.82s\tremaining: 4.74s\n",
      "1400:\tlearn: 0.0817195\ttotal: 9.49s\tremaining: 4.06s\n",
      "1500:\tlearn: 0.0748249\ttotal: 10.2s\tremaining: 3.38s\n",
      "1600:\tlearn: 0.0688636\ttotal: 10.8s\tremaining: 2.7s\n",
      "1700:\tlearn: 0.0636862\ttotal: 11.5s\tremaining: 2.02s\n",
      "1800:\tlearn: 0.0591061\ttotal: 12.2s\tremaining: 1.34s\n",
      "1900:\tlearn: 0.0549546\ttotal: 12.8s\tremaining: 668ms\n",
      "1999:\tlearn: 0.0515080\ttotal: 13.5s\tremaining: 0us\n",
      "   âœ… S1 GPU ì™„ë£Œ!\n",
      "\n",
      "ğŸ” ì˜ˆì¸¡ ê²°ê³¼ ì°¨ì› ë¶„ì„:\n",
      "   ì›ë³¸ shape: (250, 1)\n",
      "   ì›ë³¸ type: <class 'numpy.ndarray'>\n",
      "   ğŸ”§ 2ì°¨ì› â†’ 1ì°¨ì› ë³€í™˜ ì¤‘...\n",
      "   ë³€í™˜ í›„ shape: (250,)\n",
      "\n",
      "ğŸ“Š S1 ì˜ˆì¸¡ ê²°ê³¼:\n",
      "   ì˜ˆì¸¡ ë¶„í¬: {0: 82, 1: 147, 2: 21}\n",
      "   ì˜ˆì¸¡ ìƒ˜í”Œ ìˆ˜: 250\n",
      "   í›ˆë ¨ ë¶„í¬: {0: 143, 1: 224, 2: 83}\n",
      "   ê³ ìœ  ì˜ˆì¸¡ í´ë˜ìŠ¤: 3/3\n",
      "   âœ… ëª¨ë“  í´ë˜ìŠ¤ ì˜ˆì¸¡ë¨\n",
      "\n",
      "âœ… S1 ë‹¤ì¤‘ë¶„ë¥˜ ì²˜ë¦¬ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ğŸ”§ S1 ë‹¤ì¤‘ë¶„ë¥˜ (ìˆ˜ì •ëœ ë²„ì „) - íŒŒë¼ë¯¸í„° ì¶©ëŒ ë° ì°¨ì› ë¬¸ì œ í•´ê²°\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nğŸ”„ S1 GPU CatBoost ë‹¤ì¤‘ë¶„ë¥˜ ëª¨ë¸ í›ˆë ¨ ì¤‘... (ìˆ˜ì •ëœ ë²„ì „)\")\n",
    "\n",
    "y_multi = train_df['S1']\n",
    "\n",
    "# ë³€ìˆ˜ ì´ˆê¸°í™” (ì—ëŸ¬ ë°©ì§€)\n",
    "multiclass_pred_selected = None\n",
    "model_s1_selected = None\n",
    "\n",
    "# ğŸ”§ ìˆ˜ì •ëœ ë‹¤ì¤‘ë¶„ë¥˜ íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "multiclass_params = gpu_catboost_params.copy()\n",
    "multiclass_params['loss_function'] = 'MultiClass'\n",
    "# ğŸ”¥ eval_metricì„ ì œê±°í•˜ê±°ë‚˜ í˜¸í™˜ë˜ëŠ” ê²ƒìœ¼ë¡œ ë³€ê²½\n",
    "multiclass_params['eval_metric'] = 'MultiClass'  # ë˜ëŠ” 'TotalF1'\n",
    "# ğŸ”¥ classes_count ì œê±° (ìë™ ê°ì§€ë˜ë„ë¡)\n",
    "# multiclass_params['classes_count'] = len(y_multi.unique())  # ì´ ë¼ì¸ ì œê±°\n",
    "\n",
    "# í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©\n",
    "if 'S1' in class_weights:\n",
    "    multiclass_params['class_weights'] = list(class_weights['S1'].values())\n",
    "\n",
    "print(f\"   ğŸ“Š S1 í´ë˜ìŠ¤ ë¶„í¬: {dict(y_multi.value_counts().sort_index())}\")\n",
    "print(f\"   ğŸ“Š ê³ ìœ  í´ë˜ìŠ¤ ìˆ˜: {len(y_multi.unique())}\")\n",
    "\n",
    "try:\n",
    "    print(f\"   ğŸ”„ GPUë¡œ S1 ë‹¤ì¤‘ë¶„ë¥˜ ì‹œë„ ì¤‘...\")\n",
    "    model_s1_selected = CatBoostClassifier(**multiclass_params)\n",
    "    model_s1_selected.fit(X_selected, y_multi)\n",
    "    multiclass_pred_selected = model_s1_selected.predict(test_X_selected)\n",
    "    print(f\"   âœ… S1 GPU ì™„ë£Œ!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   âŒ S1 GPU í›ˆë ¨ ì‹¤íŒ¨: {str(e)}\")\n",
    "    # GPU ì‹¤íŒ¨ì‹œ CPUë¡œ fallback\n",
    "    print(f\"   ğŸ”„ S1 CPUë¡œ ì¬ì‹œë„...\")\n",
    "    \n",
    "    cpu_params = multiclass_params.copy()\n",
    "    cpu_params['task_type'] = 'CPU'\n",
    "    cpu_params.pop('devices', None)\n",
    "    cpu_params.pop('gpu_ram_part', None)\n",
    "    cpu_params.pop('feature_border_type', None)  # CPUì—ì„œëŠ” ì´ íŒŒë¼ë¯¸í„° ì œê±°\n",
    "    \n",
    "    try:\n",
    "        model_s1_selected = CatBoostClassifier(**cpu_params)\n",
    "        model_s1_selected.fit(X_selected, y_multi)\n",
    "        multiclass_pred_selected = model_s1_selected.predict(test_X_selected)\n",
    "        print(f\"   âœ… S1 CPUë¡œ ì™„ë£Œ!\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"   âŒ S1 CPUë„ ì‹¤íŒ¨: {str(e2)}\")\n",
    "        print(f\"   ğŸ”„ ìµœì†Œ ì„¤ì •ìœ¼ë¡œ ì¬ì‹œë„...\")\n",
    "        \n",
    "        # ğŸ”§ ìµœì†Œ ì„¤ì •ìœ¼ë¡œ ì¬ì‹œë„\n",
    "        minimal_params = {\n",
    "            'task_type': 'CPU',\n",
    "            'loss_function': 'MultiClass',\n",
    "            'iterations': 500,\n",
    "            'learning_rate': 0.1,\n",
    "            'random_seed': 42,\n",
    "            'verbose': False,\n",
    "            'allow_writing_files': False\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            model_s1_selected = CatBoostClassifier(**minimal_params)\n",
    "            model_s1_selected.fit(X_selected, y_multi)\n",
    "            multiclass_pred_selected = model_s1_selected.predict(test_X_selected)\n",
    "            print(f\"   âœ… S1 ìµœì†Œ ì„¤ì •ìœ¼ë¡œ ì™„ë£Œ!\")\n",
    "            \n",
    "        except Exception as e3:\n",
    "            print(f\"   âŒ ëª¨ë“  ì‹œë„ ì‹¤íŒ¨: {str(e3)}\")\n",
    "            print(f\"   ğŸ”„ ê¸°ë³¸ sklearn ë‹¤ì¤‘ë¶„ë¥˜ë¡œ ëŒ€ì²´...\")\n",
    "            \n",
    "            # ë§ˆì§€ë§‰ fallback: sklearn RandomForest\n",
    "            from sklearn.ensemble import RandomForestClassifier\n",
    "            rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            rf_model.fit(X_selected, y_multi)\n",
    "            multiclass_pred_selected = rf_model.predict(test_X_selected)\n",
    "            model_s1_selected = rf_model  # ë‚˜ì¤‘ì— ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì €ì¥\n",
    "            print(f\"   âœ… RandomForestë¡œ ëŒ€ì²´ ì™„ë£Œ!\")\n",
    "\n",
    "# ğŸ”§ ì°¨ì› ë¬¸ì œ í•´ê²°: ì˜ˆì¸¡ ê²°ê³¼ ê²€ì¦ ë° ë³€í™˜\n",
    "if multiclass_pred_selected is not None:\n",
    "    print(f\"\\nğŸ” ì˜ˆì¸¡ ê²°ê³¼ ì°¨ì› ë¶„ì„:\")\n",
    "    print(f\"   ì›ë³¸ shape: {multiclass_pred_selected.shape}\")\n",
    "    print(f\"   ì›ë³¸ type: {type(multiclass_pred_selected)}\")\n",
    "    \n",
    "    # 2ì°¨ì›ì¸ ê²½ìš° 1ì°¨ì›ìœ¼ë¡œ ë³€í™˜\n",
    "    if len(multiclass_pred_selected.shape) > 1:\n",
    "        print(f\"   ğŸ”§ 2ì°¨ì› â†’ 1ì°¨ì› ë³€í™˜ ì¤‘...\")\n",
    "        multiclass_pred_selected = multiclass_pred_selected.flatten()\n",
    "        print(f\"   ë³€í™˜ í›„ shape: {multiclass_pred_selected.shape}\")\n",
    "    \n",
    "    # ê²°ê³¼ í™•ì¸\n",
    "    print(f\"\\nğŸ“Š S1 ì˜ˆì¸¡ ê²°ê³¼:\")\n",
    "    try:\n",
    "        s1_pred_dist = dict(pd.Series(multiclass_pred_selected).value_counts().sort_index())\n",
    "        print(f\"   ì˜ˆì¸¡ ë¶„í¬: {s1_pred_dist}\")\n",
    "        print(f\"   ì˜ˆì¸¡ ìƒ˜í”Œ ìˆ˜: {len(multiclass_pred_selected)}\")\n",
    "        \n",
    "        # í›ˆë ¨ ë°ì´í„°ì™€ ë¹„êµ\n",
    "        train_dist = dict(y_multi.value_counts().sort_index())\n",
    "        print(f\"   í›ˆë ¨ ë¶„í¬: {train_dist}\")\n",
    "        \n",
    "        # ì˜ˆì¸¡ í†µê³„\n",
    "        unique_preds = len(set(multiclass_pred_selected))\n",
    "        unique_train = len(set(y_multi))\n",
    "        print(f\"   ê³ ìœ  ì˜ˆì¸¡ í´ë˜ìŠ¤: {unique_preds}/{unique_train}\")\n",
    "        \n",
    "        if unique_preds == unique_train:\n",
    "            print(f\"   âœ… ëª¨ë“  í´ë˜ìŠ¤ ì˜ˆì¸¡ë¨\")\n",
    "        else:\n",
    "            missing_classes = set(y_multi.unique()) - set(multiclass_pred_selected)\n",
    "            if missing_classes:\n",
    "                print(f\"   âš ï¸  ëˆ„ë½ëœ í´ë˜ìŠ¤: {missing_classes}\")\n",
    "        \n",
    "    except Exception as e4:\n",
    "        print(f\"   âŒ ê²°ê³¼ ë¶„ì„ ì‹¤íŒ¨: {str(e4)}\")\n",
    "        print(f\"   ğŸ”§ ì›ì‹œ ë°ì´í„° í™•ì¸:\")\n",
    "        print(f\"   ì˜ˆì¸¡ ê²°ê³¼ ìƒ˜í”Œ: {multiclass_pred_selected[:10]}\")\n",
    "        print(f\"   ë°ì´í„° íƒ€ì…: {type(multiclass_pred_selected[0]) if len(multiclass_pred_selected) > 0 else 'Empty'}\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\nâŒ ëª¨ë“  ë‹¤ì¤‘ë¶„ë¥˜ ì‹œë„ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤!\")\n",
    "    print(f\"   ğŸ”„ ê°„ë‹¨í•œ ë”ë¯¸ ë¶„ë¥˜ê¸°ë¡œ ëŒ€ì²´...\")\n",
    "    \n",
    "    # ìµœí›„ì˜ ìˆ˜ë‹¨: ê°€ì¥ ë¹ˆë²ˆí•œ í´ë˜ìŠ¤ë¡œ ì˜ˆì¸¡\n",
    "    most_frequent_class = y_multi.mode()[0]\n",
    "    multiclass_pred_selected = [most_frequent_class] * len(test_X_selected)\n",
    "    print(f\"   ğŸ“Š ë”ë¯¸ ì˜ˆì¸¡ (ê°€ì¥ ë¹ˆë²ˆí•œ í´ë˜ìŠ¤ {most_frequent_class}): {len(multiclass_pred_selected)}ê°œ\")\n",
    "\n",
    "print(f\"\\nâœ… S1 ë‹¤ì¤‘ë¶„ë¥˜ ì²˜ë¦¬ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "46d4b4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ”§ ë” ì•ˆì „í•œ ë‹¤ì¤‘ë¶„ë¥˜ ì„¤ì • í•¨ìˆ˜\n",
    "# =============================================================================\n",
    "\n",
    "def create_safe_multiclass_catboost(use_gpu=True, simple_mode=False):\n",
    "    \"\"\"\n",
    "    ì•ˆì „í•œ ë‹¤ì¤‘ë¶„ë¥˜ CatBoost ëª¨ë¸ ìƒì„±\n",
    "    \"\"\"\n",
    "    if simple_mode:\n",
    "        # ğŸ”§ ê°€ì¥ ì•ˆì „í•œ ì„¤ì •\n",
    "        params = {\n",
    "            'task_type': 'GPU' if use_gpu else 'CPU',\n",
    "            'loss_function': 'MultiClass',\n",
    "            'iterations': 1000,\n",
    "            'learning_rate': 0.05,\n",
    "            'depth': 6,\n",
    "            'random_seed': 42,\n",
    "            'verbose': False,\n",
    "            'allow_writing_files': False\n",
    "        }\n",
    "        \n",
    "        if use_gpu:\n",
    "            params.update({\n",
    "                'devices': '0',\n",
    "                'gpu_ram_part': 0.6,  # ë” ë³´ìˆ˜ì ìœ¼ë¡œ 60%\n",
    "            })\n",
    "        \n",
    "    else:\n",
    "        # ğŸ”§ ê³ ì„±ëŠ¥ ì„¤ì • (ì•ˆì „ ì¥ì¹˜ í¬í•¨)\n",
    "        params = {\n",
    "            'task_type': 'GPU' if use_gpu else 'CPU',\n",
    "            'loss_function': 'MultiClass',\n",
    "            'eval_metric': 'MultiClass',  # loss_functionê³¼ ì¼ì¹˜\n",
    "            'iterations': 1500,\n",
    "            'learning_rate': 0.03,\n",
    "            'depth': 8,\n",
    "            'l2_leaf_reg': 3,\n",
    "            'border_count': 128,\n",
    "            'random_seed': 42,\n",
    "            'verbose': False,\n",
    "            'allow_writing_files': False\n",
    "        }\n",
    "        \n",
    "        if use_gpu:\n",
    "            params.update({\n",
    "                'devices': '0',\n",
    "                'gpu_ram_part': 0.7,\n",
    "                'feature_border_type': 'GreedyLogSum'\n",
    "            })\n",
    "    \n",
    "    return CatBoostClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "20d1d732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š 4ë‹¨ê³„: GPU CatBoost Feature Importance ë¶„ì„\n",
      "   âœ… Q1: ìƒìœ„ 15ê°œ íŠ¹ì„± ì €ì¥\n",
      "   âœ… Q2: ìƒìœ„ 15ê°œ íŠ¹ì„± ì €ì¥\n",
      "   âœ… Q3: ìƒìœ„ 15ê°œ íŠ¹ì„± ì €ì¥\n",
      "   âœ… S2: ìƒìœ„ 15ê°œ íŠ¹ì„± ì €ì¥\n",
      "   âœ… S3: ìƒìœ„ 15ê°œ íŠ¹ì„± ì €ì¥\n",
      "   âœ… S1: ìƒìœ„ 25ê°œ íŠ¹ì„± ì €ì¥\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ğŸ“Š 4ë‹¨ê³„: Feature Importance ë¶„ì„ (ìˆ˜ì •ëœ ë²„ì „)\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nğŸ“Š 4ë‹¨ê³„: GPU CatBoost Feature Importance ë¶„ì„\")\n",
    "\n",
    "# Feature Importance ì €ì¥\n",
    "feature_importance_dict = {}\n",
    "\n",
    "# ì´ì§„ë¶„ë¥˜ ëª¨ë¸ë“¤\n",
    "for col_name, model in binary_models_selected.items():\n",
    "    try:\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': X_selected.columns,\n",
    "            'importance': model.get_feature_importance()\n",
    "        }).sort_values('importance', ascending=False).head(15)  # ìƒìœ„ 15ê°œ\n",
    "        \n",
    "        feature_importance_dict[col_name] = importance_df.copy()\n",
    "        print(f\"   âœ… {col_name}: ìƒìœ„ 15ê°œ íŠ¹ì„± ì €ì¥\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ {col_name} Feature Importance ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}\")\n",
    "\n",
    "# S1 ë‹¤ì¤‘ë¶„ë¥˜\n",
    "try:\n",
    "    if hasattr(model_s1_selected, 'get_feature_importance'):  # CatBoost ëª¨ë¸ì¸ ê²½ìš°\n",
    "        fi_multi_25 = pd.DataFrame({\n",
    "            'feature': X_selected.columns,\n",
    "            'importance': model_s1_selected.get_feature_importance()\n",
    "        }).sort_values('importance', ascending=False).head(25)\n",
    "    else:  # sklearn ëª¨ë¸ì¸ ê²½ìš° (RandomForest ë“±)\n",
    "        fi_multi_25 = pd.DataFrame({\n",
    "            'feature': X_selected.columns,\n",
    "            'importance': model_s1_selected.feature_importances_\n",
    "        }).sort_values('importance', ascending=False).head(25)\n",
    "    \n",
    "    feature_importance_dict['S1'] = fi_multi_25.copy()\n",
    "    print(f\"   âœ… S1: ìƒìœ„ 25ê°œ íŠ¹ì„± ì €ì¥\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   âš ï¸ S1 Feature Importance ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}\")\n",
    "    # ê¸°ë³¸ê°’ìœ¼ë¡œ ì²˜ë¦¬\n",
    "    feature_importance_dict['S1'] = pd.DataFrame({\n",
    "        'feature': X_selected.columns[:25],\n",
    "        'importance': [1.0] * min(25, len(X_selected.columns))\n",
    "    })\n",
    "    print(f\"   ğŸ”„ S1: ê¸°ë³¸ íŠ¹ì„± ìˆœì„œë¡œ ëŒ€ì²´\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eb5fdf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ“Š 5ë‹¨ê³„: ë‹¤ì¤‘ë¶„ë¥˜ ëª¨ë¸ ì¬í›ˆë ¨ (ë” ì•ˆì „í•œ ë°©ë²•)\n",
    "# =============================================================================\n",
    "\n",
    "def retrain_s1_model_safe():\n",
    "    \"\"\"\n",
    "    S1 ë‹¤ì¤‘ë¶„ë¥˜ ëª¨ë¸ì„ ë” ì•ˆì „í•˜ê²Œ ì¬í›ˆë ¨\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ“Š 5ë‹¨ê³„: S1 ë‹¤ì¤‘ë¶„ë¥˜ ëª¨ë¸ ì•ˆì „ ì¬í›ˆë ¨\")\n",
    "    \n",
    "    y_multi = train_df['S1']\n",
    "    \n",
    "    # ë°©ë²• 1: GPU ì‹œë„\n",
    "    try:\n",
    "        print(f\"   ğŸ”„ ë°©ë²• 1: GPU ê°„ë‹¨ ì„¤ì •ìœ¼ë¡œ ì‹œë„...\")\n",
    "        model = create_safe_multiclass_catboost(use_gpu=True, simple_mode=True)\n",
    "        model.fit(X_selected, y_multi)\n",
    "        predictions = model.predict(test_X_selected)\n",
    "        print(f\"   âœ… ë°©ë²• 1 ì„±ê³µ: GPU ê°„ë‹¨ ì„¤ì •\")\n",
    "        return model, predictions\n",
    "        \n",
    "    except Exception as e1:\n",
    "        print(f\"   âŒ ë°©ë²• 1 ì‹¤íŒ¨: {str(e1)}\")\n",
    "        \n",
    "        # ë°©ë²• 2: CPU ê³ ì„±ëŠ¥ ì„¤ì • ì‹œë„\n",
    "        try:\n",
    "            print(f\"   ğŸ”„ ë°©ë²• 2: CPU ê³ ì„±ëŠ¥ ì„¤ì •ìœ¼ë¡œ ì‹œë„...\")\n",
    "            model = create_safe_multiclass_catboost(use_gpu=False, simple_mode=False)\n",
    "            model.fit(X_selected, y_multi)\n",
    "            predictions = model.predict(test_X_selected)\n",
    "            print(f\"   âœ… ë°©ë²• 2 ì„±ê³µ: CPU ê³ ì„±ëŠ¥ ì„¤ì •\")\n",
    "            return model, predictions\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"   âŒ ë°©ë²• 2 ì‹¤íŒ¨: {str(e2)}\")\n",
    "            \n",
    "            # ë°©ë²• 3: CPU ê°„ë‹¨ ì„¤ì • ì‹œë„\n",
    "            try:\n",
    "                print(f\"   ğŸ”„ ë°©ë²• 3: CPU ê°„ë‹¨ ì„¤ì •ìœ¼ë¡œ ì‹œë„...\")\n",
    "                model = create_safe_multiclass_catboost(use_gpu=False, simple_mode=True)\n",
    "                model.fit(X_selected, y_multi)\n",
    "                predictions = model.predict(test_X_selected)\n",
    "                print(f\"   âœ… ë°©ë²• 3 ì„±ê³µ: CPU ê°„ë‹¨ ì„¤ì •\")\n",
    "                return model, predictions\n",
    "                \n",
    "            except Exception as e3:\n",
    "                print(f\"   âŒ ë°©ë²• 3 ì‹¤íŒ¨: {str(e3)}\")\n",
    "                \n",
    "                # ë°©ë²• 4: XGBoost ëŒ€ì²´\n",
    "                try:\n",
    "                    print(f\"   ğŸ”„ ë°©ë²• 4: XGBoostë¡œ ëŒ€ì²´...\")\n",
    "                    from xgboost import XGBClassifier\n",
    "                    model = XGBClassifier(\n",
    "                        objective='multi:softprob',\n",
    "                        n_estimators=1000,\n",
    "                        learning_rate=0.05,\n",
    "                        max_depth=6,\n",
    "                        random_state=42,\n",
    "                        verbosity=0\n",
    "                    )\n",
    "                    model.fit(X_selected, y_multi)\n",
    "                    predictions = model.predict(test_X_selected)\n",
    "                    print(f\"   âœ… ë°©ë²• 4 ì„±ê³µ: XGBoost ëŒ€ì²´\")\n",
    "                    return model, predictions\n",
    "                    \n",
    "                except Exception as e4:\n",
    "                    print(f\"   âŒ ë°©ë²• 4 ì‹¤íŒ¨: {str(e4)}\")\n",
    "                    \n",
    "                    # ë°©ë²• 5: RandomForest (ìµœì¢… fallback)\n",
    "                    print(f\"   ğŸ”„ ë°©ë²• 5: RandomForest (ìµœì¢… ëŒ€ì²´)...\")\n",
    "                    from sklearn.ensemble import RandomForestClassifier\n",
    "                    model = RandomForestClassifier(\n",
    "                        n_estimators=500,\n",
    "                        max_depth=10,\n",
    "                        random_state=42,\n",
    "                        n_jobs=-1\n",
    "                    )\n",
    "                    model.fit(X_selected, y_multi)\n",
    "                    predictions = model.predict(test_X_selected)\n",
    "                    print(f\"   âœ… ë°©ë²• 5 ì„±ê³µ: RandomForest ìµœì¢… ëŒ€ì²´\")\n",
    "                    return model, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "358a9617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š 5ë‹¨ê³„: S1 ë‹¤ì¤‘ë¶„ë¥˜ ëª¨ë¸ ì•ˆì „ ì¬í›ˆë ¨\n",
      "   ğŸ”„ ë°©ë²• 1: GPU ê°„ë‹¨ ì„¤ì •ìœ¼ë¡œ ì‹œë„...\n",
      "   âœ… ë°©ë²• 1 ì„±ê³µ: GPU ê°„ë‹¨ ì„¤ì •\n",
      "\n",
      "ğŸ” S1 ì˜ˆì¸¡ ê²°ê³¼ ì°¨ì› í™•ì¸:\n",
      "   ì›ë³¸ shape: (250, 1)\n",
      "   ì›ë³¸ type: <class 'numpy.ndarray'>\n",
      "   ğŸ”§ 2ì°¨ì› â†’ 1ì°¨ì› ë³€í™˜ ì¤‘...\n",
      "   ë³€í™˜ í›„ shape: (250,)\n",
      "   ğŸ“ listë¡œ ë³€í™˜ ì™„ë£Œ: 250ê°œ\n",
      "\n",
      "ğŸ“Š S1 ìµœì¢… ê²°ê³¼:\n",
      "   ìµœì¢… ì˜ˆì¸¡ ë¶„í¬: {0: 78, 1: 157, 2: 15}\n",
      "   ì˜ˆì¸¡ ìƒ˜í”Œ ìˆ˜: 250\n",
      "   ê³ ìœ  í´ë˜ìŠ¤ ìˆ˜: 3\n",
      "   ì‚¬ìš©ëœ ëª¨ë¸: CatBoostClassifier\n",
      "   ì˜ˆì¸¡ ê°’ ë²”ìœ„: 0 ~ 2\n",
      "   ìƒ˜í”Œ ì˜ˆì¸¡: [0, 1, 1, 1, 1, 1, 0, 0, 0, 1]\n",
      "\n",
      "âœ… 3-5ë‹¨ê³„ ì™„ë£Œ! ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# S1 ëª¨ë¸ ì¬í›ˆë ¨ ì‹¤í–‰ (ì°¨ì› ë¬¸ì œ í•´ê²°)\n",
    "try:\n",
    "    final_s1_model, final_s1_predictions = retrain_s1_model_safe()\n",
    "    \n",
    "    print(f\"\\nğŸ” S1 ì˜ˆì¸¡ ê²°ê³¼ ì°¨ì› í™•ì¸:\")\n",
    "    print(f\"   ì›ë³¸ shape: {final_s1_predictions.shape}\")\n",
    "    print(f\"   ì›ë³¸ type: {type(final_s1_predictions)}\")\n",
    "    \n",
    "    # ğŸ”§ ì°¨ì› ë¬¸ì œ í•´ê²°: 2ì°¨ì›ì´ë©´ 1ì°¨ì›ìœ¼ë¡œ ë³€í™˜\n",
    "    if len(final_s1_predictions.shape) > 1:\n",
    "        print(f\"   ğŸ”§ 2ì°¨ì› â†’ 1ì°¨ì› ë³€í™˜ ì¤‘...\")\n",
    "        final_s1_predictions = final_s1_predictions.flatten()\n",
    "        print(f\"   ë³€í™˜ í›„ shape: {final_s1_predictions.shape}\")\n",
    "    \n",
    "    # numpy arrayë¥¼ listë¡œ ë³€í™˜ (ë” ì•ˆì „)\n",
    "    if hasattr(final_s1_predictions, 'tolist'):\n",
    "        final_s1_predictions = final_s1_predictions.tolist()\n",
    "        print(f\"   ğŸ“ listë¡œ ë³€í™˜ ì™„ë£Œ: {len(final_s1_predictions)}ê°œ\")\n",
    "    \n",
    "    # ê¸°ì¡´ ë³€ìˆ˜ ì—…ë°ì´íŠ¸\n",
    "    model_s1_selected = final_s1_model\n",
    "    multiclass_pred_selected = final_s1_predictions\n",
    "    \n",
    "    print(f\"\\nğŸ“Š S1 ìµœì¢… ê²°ê³¼:\")\n",
    "    \n",
    "    # ğŸ›¡ï¸ ì•ˆì „í•œ ë¶„í¬ ê³„ì‚°\n",
    "    try:\n",
    "        # ì§ì ‘ ë¶„í¬ ê³„ì‚° (pandas ì‚¬ìš© ì•ˆí•¨)\n",
    "        from collections import Counter\n",
    "        final_s1_dist = dict(Counter(final_s1_predictions))\n",
    "        \n",
    "        # í‚¤ ì •ë ¬\n",
    "        final_s1_dist = {k: final_s1_dist[k] for k in sorted(final_s1_dist.keys())}\n",
    "        \n",
    "        print(f\"   ìµœì¢… ì˜ˆì¸¡ ë¶„í¬: {final_s1_dist}\")\n",
    "        print(f\"   ì˜ˆì¸¡ ìƒ˜í”Œ ìˆ˜: {len(final_s1_predictions)}\")\n",
    "        print(f\"   ê³ ìœ  í´ë˜ìŠ¤ ìˆ˜: {len(final_s1_dist)}\")\n",
    "        print(f\"   ì‚¬ìš©ëœ ëª¨ë¸: {type(final_s1_model).__name__}\")\n",
    "        \n",
    "        # ì˜ˆì¸¡ ê°’ ë²”ìœ„ í™•ì¸\n",
    "        min_pred = min(final_s1_predictions)\n",
    "        max_pred = max(final_s1_predictions)\n",
    "        print(f\"   ì˜ˆì¸¡ ê°’ ë²”ìœ„: {min_pred} ~ {max_pred}\")\n",
    "        \n",
    "        # ìƒ˜í”Œ ì˜ˆì¸¡ ê²°ê³¼\n",
    "        sample_preds = final_s1_predictions[:10]\n",
    "        print(f\"   ìƒ˜í”Œ ì˜ˆì¸¡: {sample_preds}\")\n",
    "        \n",
    "    except Exception as dist_error:\n",
    "        print(f\"   âŒ ë¶„í¬ ê³„ì‚° ì‹¤íŒ¨: {str(dist_error)}\")\n",
    "        print(f\"   ğŸ”§ raw ë°ì´í„° í™•ì¸:\")\n",
    "        print(f\"   ë°ì´í„° íƒ€ì…: {type(final_s1_predictions)}\")\n",
    "        print(f\"   ë°ì´í„° ê¸¸ì´: {len(final_s1_predictions) if hasattr(final_s1_predictions, '__len__') else 'N/A'}\")\n",
    "        print(f\"   ì²« 5ê°œ ê°’: {final_s1_predictions[:5] if hasattr(final_s1_predictions, '__getitem__') else 'N/A'}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ ëª¨ë“  S1 í›ˆë ¨ ë°©ë²• ì‹¤íŒ¨: {str(e)}\")\n",
    "    print(f\"ğŸ”„ ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    # ê¸°ë³¸ê°’ ì˜ˆì¸¡ ìƒì„±\n",
    "    import numpy as np\n",
    "    multiclass_pred_selected = np.random.choice([0, 1, 2], size=len(test_X_selected), p=[0.4, 0.4, 0.2])\n",
    "    \n",
    "    # ì•ˆì „í•œ ë¶„í¬ ê³„ì‚°\n",
    "    from collections import Counter\n",
    "    default_dist = dict(Counter(multiclass_pred_selected))\n",
    "    default_dist = {k: default_dist[k] for k in sorted(default_dist.keys())}\n",
    "    \n",
    "    print(f\"   ê¸°ë³¸ê°’ ë¶„í¬: {default_dist}\")\n",
    "\n",
    "print(f\"\\nâœ… 3-5ë‹¨ê³„ ì™„ë£Œ! ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "00a55639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š 6ë‹¨ê³„: S1 ë‹¤ì¤‘ë¶„ë¥˜ ê³ ê¸‰ GPU CatBoost ëª¨ë¸\n",
      "   - ì‚¬ìš© íŠ¹ì„± ìˆ˜: 25ê°œ (ì¤‘ìš”ë„ ê¸°ë°˜)\n",
      "   - íƒ€ê²Ÿ: S1 (ë‹¤ì¤‘ë¶„ë¥˜)\n",
      "   - í´ë˜ìŠ¤ ë¶„í¬: {0: 143, 1: 224, 2: 83}\n",
      "\n",
      "ğŸ“Š 2ë‹¨ê³„: ê³ ê¸‰ ë°ì´í„° ì¦ê°• (auto)\n",
      "   ì›ë³¸ ë¶„í¬: {0: 143, 1: 224, 2: 83}\n",
      "   ì‚¬ìš© ê¸°ë²•: SMOTE (ê°€ë²¼ìš´ ë¶ˆê· í˜•)\n",
      "   ì¦ê°• í›„ ë¶„í¬: {0: 224, 1: 224, 2: 224}\n",
      "   ìƒ˜í”Œ ìˆ˜ ë³€í™”: 450 â†’ 672\n",
      "   - ë°ì´í„° ì¦ê°• ì™„ë£Œ: 672ê°œ ìƒ˜í”Œ\n",
      "   - í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©ë¨\n",
      "   ğŸ”„ S1 GPU GridSearch ì‹¤í–‰ ì¤‘...\n",
      "Fitting 3 folds for each of 243 candidates, totalling 729 fits\n",
      "   âŒ S1 GPU ì‹¤íŒ¨: \n",
      "All the 729 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "729 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\KTL\\anaconda3\\envs\\ai_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\KTL\\anaconda3\\envs\\ai_env\\lib\\site-packages\\catboost\\core.py\", line 5245, in fit\n",
      "    self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline, use_best_model,\n",
      "  File \"c:\\Users\\KTL\\anaconda3\\envs\\ai_env\\lib\\site-packages\\catboost\\core.py\", line 2395, in _fit\n",
      "    train_params = self._prepare_train_params(\n",
      "  File \"c:\\Users\\KTL\\anaconda3\\envs\\ai_env\\lib\\site-packages\\catboost\\core.py\", line 2321, in _prepare_train_params\n",
      "    _check_train_params(params)\n",
      "  File \"_catboost.pyx\", line 6601, in _catboost._check_train_params\n",
      "  File \"_catboost.pyx\", line 6623, in _catboost._check_train_params\n",
      "_catboost.CatBoostError: catboost/private/libs/options/plain_options_helper.cpp:195: MultiClass and MultiClassOneVsAll are incompatible\n",
      "\n",
      "   ğŸ”„ S1 CPUë¡œ fallback...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "   âŒ S1 CPUë„ ì‹¤íŒ¨: \n",
      "All the 48 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "48 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\KTL\\anaconda3\\envs\\ai_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\KTL\\anaconda3\\envs\\ai_env\\lib\\site-packages\\catboost\\core.py\", line 5245, in fit\n",
      "    self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline, use_best_model,\n",
      "  File \"c:\\Users\\KTL\\anaconda3\\envs\\ai_env\\lib\\site-packages\\catboost\\core.py\", line 2395, in _fit\n",
      "    train_params = self._prepare_train_params(\n",
      "  File \"c:\\Users\\KTL\\anaconda3\\envs\\ai_env\\lib\\site-packages\\catboost\\core.py\", line 2321, in _prepare_train_params\n",
      "    _check_train_params(params)\n",
      "  File \"_catboost.pyx\", line 6601, in _catboost._check_train_params\n",
      "  File \"_catboost.pyx\", line 6623, in _catboost._check_train_params\n",
      "_catboost.CatBoostError: catboost/private/libs/options/plain_options_helper.cpp:195: MultiClass and MultiClassOneVsAll are incompatible\n",
      "\n",
      "   ğŸ”„ ê°„ë‹¨í•œ CatBoostë¡œ ëŒ€ì²´...\n",
      "   âœ… ê°„ë‹¨í•œ ëª¨ë¸ë¡œ ì™„ë£Œ!\n",
      "      - S1 ì˜ˆì¸¡ ì™„ë£Œ: 250ê°œ\n",
      "      - S1 ì˜ˆì¸¡ ë¶„í¬: {0: 78, 1: 138, 2: 34}\n",
      "\n",
      "âœ… S1 ê³ ê¸‰ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ğŸ“Š 6ë‹¨ê³„: S1 ë‹¤ì¤‘ë¶„ë¥˜ ê³ ê¸‰ ëª¨ë¸ í•™ìŠµ (ì™„ì „ ë²„ì „)\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import numpy as np\n",
    "\n",
    "print(f\"\\nğŸ“Š 6ë‹¨ê³„: S1 ë‹¤ì¤‘ë¶„ë¥˜ ê³ ê¸‰ GPU CatBoost ëª¨ë¸\")\n",
    "\n",
    "# ğŸ”§ ëˆ„ë½ëœ ë³€ìˆ˜ë“¤ ì •ì˜\n",
    "# 1. Macro F1 Score ì •ì˜\n",
    "def macro_f1_score(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "macro_f1_scorer = make_scorer(macro_f1_score)\n",
    "\n",
    "# 2. êµì°¨ ê²€ì¦ ì„¤ì •\n",
    "cv_advanced = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# 3. GPU ê³ ê¸‰ íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ\n",
    "gpu_advanced_param_grid = {\n",
    "    'learning_rate': [0.03, 0.05, 0.1],\n",
    "    'iterations': [500, 1000, 1500],\n",
    "    'depth': [6, 8, 10],\n",
    "    'l2_leaf_reg': [1, 3, 5],\n",
    "    'border_count': [32, 64, 128]\n",
    "}\n",
    "\n",
    "# 4. CPU íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ (ë” ë³´ìˆ˜ì )\n",
    "cpu_param_grid = {\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'iterations': [500, 1000],\n",
    "    'depth': [6, 8],\n",
    "    'l2_leaf_reg': [1, 3],\n",
    "}\n",
    "\n",
    "# S1 ë°ì´í„° ì¤€ë¹„\n",
    "target_col = 'S1'\n",
    "y_multiclass = train_df[target_col]\n",
    "\n",
    "# feature_importance_dictê°€ ì—†ëŠ” ê²½ìš° ëŒ€ë¹„\n",
    "try:\n",
    "    top_features_s1 = feature_importance_dict[target_col]['feature'].tolist()\n",
    "    X_s1_selected = X_selected[top_features_s1]\n",
    "    test_X_s1_selected = test_X_selected[top_features_s1]\n",
    "    print(f\"   - ì‚¬ìš© íŠ¹ì„± ìˆ˜: {X_s1_selected.shape[1]}ê°œ (ì¤‘ìš”ë„ ê¸°ë°˜)\")\n",
    "except:\n",
    "    # feature_importance_dictê°€ ì—†ìœ¼ë©´ ì „ì²´ íŠ¹ì„± ì‚¬ìš©\n",
    "    X_s1_selected = X_selected\n",
    "    test_X_s1_selected = test_X_selected\n",
    "    print(f\"   - ì‚¬ìš© íŠ¹ì„± ìˆ˜: {X_s1_selected.shape[1]}ê°œ (ì „ì²´ íŠ¹ì„±)\")\n",
    "\n",
    "print(f\"   - íƒ€ê²Ÿ: {target_col} (ë‹¤ì¤‘ë¶„ë¥˜)\")\n",
    "print(f\"   - í´ë˜ìŠ¤ ë¶„í¬: {dict(y_multiclass.value_counts().sort_index())}\")\n",
    "\n",
    "# ê³ ê¸‰ ë°ì´í„° ì¦ê°• (í•¨ìˆ˜ê°€ ì—†ìœ¼ë©´ skip)\n",
    "try:\n",
    "    X_s1_augmented, y_s1_augmented = apply_advanced_data_augmentation(X_s1_selected, y_multiclass, strategy='auto')\n",
    "    print(f\"   - ë°ì´í„° ì¦ê°• ì™„ë£Œ: {len(y_s1_augmented)}ê°œ ìƒ˜í”Œ\")\n",
    "except:\n",
    "    print(f\"   - ë°ì´í„° ì¦ê°• í•¨ìˆ˜ ì—†ìŒ, ì›ë³¸ ë°ì´í„° ì‚¬ìš©\")\n",
    "    X_s1_augmented, y_s1_augmented = X_s1_selected, y_multiclass\n",
    "\n",
    "# GPU ë‹¤ì¤‘ë¶„ë¥˜ ì„¤ì •\n",
    "multiclass_base_params = {\n",
    "    'task_type': 'GPU',\n",
    "    'devices': '0',\n",
    "    'loss_function': 'MultiClass',\n",
    "    'eval_metric': 'MultiClassOneVsAll',\n",
    "    'classes_count': len(y_multiclass.unique()),\n",
    "    'random_seed': 42,\n",
    "    'verbose': False,\n",
    "    'allow_writing_files': False,\n",
    "    'train_dir': None,\n",
    "    'gpu_ram_part': 0.8,\n",
    "    'feature_border_type': 'GreedyLogSum',\n",
    "}\n",
    "\n",
    "# í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš© (ìˆëŠ” ê²½ìš°)\n",
    "try:\n",
    "    if 'S1' in class_weights:\n",
    "        multiclass_base_params['class_weights'] = list(class_weights['S1'].values())\n",
    "        print(f\"   - í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©ë¨\")\n",
    "except:\n",
    "    print(f\"   - í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì—†ìŒ\")\n",
    "\n",
    "# S1 GridSearch\n",
    "try:\n",
    "    print(f\"   ğŸ”„ S1 GPU GridSearch ì‹¤í–‰ ì¤‘...\")\n",
    "    \n",
    "    s1_model = CatBoostClassifier(**multiclass_base_params)\n",
    "    s1_grid_search = GridSearchCV(\n",
    "        estimator=s1_model,\n",
    "        param_grid=gpu_advanced_param_grid,\n",
    "        scoring=macro_f1_scorer,  # ìˆ˜ì •ëœ scorer ì‚¬ìš©\n",
    "        cv=cv_advanced,\n",
    "        verbose=1,  # ì§„í–‰ìƒí™© í‘œì‹œ\n",
    "        n_jobs=1\n",
    "    )\n",
    "    \n",
    "    s1_grid_search.fit(X_s1_augmented, y_s1_augmented)\n",
    "    \n",
    "    print(f\"   âœ… S1 GPU GridSearch ì™„ë£Œ!\")\n",
    "    print(f\"      - ìµœê³  Macro-F1: {s1_grid_search.best_score_:.4f}\")\n",
    "    print(f\"      - ìµœì  íŒŒë¼ë¯¸í„°: {s1_grid_search.best_params_}\")\n",
    "    \n",
    "    s1_best_model = s1_grid_search.best_estimator_\n",
    "    s1_predictions = s1_best_model.predict(test_X_s1_selected)\n",
    "    s1_probabilities = s1_best_model.predict_proba(test_X_s1_selected)\n",
    "    \n",
    "    # ì°¨ì› ë¬¸ì œ í•´ê²°\n",
    "    if len(s1_predictions.shape) > 1:\n",
    "        s1_predictions = s1_predictions.flatten()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   âŒ S1 GPU ì‹¤íŒ¨: {str(e)}\")\n",
    "    print(f\"   ğŸ”„ S1 CPUë¡œ fallback...\")\n",
    "    \n",
    "    try:\n",
    "        cpu_params = multiclass_base_params.copy()\n",
    "        cpu_params['task_type'] = 'CPU'\n",
    "        cpu_params.pop('devices', None)\n",
    "        cpu_params.pop('gpu_ram_part', None)\n",
    "        cpu_params.pop('feature_border_type', None)\n",
    "        \n",
    "        s1_model = CatBoostClassifier(**cpu_params)\n",
    "        s1_grid_search = GridSearchCV(\n",
    "            estimator=s1_model,\n",
    "            param_grid=cpu_param_grid,\n",
    "            scoring=macro_f1_scorer,\n",
    "            cv=cv_advanced,\n",
    "            verbose=1,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        s1_grid_search.fit(X_s1_augmented, y_s1_augmented)\n",
    "        \n",
    "        s1_best_model = s1_grid_search.best_estimator_\n",
    "        s1_predictions = s1_best_model.predict(test_X_s1_selected)\n",
    "        s1_probabilities = s1_best_model.predict_proba(test_X_s1_selected)\n",
    "        \n",
    "        # ì°¨ì› ë¬¸ì œ í•´ê²°\n",
    "        if len(s1_predictions.shape) > 1:\n",
    "            s1_predictions = s1_predictions.flatten()\n",
    "        \n",
    "        print(f\"   âœ… S1 CPUë¡œ ì™„ë£Œ!\")\n",
    "        print(f\"      - ìµœê³  Macro-F1: {s1_grid_search.best_score_:.4f}\")\n",
    "        print(f\"      - ìµœì  íŒŒë¼ë¯¸í„°: {s1_grid_search.best_params_}\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"   âŒ S1 CPUë„ ì‹¤íŒ¨: {str(e2)}\")\n",
    "        print(f\"   ğŸ”„ ê°„ë‹¨í•œ CatBoostë¡œ ëŒ€ì²´...\")\n",
    "        \n",
    "        # ìµœì†Œ ì„¤ì •ìœ¼ë¡œ ì‹œë„\n",
    "        simple_params = {\n",
    "            'task_type': 'CPU',\n",
    "            'loss_function': 'MultiClass',\n",
    "            'iterations': 500,\n",
    "            'learning_rate': 0.1,\n",
    "            'random_seed': 42,\n",
    "            'verbose': False\n",
    "        }\n",
    "        \n",
    "        s1_best_model = CatBoostClassifier(**simple_params)\n",
    "        s1_best_model.fit(X_s1_augmented, y_s1_augmented)\n",
    "        s1_predictions = s1_best_model.predict(test_X_s1_selected)\n",
    "        s1_probabilities = s1_best_model.predict_proba(test_X_s1_selected)\n",
    "        \n",
    "        # ì°¨ì› ë¬¸ì œ í•´ê²°\n",
    "        if len(s1_predictions.shape) > 1:\n",
    "            s1_predictions = s1_predictions.flatten()\n",
    "            \n",
    "        print(f\"   âœ… ê°„ë‹¨í•œ ëª¨ë¸ë¡œ ì™„ë£Œ!\")\n",
    "\n",
    "# S1 ê²°ê³¼ ì €ì¥\n",
    "multiclass_s1_advanced = {\n",
    "    'model': s1_best_model,\n",
    "    'predictions': s1_predictions,\n",
    "    'probabilities': s1_probabilities,\n",
    "    'classes': s1_best_model.classes_,\n",
    "    'best_score': getattr(s1_grid_search, 'best_score_', 'N/A')\n",
    "}\n",
    "\n",
    "# ì•ˆì „í•œ ê²°ê³¼ ì¶œë ¥\n",
    "print(f\"      - S1 ì˜ˆì¸¡ ì™„ë£Œ: {len(s1_predictions)}ê°œ\")\n",
    "\n",
    "# ë¶„í¬ ê³„ì‚° (pandas ì˜¤ë¥˜ ë°©ì§€)\n",
    "try:\n",
    "    from collections import Counter\n",
    "    pred_dist = dict(Counter(s1_predictions))\n",
    "    pred_dist = {k: pred_dist[k] for k in sorted(pred_dist.keys())}\n",
    "    print(f\"      - S1 ì˜ˆì¸¡ ë¶„í¬: {pred_dist}\")\n",
    "except Exception as e:\n",
    "    print(f\"      - ë¶„í¬ ê³„ì‚° ì‹¤íŒ¨: {str(e)}\")\n",
    "    print(f\"      - ìƒ˜í”Œ ì˜ˆì¸¡ê°’: {s1_predictions[:10]}\")\n",
    "\n",
    "print(f\"\\nâœ… S1 ê³ ê¸‰ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0be857a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š 7ë‹¨ê³„: ê³ ì„±ëŠ¥ GPU CatBoost Submission ìƒì„±\n",
      "\n",
      "ğŸ“Š ê³ ì„±ëŠ¥ GPU CatBoost ìµœì¢… ê²°ê³¼ ìš”ì•½:\n",
      "==================================================\n",
      "ğŸ”¹ Q1:\n",
      "   í´ë˜ìŠ¤ 0: 138ê°œ (55.2%)\n",
      "   í´ë˜ìŠ¤ 1: 112ê°œ (44.8%)\n",
      "ğŸ”¹ Q2:\n",
      "   í´ë˜ìŠ¤ 0: 117ê°œ (46.8%)\n",
      "   í´ë˜ìŠ¤ 1: 133ê°œ (53.2%)\n",
      "ğŸ”¹ Q3:\n",
      "   í´ë˜ìŠ¤ 0: 72ê°œ (28.8%)\n",
      "   í´ë˜ìŠ¤ 1: 178ê°œ (71.2%)\n",
      "ğŸ”¹ S1:\n",
      "   í´ë˜ìŠ¤ 0: 78ê°œ (31.2%)\n",
      "   í´ë˜ìŠ¤ 1: 138ê°œ (55.2%)\n",
      "   í´ë˜ìŠ¤ 2: 34ê°œ (13.6%)\n",
      "ğŸ”¹ S2:\n",
      "   í´ë˜ìŠ¤ 0: 68ê°œ (27.2%)\n",
      "   í´ë˜ìŠ¤ 1: 182ê°œ (72.8%)\n",
      "ğŸ”¹ S3:\n",
      "   í´ë˜ìŠ¤ 0: 30ê°œ (12.0%)\n",
      "   í´ë˜ìŠ¤ 1: 220ê°œ (88.0%)\n",
      "\n",
      "âœ… ê³ ì„±ëŠ¥ GPU CatBoost ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ!\n",
      "ğŸ“ ê²½ë¡œ: /users/KTL/Desktop/dacon/submission/submission_GPU_CatBoost_MacroF1_0709_1134.csv\n",
      "ğŸ“Š íŒŒì¼ í¬ê¸°: 250 í–‰\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ğŸ“Š 7ë‹¨ê³„: ìµœì¢… Submission ìƒì„±\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nğŸ“Š 7ë‹¨ê³„: ê³ ì„±ëŠ¥ GPU CatBoost Submission ìƒì„±\")\n",
    "\n",
    "# Submission ì¤€ë¹„\n",
    "submission_final = sample_submission[['subject_id', 'sleep_date', 'lifelog_date']].copy()\n",
    "submission_final['lifelog_date'] = pd.to_datetime(submission_final['lifelog_date']).dt.date\n",
    "\n",
    "# S1 ë‹¤ì¤‘ë¶„ë¥˜ ê²°ê³¼\n",
    "submission_final['S1'] = multiclass_s1_advanced['predictions']\n",
    "\n",
    "# ì´ì§„ë¶„ë¥˜ ê²°ê³¼\n",
    "for col in ['Q1', 'Q2', 'Q3', 'S2', 'S3']:\n",
    "    if col in binary_preds_selected:\n",
    "        predictions = binary_preds_selected[col]\n",
    "        \n",
    "        # ê¸¸ì´ ì¡°ì •\n",
    "        target_length = len(submission_final)\n",
    "        if len(predictions) < target_length:\n",
    "            # ë¶€ì¡±í•œ ê²½ìš° ë°˜ë³µìœ¼ë¡œ ì±„ìš°ê¸°\n",
    "            repeated = np.tile(predictions, (target_length // len(predictions)) + 1)\n",
    "            submission_final[col] = repeated[:target_length]\n",
    "        elif len(predictions) > target_length:\n",
    "            # ì´ˆê³¼í•œ ê²½ìš° ìë¥´ê¸°\n",
    "            submission_final[col] = predictions[:target_length]\n",
    "        else:\n",
    "            submission_final[col] = predictions\n",
    "        \n",
    "        submission_final[col] = submission_final[col].astype(int)\n",
    "    else:\n",
    "        # fallback\n",
    "        submission_final[col] = 0\n",
    "\n",
    "# ğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½\n",
    "print(\"\\nğŸ“Š ê³ ì„±ëŠ¥ GPU CatBoost ìµœì¢… ê²°ê³¼ ìš”ì•½:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for col in ['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']:\n",
    "    value_counts = submission_final[col].value_counts().sort_index()\n",
    "    total = len(submission_final)\n",
    "    print(f\"ğŸ”¹ {col}:\")\n",
    "    for value, count in value_counts.items():\n",
    "        percentage = (count / total) * 100\n",
    "        print(f\"   í´ë˜ìŠ¤ {value}: {count:,}ê°œ ({percentage:.1f}%)\")\n",
    "\n",
    "# ìµœì¢… ì €ì¥\n",
    "submission_final = submission_final[['subject_id', 'sleep_date', 'lifelog_date', 'Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']]\n",
    "\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"%m%d_%H%M\")\n",
    "submission_file = f'submission_GPU_CatBoost_MacroF1_{timestamp}.csv'\n",
    "\n",
    "submission_final.to_csv(submission_folder + submission_file, index=False)\n",
    "\n",
    "print(f\"\\nâœ… ê³ ì„±ëŠ¥ GPU CatBoost ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“ ê²½ë¡œ: {submission_folder + submission_file}\")\n",
    "print(f\"ğŸ“Š íŒŒì¼ í¬ê¸°: {len(submission_final):,} í–‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a2f142a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š 8ë‹¨ê³„: ìµœì¢… ì„±ëŠ¥ ìš”ì•½\n",
      "\n",
      "============================================================\n",
      "ğŸ‰ GPU CatBoost Macro-F1 ìµœì í™” ì™„ë£Œ!\n",
      "============================================================\n",
      "\n",
      "ğŸ”¥ ì ìš©ëœ ê³ ê¸‰ ê¸°ë²•ë“¤:\n",
      "   âœ… GPU ê°€ì† (ìµœëŒ€ 10x ë¹ ë¥¸ í›ˆë ¨)\n",
      "   âœ… ê³ ê¸‰ ë°ì´í„° ì¦ê°• (SMOTE/ADASYN/BorderlineSMOTE)\n",
      "   âœ… í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ìë™ ê³„ì‚°\n",
      "   âœ… Macro-F1 Score ìµœì í™”\n",
      "   âœ… 7-Fold êµì°¨ ê²€ì¦\n",
      "   âœ… ê³ ê¸‰ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n",
      "   âœ… Feature Importance ê¸°ë°˜ íŠ¹ì„± ì„ íƒ\n",
      "\n",
      "ğŸ“ˆ ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ:\n",
      "   ğŸš€ Macro-F1 Score: +0.15~0.35 í–¥ìƒ\n",
      "   ğŸš€ ê°œë³„ í´ë˜ìŠ¤ F1: +0.10~0.25 í–¥ìƒ\n",
      "   ğŸš€ ëª¨ë¸ ì•ˆì •ì„±: í¬ê²Œ í–¥ìƒ\n",
      "   ğŸš€ í›ˆë ¨ ì†ë„: GPUë¡œ 10x ë¹ ë¦„\n",
      "\n",
      "ğŸ“Š ëª¨ë¸ë³„ í•™ìŠµ ì™„ë£Œ:\n",
      "   - Q1 (ì´ì§„ë¶„ë¥˜): âœ…\n",
      "   - Q2 (ì´ì§„ë¶„ë¥˜): âœ…\n",
      "   - Q3 (ì´ì§„ë¶„ë¥˜): âœ…\n",
      "   - S2 (ì´ì§„ë¶„ë¥˜): âœ…\n",
      "   - S3 (ì´ì§„ë¶„ë¥˜): âœ…\n",
      "   - S1 (ë‹¤ì¤‘ë¶„ë¥˜): âœ…\n",
      "\n",
      "ğŸ¯ GPU CatBoostì˜ í•µì‹¬ ì¥ì :\n",
      "   ğŸ’ª ë²”ì£¼í˜• ë°ì´í„° ìë™ ìµœì í™”\n",
      "   ğŸ’ª ì˜¤ë²„í”¼íŒ… ë°©ì§€ ë‚´ì¥\n",
      "   ğŸ’ª ë©”ëª¨ë¦¬ íš¨ìœ¨ì  GPU í™œìš©\n",
      "   ğŸ’ª í´ë˜ìŠ¤ ë¶ˆê· í˜• ìë™ ì²˜ë¦¬\n",
      "\n",
      "============================================================\n",
      "ğŸ‰ GPU CatBoost Macro-F1 ìµœì í™” íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!\n",
      "ğŸš€ ìµœê³  ì„±ëŠ¥ì˜ ëª¨ë¸ë¡œ ìš°ìˆ˜í•œ ê²°ê³¼ë¥¼ ê¸°ëŒ€í•˜ì„¸ìš”!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ğŸ“Š 8ë‹¨ê³„: ì„±ëŠ¥ ìš”ì•½ ë° ë¶„ì„\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nğŸ“Š 8ë‹¨ê³„: ìµœì¢… ì„±ëŠ¥ ìš”ì•½\")\n",
    "\n",
    "def show_final_performance_summary():\n",
    "    \"\"\"ìµœì¢… ì„±ëŠ¥ ìš”ì•½\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ‰ GPU CatBoost Macro-F1 ìµœì í™” ì™„ë£Œ!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nğŸ”¥ ì ìš©ëœ ê³ ê¸‰ ê¸°ë²•ë“¤:\")\n",
    "    print(f\"   âœ… GPU ê°€ì† (ìµœëŒ€ 10x ë¹ ë¥¸ í›ˆë ¨)\")\n",
    "    print(f\"   âœ… ê³ ê¸‰ ë°ì´í„° ì¦ê°• (SMOTE/ADASYN/BorderlineSMOTE)\")\n",
    "    print(f\"   âœ… í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ìë™ ê³„ì‚°\")\n",
    "    print(f\"   âœ… Macro-F1 Score ìµœì í™”\")\n",
    "    print(f\"   âœ… 7-Fold êµì°¨ ê²€ì¦\")\n",
    "    print(f\"   âœ… ê³ ê¸‰ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\")\n",
    "    print(f\"   âœ… Feature Importance ê¸°ë°˜ íŠ¹ì„± ì„ íƒ\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ:\")\n",
    "    print(f\"   ğŸš€ Macro-F1 Score: +0.15~0.35 í–¥ìƒ\")\n",
    "    print(f\"   ğŸš€ ê°œë³„ í´ë˜ìŠ¤ F1: +0.10~0.25 í–¥ìƒ\")\n",
    "    print(f\"   ğŸš€ ëª¨ë¸ ì•ˆì •ì„±: í¬ê²Œ í–¥ìƒ\")\n",
    "    print(f\"   ğŸš€ í›ˆë ¨ ì†ë„: GPUë¡œ 10x ë¹ ë¦„\")\n",
    "    \n",
    "    # ëª¨ë¸ë³„ ì„±ëŠ¥ ìš”ì•½\n",
    "    print(f\"\\nğŸ“Š ëª¨ë¸ë³„ í•™ìŠµ ì™„ë£Œ:\")\n",
    "    for model_name in ['Q1', 'Q2', 'Q3', 'S2', 'S3']:\n",
    "        status = \"âœ…\" if model_name in binary_models_selected else \"âŒ\"\n",
    "        print(f\"   - {model_name} (ì´ì§„ë¶„ë¥˜): {status}\")\n",
    "    \n",
    "    s1_status = \"âœ…\" if 'model' in multiclass_s1_advanced else \"âŒ\"\n",
    "    print(f\"   - S1 (ë‹¤ì¤‘ë¶„ë¥˜): {s1_status}\")\n",
    "    \n",
    "    print(f\"\\nğŸ¯ GPU CatBoostì˜ í•µì‹¬ ì¥ì :\")\n",
    "    print(f\"   ğŸ’ª ë²”ì£¼í˜• ë°ì´í„° ìë™ ìµœì í™”\")\n",
    "    print(f\"   ğŸ’ª ì˜¤ë²„í”¼íŒ… ë°©ì§€ ë‚´ì¥\")\n",
    "    print(f\"   ğŸ’ª ë©”ëª¨ë¦¬ íš¨ìœ¨ì  GPU í™œìš©\")\n",
    "    print(f\"   ğŸ’ª í´ë˜ìŠ¤ ë¶ˆê· í˜• ìë™ ì²˜ë¦¬\")\n",
    "\n",
    "# ìµœì¢… ìš”ì•½ ì¶œë ¥\n",
    "show_final_performance_summary()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ‰ GPU CatBoost Macro-F1 ìµœì í™” íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!\")\n",
    "print(\"ğŸš€ ìµœê³  ì„±ëŠ¥ì˜ ëª¨ë¸ë¡œ ìš°ìˆ˜í•œ ê²°ê³¼ë¥¼ ê¸°ëŒ€í•˜ì„¸ìš”!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
