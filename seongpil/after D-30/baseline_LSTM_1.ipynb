{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1f085c0",
   "metadata": {},
   "source": [
    "### feature ëŒ€ìƒìœ¼ë¡œ LGBM í•™ìŠµ  \n",
    "(Developed from dacon_etri_base_mod1.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0593c4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# í˜„ì¬ ë‚ ì§œ ë° ì‹œê°„ ê°€ì ¸ì˜¤ê¸°\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"%m%d_%H%M\")  # ì˜ˆ: 0517_1530\n",
    "\n",
    "\n",
    "submission_folder = '/users/KTL/Desktop/dacon/submission/'\n",
    "submission_file = f'submission_final_Lstm_2_{timestamp}.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0bead988",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import glob \n",
    "import random \n",
    "import os \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import ast \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "903c2113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed ê³ ì • \n",
    "SD = 42 \n",
    "random.seed(SD) \n",
    "np.random.seed(SD) \n",
    "os.environ['PYTHONHASHSEED'] = str(SD)\n",
    "tf.random.set_seed(SD)  # TensorFlow ì‹œë“œ ì„¤ì •\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ ì„¤ì • - VSCode ìƒëŒ€ê²½ë¡œë¡œ ë³€ê²½\n",
    "# ì‹¤ì œ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì • í•„ìš”\n",
    "base_folder =  '/users/KTL/Desktop/ETRI_lifelog_dataset'\n",
    "folder = '/ch2025_data_items'\n",
    "\n",
    "data_dir = base_folder + folder \n",
    "\n",
    "\n",
    "# Parquet íŒŒì¼ ì „ì²´ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ \n",
    "parquet_files = glob.glob(os.path.join(data_dir, 'ch2025_*.parquet')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9028dc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded: mACStatus, shape = (939896, 3)\n",
      "âœ… Loaded: mActivity, shape = (961062, 3)\n",
      "âœ… Loaded: mAmbience, shape = (476577, 3)\n",
      "âœ… Loaded: mBle, shape = (21830, 3)\n",
      "âœ… Loaded: mGps, shape = (800611, 3)\n",
      "âœ… Loaded: mLight, shape = (96258, 3)\n",
      "âœ… Loaded: mScreenStatus, shape = (939653, 3)\n",
      "âœ… Loaded: mUsageStats, shape = (45197, 3)\n",
      "âœ… Loaded: mWifi, shape = (76336, 3)\n",
      "âœ… Loaded: wHr, shape = (382918, 3)\n",
      "âœ… Loaded: wLight, shape = (633741, 3)\n",
      "âœ… Loaded: wPedo, shape = (748100, 9)\n"
     ]
    }
   ],
   "source": [
    "# íŒŒì¼ ì´ë¦„ì„ í‚¤ë¡œ, DataFrameì„ ê°’ìœ¼ë¡œ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ \n",
    "lifelog_data = {} \n",
    "\n",
    "# íŒŒì¼ë³„ë¡œ ì½ê¸° \n",
    "for file_path in parquet_files: \n",
    "    name = os.path.basename(file_path).replace('.parquet', '').replace('ch2025_', '') \n",
    "    lifelog_data[name] = pd.read_parquet(file_path) \n",
    "    print(f\"âœ… Loaded: {name}, shape = {lifelog_data[name].shape}\") \n",
    "\n",
    "# ë”•ì…”ë„ˆë¦¬ì— ìˆëŠ” ëª¨ë“  í•­ëª©ì„ ë…ë¦½ì ì¸ ë³€ìˆ˜ë¡œ í• ë‹¹ \n",
    "for key, df in lifelog_data.items(): \n",
    "    globals()[f\"{key}_df\"] = df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7cbd9424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”íŠ¸ë¦­ìŠ¤ íŒŒì¼ ì½ê¸°\n",
    "metrics_train = pd.read_csv(base_folder + '/ch2025_metrics_train.csv')\n",
    "sample_submission = pd.read_csv(base_folder+'/ch2025_submission_sample.csv')\n",
    "\n",
    "# âœ… ê¸°ì¤€ ìŒ (subject_id, lifelog_date) \n",
    "sample_submission['lifelog_date'] = pd.to_datetime(sample_submission['lifelog_date']) \n",
    "test_keys = set(zip(sample_submission['subject_id'], sample_submission['lifelog_date'].dt.date)) \n",
    "\n",
    "# âœ… DataFrame ë³„ timestamp ì»¬ëŸ¼ ìˆ˜ë™ ì§€ì • \n",
    "dataframes = { \n",
    "    'mACStatus': (mACStatus_df, 'timestamp'), \n",
    "    'mActivity': (mActivity_df, 'timestamp'), \n",
    "    'mAmbience': (mAmbience_df, 'timestamp'), \n",
    "    'mBle': (mBle_df, 'timestamp'), \n",
    "    'mGps': (mGps_df, 'timestamp'), \n",
    "    'mLight': (mLight_df, 'timestamp'), \n",
    "    'mScreenStatus': (mScreenStatus_df, 'timestamp'), \n",
    "    'mUsageStats': (mUsageStats_df, 'timestamp'), \n",
    "    'mWifi': (mWifi_df, 'timestamp'), \n",
    "    'wHr': (wHr_df, 'timestamp'), \n",
    "    'wLight': (wLight_df, 'timestamp'), \n",
    "    'wPedo': (wPedo_df, 'timestamp'), \n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b77b6fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… ë¶„ë¦¬ í•¨ìˆ˜ \n",
    "def split_test_train(df, subject_col='subject_id', timestamp_col='timestamp'): \n",
    "    df[timestamp_col] = pd.to_datetime(df[timestamp_col], errors='coerce') \n",
    "    df = df.dropna(subset=[timestamp_col]) \n",
    "    df['date_only'] = df[timestamp_col].dt.date \n",
    "    df['key'] = list(zip(df[subject_col], df['date_only'])) \n",
    "    test_df = df[df['key'].isin(test_keys)].drop(columns=['date_only', 'key']) \n",
    "    train_df = df[~df['key'].isin(test_keys)].drop(columns=['date_only', 'key']) \n",
    "    return test_df, train_df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a3416230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ mACStatus ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mACStatus_test â†’ (335849, 3), mACStatus_train â†’ (604047, 3)\n",
      "â³ mActivity ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mActivity_test â†’ (343579, 3), mActivity_train â†’ (617483, 3)\n",
      "â³ mAmbience ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mAmbience_test â†’ (170453, 3), mAmbience_train â†’ (306124, 3)\n",
      "â³ mBle ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mBle_test â†’ (8140, 3), mBle_train â†’ (13690, 3)\n",
      "â³ mGps ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mGps_test â†’ (287386, 3), mGps_train â†’ (513225, 3)\n",
      "â³ mLight ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mLight_test â†’ (34439, 3), mLight_train â†’ (61819, 3)\n",
      "â³ mScreenStatus ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mScreenStatus_test â†’ (336160, 3), mScreenStatus_train â†’ (603493, 3)\n",
      "â³ mUsageStats ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mUsageStats_test â†’ (16499, 3), mUsageStats_train â†’ (28698, 3)\n",
      "â³ mWifi ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mWifi_test â†’ (27467, 3), mWifi_train â†’ (48869, 3)\n",
      "â³ wHr ë¶„ë¦¬ ì¤‘...\n",
      "âœ… wHr_test â†’ (143311, 3), wHr_train â†’ (239607, 3)\n",
      "â³ wLight ë¶„ë¦¬ ì¤‘...\n",
      "âœ… wLight_test â†’ (233809, 3), wLight_train â†’ (399932, 3)\n",
      "â³ wPedo ë¶„ë¦¬ ì¤‘...\n",
      "âœ… wPedo_test â†’ (288832, 9), wPedo_train â†’ (459268, 9)\n"
     ]
    }
   ],
   "source": [
    "# âœ… ê²°ê³¼ ì €ì¥ \n",
    "for name, (df, ts_col) in dataframes.items(): \n",
    "    print(f\"â³ {name} ë¶„ë¦¬ ì¤‘...\") \n",
    "    test_df, train_df = split_test_train(df.copy(), subject_col='subject_id', timestamp_col=ts_col) \n",
    "    globals()[f\"{name}_test\"] = test_df \n",
    "    globals()[f\"{name}_train\"] = train_df \n",
    "    print(f\"âœ… {name}_test â†’ {test_df.shape}, {name}_train â†’ {train_df.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f7f34d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mACStatus(df): \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    df = df.sort_values(['subject_id', 'timestamp']) \n",
    "    results = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        status = group['m_charging'].values # 0/1 ìƒíƒœ \n",
    "        times = group['timestamp'].values # ì¶©ì „ ìƒíƒœ ë¹„ìœ¨ \n",
    "        ratio_charging = status.mean() \n",
    "        # ìƒíƒœ ì „ì´ íšŸìˆ˜ \n",
    "        transitions = (status[1:] != status[:-1]).sum() \n",
    "        # ì—°ì†ëœ 1 ìƒíƒœ ê¸¸ì´ë“¤ \n",
    "        lengths = [] \n",
    "        current_len = 0 \n",
    "        for val in status: \n",
    "            if val == 1: \n",
    "                current_len += 1 \n",
    "            elif current_len > 0: \n",
    "                lengths.append(current_len) \n",
    "                current_len = 0 \n",
    "        if current_len > 0: \n",
    "            lengths.append(current_len) \n",
    "        avg_charging_duration = np.mean(lengths) if lengths else 0 \n",
    "        max_charging_duration = np.max(lengths) if lengths else 0 \n",
    "        results.append({ \n",
    "            'subject_id': subj, \n",
    "            'date': date, \n",
    "            'charging_ratio': ratio_charging, \n",
    "            'charging_transitions': transitions, \n",
    "            'avg_charging_duration': avg_charging_duration, \n",
    "            'max_charging_duration': max_charging_duration, \n",
    "        }) \n",
    "    return pd.DataFrame(results) \n",
    "\n",
    "mACStatus_df2 = process_mACStatus(mACStatus_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f7c3c0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mActivity(df): \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    summary = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        counts = group['m_activity'].value_counts(normalize=True) # ë¹„ìœ¨ \n",
    "        row = {'subject_id': subj, 'date': date} \n",
    "        # 0~8 ë¹„ìœ¨ ì €ì¥ \n",
    "        for i in range(9): \n",
    "            row[f'activity_{i}_ratio'] = counts.get(i, 0) \n",
    "        # ì£¼ìš” í™œë™ ì •ë³´ \n",
    "        row['dominant_activity'] = group['m_activity'].mode()[0] \n",
    "        row['num_unique_activities'] = group['m_activity'].nunique() \n",
    "        summary.append(row) \n",
    "    return pd.DataFrame(summary) \n",
    "\n",
    "mActivity_df2 = process_mActivity(mActivity_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7ddb753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì§€ì •ëœ 10ê°œ ë¼ë²¨ \n",
    "top_10_labels = [ \n",
    "    \"Inside, small room\", \"Speech\", \"Silence\", \"Music\", \"Narration, monologue\", \n",
    "    \"Child speech, kid speaking\", \"Conversation\", \"Speech synthesizer\", \"Shout\", \"Babbling\" \n",
    "] \n",
    "\n",
    "def process_mAmbience_top10(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    # ì´ˆê¸°í™” \n",
    "    for label in top_10_labels + ['others']: \n",
    "        df[label] = 0.0 \n",
    "    for idx, row in df.iterrows(): \n",
    "        parsed = ast.literal_eval(row['m_ambience']) if isinstance(row['m_ambience'], str) else row['m_ambience'] \n",
    "        others_prob = 0.0 \n",
    "        for label, prob in parsed: \n",
    "            prob = float(prob) \n",
    "            if label in top_10_labels: \n",
    "                df.at[idx, label] = prob \n",
    "            else: \n",
    "                others_prob += prob \n",
    "        df.at[idx, 'others'] = others_prob \n",
    "    return df.drop(columns=['m_ambience']) \n",
    "\n",
    "mAmbience_df2= process_mAmbience_top10(mAmbience_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f7cc98b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_mAmbience_daily(df): \n",
    "    prob_cols = [col for col in df.columns if col not in ['subject_id', 'timestamp', 'date']] \n",
    "    # í•˜ë£¨ ë‹¨ìœ„ë¡œ í‰ê· ê°’ ìš”ì•½ \n",
    "    daily_summary = df.groupby(['subject_id', 'date'])[prob_cols].mean().reset_index() \n",
    "    return daily_summary \n",
    "\n",
    "mAmbience_df2 = summarize_mAmbience_daily(mAmbience_df2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "763e541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mBle(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    features = [] \n",
    "    for idx, row in df.iterrows(): \n",
    "        entry = ast.literal_eval(row['m_ble']) if isinstance(row['m_ble'], str) else row['m_ble'] \n",
    "        rssi_list = [] \n",
    "        class_0_cnt = 0 \n",
    "        class_other_cnt = 0 \n",
    "        for device in entry: \n",
    "            try: \n",
    "                rssi = int(device['rssi']) \n",
    "                rssi_list.append(rssi) \n",
    "                if str(device['device_class']) == '0': \n",
    "                    class_0_cnt += 1 \n",
    "                else: \n",
    "                    class_other_cnt += 1 \n",
    "            except: \n",
    "                continue # malformed record \n",
    "        feature = { \n",
    "            'subject_id': row['subject_id'], \n",
    "            'date': row['date'], \n",
    "            'device_class_0_cnt': class_0_cnt, \n",
    "            'device_class_others_cnt': class_other_cnt, \n",
    "            'device_count': len(rssi_list), \n",
    "            'rssi_mean': np.mean(rssi_list) if rssi_list else np.nan, \n",
    "            'rssi_min': np.min(rssi_list) if rssi_list else np.nan, \n",
    "            'rssi_max': np.max(rssi_list) if rssi_list else np.nan, \n",
    "        } \n",
    "        features.append(feature) \n",
    "    return pd.DataFrame(features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e38ac08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_mBle_daily(df): \n",
    "    # row ë‹¨ìœ„ BLE feature ì¶”ì¶œ \n",
    "    df = process_mBle(df) \n",
    "    # í•˜ë£¨ ë‹¨ìœ„ë¡œ cnt í•©ì¹˜ê¸° \n",
    "    grouped = df.groupby(['subject_id', 'date']).agg({ \n",
    "        'device_class_0_cnt': 'sum', \n",
    "        'device_class_others_cnt': 'sum', \n",
    "        'rssi_mean': 'mean', \n",
    "        'rssi_min': 'min', \n",
    "        'rssi_max': 'max', \n",
    "    }).reset_index() \n",
    "    # ì´í•© êµ¬í•´ì„œ ë¹„ìœ¨ ê³„ì‚° \n",
    "    total_cnt = grouped['device_class_0_cnt'] + grouped['device_class_others_cnt'] \n",
    "    grouped['device_class_0_ratio'] = grouped['device_class_0_cnt'] / total_cnt.replace(0, np.nan) \n",
    "    grouped['device_class_others_ratio'] = grouped['device_class_others_cnt'] / total_cnt.replace(0, np.nan) \n",
    "    # í•„ìš” ì—†ëŠ” ì›ë˜ cnt ì»¬ëŸ¼ ì œê±° \n",
    "    grouped.drop(columns=['device_class_0_cnt', 'device_class_others_cnt'], inplace=True) \n",
    "    return grouped \n",
    "\n",
    "mBle_df2 = summarize_mBle_daily(mBle_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "618d366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mGps(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    features = [] \n",
    "    for idx, row in df.iterrows(): \n",
    "        gps_list = ast.literal_eval(row['m_gps']) if isinstance(row['m_gps'], str) else row['m_gps'] \n",
    "        altitudes = [] \n",
    "        latitudes = [] \n",
    "        longitudes = [] \n",
    "        speeds = [] \n",
    "        for entry in gps_list: \n",
    "            try: \n",
    "                altitudes.append(float(entry['altitude'])) \n",
    "                latitudes.append(float(entry['latitude'])) \n",
    "                longitudes.append(float(entry['longitude'])) \n",
    "                speeds.append(float(entry['speed'])) \n",
    "            except: \n",
    "                continue \n",
    "        features.append({ \n",
    "            'subject_id': row['subject_id'], \n",
    "            'date': row['date'], \n",
    "            'altitude_mean': np.mean(altitudes) if altitudes else np.nan, \n",
    "            'latitude_std': np.std(latitudes) if latitudes else np.nan, \n",
    "            'longitude_std': np.std(longitudes) if longitudes else np.nan, \n",
    "            'speed_mean': np.mean(speeds) if speeds else np.nan, \n",
    "            'speed_max': np.max(speeds) if speeds else np.nan, \n",
    "            'speed_std': np.std(speeds) if speeds else np.nan, \n",
    "        }) \n",
    "    return pd.DataFrame(features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "14a0be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_Gps_df2 = process_mGps(mGps_df) \n",
    "m_Gps_df2 = m_Gps_df2.groupby(['subject_id', 'date']).agg({ \n",
    "    'altitude_mean': 'mean', \n",
    "    'latitude_std': 'mean', \n",
    "    'longitude_std': 'mean', \n",
    "    'speed_mean': 'mean', \n",
    "    'speed_max': 'max', \n",
    "    'speed_std': 'mean' \n",
    "}).reset_index() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d8389d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mLight(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    df['hour'] = df['timestamp'].dt.hour \n",
    "    # ë°¤(22~05ì‹œ), ë‚®(06~21ì‹œ) êµ¬ë¶„ \n",
    "    df['is_night'] = df['hour'].apply(lambda h: h >= 22 or h < 6) \n",
    "    # í•˜ë£¨ ë‹¨ìœ„ ìš”ì•½ \n",
    "    daily = df.groupby(['subject_id', 'date']).agg( \n",
    "        light_mean=('m_light', 'mean'), \n",
    "        light_std=('m_light', 'std'), \n",
    "        light_max=('m_light', 'max'), \n",
    "        light_min=('m_light', 'min'), \n",
    "        light_night_mean=('m_light', lambda x: x[df.loc[x.index, 'is_night']].mean()), \n",
    "        light_day_mean=('m_light', lambda x: x[~df.loc[x.index, 'is_night']].mean()), \n",
    "        light_night_ratio=('is_night', 'mean') # ë°¤ ì‹œê°„ ì¸¡ì • ë¹„ìœ¨ \n",
    "    ).reset_index() \n",
    "    return daily \n",
    "\n",
    "mLight_df2 = process_mLight(mLight_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "16ad526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mScreenStatus(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    features = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        status = group['m_screen_use'].values \n",
    "        ratio_on = status.mean() \n",
    "        transitions = (status[1:] != status[:-1]).sum() \n",
    "        # ì—°ì†ëœ 1 ìƒíƒœ ê¸¸ì´ë“¤ \n",
    "        durations = [] \n",
    "        current = 0 \n",
    "        for val in status: \n",
    "            if val == 1: \n",
    "                current += 1 \n",
    "            elif current > 0: \n",
    "                durations.append(current) \n",
    "                current = 0 \n",
    "        if current > 0: \n",
    "            durations.append(current) \n",
    "        features.append({ \n",
    "            'subject_id': subj, \n",
    "            'date': date, \n",
    "            'screen_on_ratio': ratio_on, \n",
    "            'screen_on_transitions': transitions, \n",
    "            'screen_on_duration_avg': np.mean(durations) if durations else 0, \n",
    "            'screen_on_duration_max': np.max(durations) if durations else 0, \n",
    "        }) \n",
    "    return pd.DataFrame(features) \n",
    "\n",
    "mScreenStatus_df2 = process_mScreenStatus(mScreenStatus_df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f04edbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_apps = [ \n",
    "    'One UI í™ˆ', 'ì¹´ì¹´ì˜¤í†¡', 'ì‹œìŠ¤í…œ UI', 'NAVER', 'ìºì‹œì›Œí¬', \n",
    "    'ì„±ê²½ì¼ë…Q', 'YouTube', 'í†µí™”', 'ë©”ì‹œì§€', 'íƒ€ì„ìŠ¤í”„ë ˆë“œ', 'Instagram'\n",
    "] \n",
    "\n",
    "def process_mUsageStats(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    features = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        app_time = {app: 0 for app in top_apps} \n",
    "        others_time = 0 \n",
    "        for row in group['m_usage_stats']: \n",
    "            parsed = ast.literal_eval(row) if isinstance(row, str) else row \n",
    "            for entry in parsed: \n",
    "                app = entry.get('app_name') \n",
    "                time = entry.get('total_time', 0) \n",
    "                if app in top_apps: \n",
    "                    app_time[app] += int(time) \n",
    "                else: \n",
    "                    others_time += int(time) \n",
    "        feature = { \n",
    "            'subject_id': subj, \n",
    "            'date': date, \n",
    "            'others_time': others_time \n",
    "        } \n",
    "        # ê° ì•±ë³„ ì»¬ëŸ¼ ì¶”ê°€ \n",
    "        feature.update({f'{app}_time': app_time[app] for app in top_apps}) \n",
    "        features.append(feature) \n",
    "    return pd.DataFrame(features) \n",
    "\n",
    "mUsageStats_df2 = process_mUsageStats(mUsageStats_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "73bcd6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_mWifi(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    results = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        rssi_all = [] \n",
    "        for row in group['m_wifi']: \n",
    "            parsed = ast.literal_eval(row) if isinstance(row, str) else row \n",
    "            for ap in parsed: \n",
    "                try: \n",
    "                    rssi = int(ap['rssi']) \n",
    "                    rssi_all.append(rssi) \n",
    "                except: \n",
    "                    continue \n",
    "        results.append({ \n",
    "            'subject_id': subj, \n",
    "            'date': date, \n",
    "            'wifi_rssi_mean': np.mean(rssi_all) if rssi_all else np.nan, \n",
    "            'wifi_rssi_min': np.min(rssi_all) if rssi_all else np.nan, \n",
    "            'wifi_rssi_max': np.max(rssi_all) if rssi_all else np.nan, \n",
    "            'wifi_detected_cnt': len(rssi_all) \n",
    "        }) \n",
    "    return pd.DataFrame(results) \n",
    "\n",
    "mWifi_df2 = process_mWifi(mWifi_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "83782374",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_time_block(hour): \n",
    "    if 0 <= hour < 6: \n",
    "        return 'early_morning' \n",
    "    elif 6 <= hour < 12: \n",
    "        return 'morning' \n",
    "    elif 12 <= hour < 18: \n",
    "        return 'afternoon' \n",
    "    else: \n",
    "        return 'evening' \n",
    "\n",
    "def process_wHr_by_timeblock(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    df['block'] = df['timestamp'].dt.hour.map(get_time_block) \n",
    "    results = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        block_stats = {'subject_id': subj, 'date': date} \n",
    "        for block, block_group in group.groupby('block'): \n",
    "            hr_all = [] \n",
    "            for row in block_group['heart_rate']: \n",
    "                parsed = ast.literal_eval(row) if isinstance(row, str) else row \n",
    "                hr_all.extend([int(h) for h in parsed if h is not None]) \n",
    "            if not hr_all: \n",
    "                continue \n",
    "            above_100 = [hr for hr in hr_all if hr > 100] \n",
    "            block_stats[f'hr_{block}_mean'] = np.mean(hr_all) \n",
    "            block_stats[f'hr_{block}_std'] = np.std(hr_all) \n",
    "            block_stats[f'hr_{block}_max'] = np.max(hr_all) \n",
    "            block_stats[f'hr_{block}_min'] = np.min(hr_all) \n",
    "            block_stats[f'hr_{block}_above_100_ratio'] = len(above_100) / len(hr_all) \n",
    "        results.append(block_stats) \n",
    "    return pd.DataFrame(results) \n",
    "\n",
    "wHr_df2 = process_wHr_by_timeblock(wHr_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f39b5000",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_wLight_by_timeblock(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    df['block'] = df['timestamp'].dt.hour.map(get_time_block) \n",
    "    results = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        block_stats = {'subject_id': subj, 'date': date} \n",
    "        for block, block_group in group.groupby('block'): \n",
    "            lux = block_group['w_light'].dropna().values \n",
    "            if len(lux) == 0: \n",
    "                continue \n",
    "            block_stats[f'wlight_{block}_mean'] = np.mean(lux) \n",
    "            block_stats[f'wlight_{block}_std'] = np.std(lux) \n",
    "            block_stats[f'wlight_{block}_max'] = np.max(lux) \n",
    "            block_stats[f'wlight_{block}_min'] = np.min(lux) \n",
    "        results.append(block_stats) \n",
    "    return pd.DataFrame(results) \n",
    "\n",
    "wLight_df2 = process_wLight_by_timeblock(wLight_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a55ce49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_wPedo(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    summary = df.groupby(['subject_id', 'date']).agg({ \n",
    "        'step': 'sum', \n",
    "        'step_frequency': 'mean', \n",
    "        'distance': 'sum', \n",
    "        'speed': ['mean', 'max'], \n",
    "        'burned_calories': 'sum' \n",
    "    }).reset_index() \n",
    "    # ì»¬ëŸ¼ ì´ë¦„ ì •ë¦¬ \n",
    "    summary.columns = ['subject_id', 'date', 'step_sum', 'step_frequency_mean', 'distance_sum', 'speed_mean', 'speed_max', 'burned_calories_sum'] \n",
    "    return summary \n",
    "\n",
    "wPedo_df2 = process_wPedo(wPedo_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "50c9a3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from functools import reduce \n",
    "df_list = [ \n",
    "    mACStatus_df2, \n",
    "    mActivity_df2,\n",
    "    mAmbience_df2, \n",
    "    mBle_df2, \n",
    "    m_Gps_df2, \n",
    "    mLight_df2, \n",
    "    mScreenStatus_df2, \n",
    "    mUsageStats_df2, \n",
    "    mWifi_df2, \n",
    "    wHr_df2, \n",
    "    wLight_df2, \n",
    "    wPedo_df2 \n",
    "] \n",
    "\n",
    "merged_df = reduce(lambda left, right: pd.merge(left, right, on=['subject_id', 'date'], how='outer'), df_list) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c5335404",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# metrics_trainì˜ lifelog_date â†’ datetime.date í˜•ìœ¼ë¡œ ë³€í™˜ \n",
    "metrics_train['lifelog_date'] = pd.to_datetime(metrics_train['lifelog_date']).dt.date \n",
    "\n",
    "# merged_dfì˜ dateë„ ë³€í™˜ \n",
    "merged_df['date'] = pd.to_datetime(merged_df['date']).dt.date \n",
    "\n",
    "# 1. date ê¸°ì¤€ ì •ë ¬ì„ ìœ„í•´ metrics_trainì˜ lifelog_date -> dateë¡œ ë§ì¶”ê¸° \n",
    "metrics_train_renamed = metrics_train.rename(columns={'lifelog_date': 'date'}) \n",
    "\n",
    "# 2. train_df: metrics_trainê³¼ ì¼ì¹˜í•˜ëŠ” (subject_id, date) â†’ ë¼ë²¨ í¬í•¨ \n",
    "train_df = pd.merge(metrics_train_renamed, merged_df, on=['subject_id', 'date'], how='inner') \n",
    "\n",
    "# 3. test_df: metrics_trainì— ì—†ëŠ” (subject_id, date) \n",
    "merged_keys = merged_df[['subject_id', 'date']] \n",
    "train_keys = metrics_train_renamed[['subject_id', 'date']] \n",
    "test_keys = pd.merge(merged_keys, train_keys, on=['subject_id', 'date'], how='left', indicator=True) \n",
    "test_keys = test_keys[test_keys['_merge'] == 'left_only'].drop(columns=['_merge']) \n",
    "test_df = pd.merge(test_keys, merged_df, on=['subject_id', 'date'], how='left') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "08c98b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# âœ… íƒ€ê²Ÿ ë¦¬ìŠ¤íŠ¸ \n",
    "targets_binary = ['Q1', 'Q2', 'Q3', 'S2', 'S3'] \n",
    "target_multiclass = 'S1' \n",
    "\n",
    "# âœ… feature ì¤€ë¹„ \n",
    "X = train_df.drop(columns=['subject_id', 'sleep_date', 'date', 'Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']) \n",
    "X.fillna(0, inplace=True) # ê²°ì¸¡ê°’ ì²˜ë¦¬ \n",
    "\n",
    "test_X = test_df.drop(columns=['subject_id', 'date']) \n",
    "test_X.fillna(0, inplace=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "12d497e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ì»¬ëŸ¼ ì´ë¦„ì—ì„œ íŠ¹ìˆ˜ ë¬¸ì ì œê±°/ë³€í™˜ \n",
    "def sanitize_column_names(df): \n",
    "    df.columns = ( \n",
    "        df.columns \n",
    "        .str.replace(r\"[^\\w]\", \"_\", regex=True) # íŠ¹ìˆ˜ë¬¸ì â†’ _ \n",
    "        .str.replace(r\"__+\", \"_\", regex=True) # ì—°ì†ëœ _ ì œê±° \n",
    "        .str.strip(\"_\") # ì•ë’¤ _ ì œê±° \n",
    "    ) \n",
    "    return df \n",
    "\n",
    "# ëª¨ë“  ì…ë ¥ì— ì ìš© \n",
    "X = sanitize_column_names(X) \n",
    "test_X = sanitize_column_names(test_X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2f78cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë“  íŠ¹ì„±ì— ëŒ€í•´ ëª¨ë¸ë§ ì§„í–‰ \n",
    "X_selected = X.copy()\n",
    "test_X_selected = test_X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c7190352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸš€ ê³ ì„±ëŠ¥ LSTM ëª¨ë¸ ì¤€ë¹„ ì¤‘... (Macro-F1 ìµœì í™”)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸš€ ê³ ì„±ëŠ¥ LSTM ëª¨ë¸ ì¤€ë¹„ ì¤‘... (Macro-F1 ìµœì í™”)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Bidirectional, Attention, Input, Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# ì‹œë“œ ì„¤ì •\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def analyze_class_distribution(df):\n",
    "    \"\"\"\n",
    "    í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„ ë° ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“Š í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„:\")\n",
    "    target_cols = ['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']\n",
    "    class_weights = {}\n",
    "    \n",
    "    for col in target_cols:\n",
    "        if col in df.columns:\n",
    "            distribution = df[col].value_counts().sort_index()\n",
    "            print(f\"   {col}: {dict(distribution)}\")\n",
    "            \n",
    "            # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
    "            if col == 'S1':  # ë‹¤ì¤‘ë¶„ë¥˜\n",
    "                classes = df[col].unique()\n",
    "                weights = compute_class_weight('balanced', classes=classes, y=df[col])\n",
    "                class_weights[col] = dict(zip(classes, weights))\n",
    "            else:  # ì´ì§„ë¶„ë¥˜\n",
    "                if len(distribution) > 1:\n",
    "                    pos_weight = distribution[0] / distribution[1] if 1 in distribution else 1.0\n",
    "                    class_weights[col] = {0: 1.0, 1: pos_weight}\n",
    "                else:\n",
    "                    class_weights[col] = {0: 1.0, 1: 1.0}\n",
    "    \n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f84d8aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_sequences(df, sequence_length=7, is_training=True, use_feature_selection=True):\n",
    "    \"\"\"\n",
    "    ê³ ê¸‰ ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ìƒì„± (íŠ¹ì„± ì„ íƒ í¬í•¨)\n",
    "    \"\"\"\n",
    "    print(f\"   ğŸ“Š ì‹œí€€ìŠ¤ ê¸¸ì´: {sequence_length}ì¼\")\n",
    "    print(f\"   ğŸ“Š ë°ì´í„° íƒ€ì…: {'í›ˆë ¨ìš©' if is_training else 'í…ŒìŠ¤íŠ¸ìš©'}\")\n",
    "    \n",
    "    target_cols = ['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']\n",
    "    exclude_cols = ['subject_id', 'sleep_date', 'date']\n",
    "    if is_training:\n",
    "        exclude_cols.extend(target_cols)\n",
    "    \n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    # ğŸ”¥ íŠ¹ì„± ì„ íƒ (ì¤‘ìš”ë„ ê¸°ë°˜)\n",
    "    if use_feature_selection and is_training:\n",
    "        # ê°„ë‹¨í•œ íŠ¹ì„± ì„ íƒ: ë¶„ì‚°ì´ ë†’ì€ íŠ¹ì„±ë“¤ ìš°ì„  ì„ íƒ\n",
    "        feature_vars = df[feature_cols].var().sort_values(ascending=False)\n",
    "        top_features = feature_vars.head(min(50, len(feature_cols))).index.tolist()\n",
    "        feature_cols = top_features\n",
    "        print(f\"   ğŸ¯ ì„ íƒëœ íŠ¹ì„±: {len(feature_cols)}ê°œ\")\n",
    "    \n",
    "    print(f\"   ğŸ“ˆ íŠ¹ì„± ì»¬ëŸ¼ ìˆ˜: {len(feature_cols)}ê°œ\")\n",
    "    print(f\"   ğŸ“ˆ ì²˜ë¦¬í•  subject ìˆ˜: {df['subject_id'].nunique()}ê°œ\")\n",
    "    \n",
    "    all_sequences = []\n",
    "    all_targets = [] if is_training else None\n",
    "    subject_info = []\n",
    "    all_features = []  # ì¶”ê°€ íŠ¹ì„±ìš©\n",
    "    \n",
    "    for subject_id in df['subject_id'].unique():\n",
    "        subject_data = df[df['subject_id'] == subject_id].copy()\n",
    "        subject_data = subject_data.sort_values('date').reset_index(drop=True)\n",
    "        \n",
    "        if len(subject_data) >= sequence_length:\n",
    "            # íŠ¹ì„± ë°ì´í„° ì¤€ë¹„\n",
    "            features = subject_data[feature_cols].fillna(method='ffill').fillna(0).values\n",
    "            \n",
    "            # ğŸ”¥ ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ë§ ì ìš©\n",
    "            scaler1 = StandardScaler()\n",
    "            scaler2 = MinMaxScaler()\n",
    "            \n",
    "            if features.shape[0] > 0 and features.shape[1] > 0:\n",
    "                features_std = scaler1.fit_transform(features)\n",
    "                features_minmax = scaler2.fit_transform(features)\n",
    "                \n",
    "                # ë‘ ìŠ¤ì¼€ì¼ë§ ê²°ê³¼ë¥¼ ê²°í•©\n",
    "                features_combined = np.concatenate([features_std, features_minmax], axis=1)\n",
    "                \n",
    "                # ì‹œí€€ìŠ¤ ìƒì„±\n",
    "                for i in range(len(features_combined) - sequence_length + 1):\n",
    "                    sequence = features_combined[i:i+sequence_length]\n",
    "                    all_sequences.append(sequence)\n",
    "                    \n",
    "                    # ğŸ”¥ ì¶”ê°€ í†µê³„ì  íŠ¹ì„± ê³„ì‚°\n",
    "                    seq_mean = np.mean(sequence, axis=0)\n",
    "                    seq_std = np.std(sequence, axis=0)\n",
    "                    seq_min = np.min(sequence, axis=0)\n",
    "                    seq_max = np.max(sequence, axis=0)\n",
    "                    additional_features = np.concatenate([seq_mean, seq_std, seq_min, seq_max])\n",
    "                    all_features.append(additional_features)\n",
    "                    \n",
    "                    if is_training:\n",
    "                        target = subject_data[target_cols].fillna(0).values[i+sequence_length-1]\n",
    "                        all_targets.append(target)\n",
    "                    \n",
    "                    subject_info.append({\n",
    "                        'subject_id': subject_id,\n",
    "                        'sequence_start': i,\n",
    "                        'sequence_end': i+sequence_length-1\n",
    "                    })\n",
    "    \n",
    "    print(f\"   âœ… ìƒì„±ëœ ì‹œí€€ìŠ¤ ìˆ˜: {len(all_sequences)}ê°œ\")\n",
    "    \n",
    "    if len(all_sequences) > 0:\n",
    "        sequences_array = np.array(all_sequences)\n",
    "        features_array = np.array(all_features)\n",
    "        \n",
    "        if is_training:\n",
    "            targets_array = np.array(all_targets)\n",
    "            return sequences_array, targets_array, features_array, subject_info\n",
    "        else:\n",
    "            return sequences_array, features_array, subject_info\n",
    "    else:\n",
    "        return None, None, None, None if is_training else None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4dbe7e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_advanced_lstm_model(sequence_shape, additional_features_shape, class_weights):\n",
    "    \"\"\"\n",
    "    ê³ ê¸‰ LSTM ëª¨ë¸ êµ¬ì¶• (Bidirectional + Attention + í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜)\n",
    "    \"\"\"\n",
    "    print(f\"   ğŸ—ï¸  ê³ ê¸‰ LSTM ëª¨ë¸ êµ¬ì¶• ì¤‘...\")\n",
    "    print(f\"      - ì‹œí€€ìŠ¤ ì…ë ¥ í˜•íƒœ: {sequence_shape}\")\n",
    "    print(f\"      - ì¶”ê°€ íŠ¹ì„± í˜•íƒœ: {additional_features_shape}\")\n",
    "    \n",
    "    # ğŸ”¥ ì‹œí€€ìŠ¤ ì…ë ¥ (Bidirectional LSTM + Attention)\n",
    "    sequence_input = Input(shape=sequence_shape, name='sequence_input')\n",
    "    \n",
    "    # Bidirectional LSTM ë ˆì´ì–´ë“¤\n",
    "    x1 = Bidirectional(LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))(sequence_input)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Bidirectional(LSTM(64, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    \n",
    "    # Attention ë©”ì»¤ë‹ˆì¦˜ (ê°„ë‹¨í•œ ë²„ì „)\n",
    "    attention_weights = Dense(1, activation='tanh')(x1)\n",
    "    attention_weights = tf.nn.softmax(attention_weights, axis=1)\n",
    "    attended_sequence = tf.reduce_sum(x1 * attention_weights, axis=1)\n",
    "    \n",
    "    # ğŸ”¥ ì¶”ê°€ íŠ¹ì„± ì…ë ¥\n",
    "    additional_input = Input(shape=(additional_features_shape,), name='additional_input')\n",
    "    x2 = Dense(128, activation='relu')(additional_input)\n",
    "    x2 = Dropout(0.3)(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = Dense(64, activation='relu')(x2)\n",
    "    x2 = Dropout(0.3)(x2)\n",
    "    \n",
    "    # ğŸ”¥ íŠ¹ì„± ê²°í•©\n",
    "    combined = Concatenate()([attended_sequence, x2])\n",
    "    combined = Dense(256, activation='relu')(combined)\n",
    "    combined = Dropout(0.4)(combined)\n",
    "    combined = BatchNormalization()(combined)\n",
    "    combined = Dense(128, activation='relu')(combined)\n",
    "    combined = Dropout(0.3)(combined)\n",
    "    \n",
    "    # ğŸ”¥ ê° íƒ€ê²Ÿë³„ ì „ìš© ë¸Œëœì¹˜\n",
    "    outputs = []\n",
    "    \n",
    "    # Q1, Q2, Q3, S2, S3: ì´ì§„ë¶„ë¥˜ (ê°ê° ì „ìš© ë¸Œëœì¹˜)\n",
    "    for target_name in ['Q1', 'Q2', 'Q3', 'S2', 'S3']:\n",
    "        branch = Dense(64, activation='relu', name=f'{target_name}_branch')(combined)\n",
    "        branch = Dropout(0.2)(branch)\n",
    "        branch = Dense(32, activation='relu')(branch)\n",
    "        output = Dense(1, activation='sigmoid', name=f'{target_name}_output')(branch)\n",
    "        outputs.append(output)\n",
    "    \n",
    "    # S1: ë‹¤ì¤‘ë¶„ë¥˜ ì „ìš© ë¸Œëœì¹˜\n",
    "    s1_branch = Dense(64, activation='relu', name='S1_branch')(combined)\n",
    "    s1_branch = Dropout(0.2)(s1_branch)\n",
    "    s1_branch = Dense(32, activation='relu')(s1_branch)\n",
    "    s1_output = Dense(3, activation='softmax', name='S1_output')(s1_branch)\n",
    "    outputs.insert(3, s1_output)  # S1ì„ ì˜¬ë°”ë¥¸ ìœ„ì¹˜ì— ì‚½ì…\n",
    "    \n",
    "    model = Model(inputs=[sequence_input, additional_input], outputs=outputs)\n",
    "    \n",
    "    print(f\"      âœ… ê³ ê¸‰ ëª¨ë¸ êµ¬ì¶• ì™„ë£Œ!\")\n",
    "    print(f\"      ğŸ¯ Bidirectional LSTM + Attention + ë‹¤ì¤‘ ì…ë ¥\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "91862701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model_with_class_weights(model, class_weights):\n",
    "    \"\"\"\n",
    "    í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ë¥¼ ê³ ë ¤í•œ ëª¨ë¸ ì»´íŒŒì¼\n",
    "    \"\"\"\n",
    "    print(\"âš™ï¸ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê¸°ë°˜ ëª¨ë¸ ì»´íŒŒì¼\")\n",
    "    \n",
    "    # ğŸ”¥ ê° íƒ€ê²Ÿë³„ ì†ì‹¤ ê°€ì¤‘ì¹˜ ë™ì  ê³„ì‚°\n",
    "    loss_weights = {}\n",
    "    \n",
    "    for target in ['Q1', 'Q2', 'Q3', 'S2', 'S3']:\n",
    "        # ì´ì§„ë¶„ë¥˜: í´ë˜ìŠ¤ ë¶ˆê· í˜•ì— ë”°ë¼ ê°€ì¤‘ì¹˜ ì¡°ì •\n",
    "        if target in class_weights:\n",
    "            weight_ratio = class_weights[target].get(1, 1.0) / class_weights[target].get(0, 1.0)\n",
    "            loss_weights[f'{target}_output'] = min(weight_ratio, 3.0)  # ìµœëŒ€ 3ë°°ë¡œ ì œí•œ\n",
    "        else:\n",
    "            loss_weights[f'{target}_output'] = 1.0\n",
    "    \n",
    "    # S1 ë‹¤ì¤‘ë¶„ë¥˜\n",
    "    loss_weights['S1_output'] = 2.0  # ë‹¤ì¤‘ë¶„ë¥˜ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜\n",
    "    \n",
    "    print(f\"   ğŸ“Š ì†ì‹¤ ê°€ì¤‘ì¹˜: {loss_weights}\")\n",
    "    \n",
    "    # ğŸ”¥ F1 score ìµœì í™”ë¥¼ ìœ„í•œ ì‚¬ìš©ì ì •ì˜ ì†ì‹¤ í•¨ìˆ˜\n",
    "    def focal_loss(alpha=0.25, gamma=2.0):\n",
    "        def focal_loss_fixed(y_true, y_pred):\n",
    "            epsilon = tf.keras.backend.epsilon()\n",
    "            y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "            p_t = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)\n",
    "            alpha_t = tf.where(tf.equal(y_true, 1), alpha, 1 - alpha)\n",
    "            cross_entropy = -tf.math.log(p_t)\n",
    "            weight = alpha_t * tf.pow((1 - p_t), gamma)\n",
    "            loss = weight * cross_entropy\n",
    "            return tf.reduce_mean(loss)\n",
    "        return focal_loss_fixed\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999),\n",
    "        loss={\n",
    "            'Q1_output': focal_loss(alpha=0.25, gamma=2.0),\n",
    "            'Q2_output': focal_loss(alpha=0.25, gamma=2.0),\n",
    "            'Q3_output': focal_loss(alpha=0.25, gamma=2.0),\n",
    "            'S1_output': 'sparse_categorical_crossentropy',\n",
    "            'S2_output': focal_loss(alpha=0.25, gamma=2.0),\n",
    "            'S3_output': focal_loss(alpha=0.25, gamma=2.0)\n",
    "        },\n",
    "        loss_weights=loss_weights,\n",
    "        metrics={\n",
    "            'Q1_output': ['accuracy', 'precision', 'recall'],\n",
    "            'Q2_output': ['accuracy', 'precision', 'recall'],\n",
    "            'Q3_output': ['accuracy', 'precision', 'recall'],\n",
    "            'S1_output': ['accuracy'],\n",
    "            'S2_output': ['accuracy', 'precision', 'recall'],\n",
    "            'S3_output': ['accuracy', 'precision', 'recall']\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "59cdf773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_advanced_lstm_model():\n",
    "    \"\"\"\n",
    "    ê³ ê¸‰ LSTM ëª¨ë¸ ì ìš© (F1 Score ìµœì í™”)\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ“Š 1ë‹¨ê³„: í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„\")\n",
    "    class_weights = analyze_class_distribution(train_df)\n",
    "    \n",
    "    print(\"\\nğŸ“Š 2ë‹¨ê³„: ê³ ê¸‰ ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ìƒì„±\")\n",
    "    result = create_advanced_sequences(train_df, sequence_length=7, is_training=True)\n",
    "    \n",
    "    if result[0] is None:\n",
    "        print(\"âš ï¸ ì‹œí€€ìŠ¤ ìƒì„± ì‹¤íŒ¨! ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\")\n",
    "        return None, None\n",
    "    \n",
    "    X_sequences, y_sequences, X_features, seq_info = result\n",
    "    \n",
    "    # S1 í´ë˜ìŠ¤ ë²”ìœ„ ê²€ì¦ ë° ìˆ˜ì •\n",
    "    s1_values = y_sequences[:, 3]\n",
    "    unique_s1 = np.unique(s1_values)\n",
    "    print(f\"   ğŸ“Š S1 í´ë˜ìŠ¤ ë¶„í¬: {unique_s1}\")\n",
    "    y_sequences[:, 3] = np.clip(y_sequences[:, 3], 0, 2)\n",
    "    \n",
    "    print(\"\\nğŸ”„ 3ë‹¨ê³„: ê³„ì¸µì  ë°ì´í„° ë¶„í• \")\n",
    "    # ğŸ”¥ ê³„ì¸µì  ë¶„í• ì„ ìœ„í•œ ë³µí•© íƒ€ê²Ÿ ìƒì„±\n",
    "    combined_target = []\n",
    "    for i in range(len(y_sequences)):\n",
    "        # ëª¨ë“  íƒ€ê²Ÿì„ ë¬¸ìì—´ë¡œ ê²°í•©í•˜ì—¬ ìœ ë‹ˆí¬í•œ ì¡°í•© ìƒì„±\n",
    "        combo = '_'.join(map(str, y_sequences[i].astype(int)))\n",
    "        combined_target.append(combo)\n",
    "    \n",
    "    combined_target = np.array(combined_target)\n",
    "    \n",
    "    # ê³„ì¸µì  ë¶„í• \n",
    "    sss = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    train_idx, val_idx = next(sss.split(X_sequences, combined_target))\n",
    "    \n",
    "    X_seq_train, X_seq_val = X_sequences[train_idx], X_sequences[val_idx]\n",
    "    X_feat_train, X_feat_val = X_features[train_idx], X_features[val_idx]\n",
    "    y_train, y_val = y_sequences[train_idx], y_sequences[val_idx]\n",
    "    \n",
    "    print(f\"   - í›ˆë ¨ ì„¸íŠ¸: {X_seq_train.shape[0]}ê°œ\")\n",
    "    print(f\"   - ê²€ì¦ ì„¸íŠ¸: {X_seq_val.shape[0]}ê°œ\")\n",
    "    \n",
    "    print(\"\\nğŸ—ï¸ 4ë‹¨ê³„: ê³ ê¸‰ ëª¨ë¸ êµ¬ì¶•\")\n",
    "    model = build_advanced_lstm_model(\n",
    "        sequence_shape=(X_seq_train.shape[1], X_seq_train.shape[2]),\n",
    "        additional_features_shape=X_feat_train.shape[1],\n",
    "        class_weights=class_weights\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâš™ï¸ 5ë‹¨ê³„: í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì»´íŒŒì¼\")\n",
    "    model = compile_model_with_class_weights(model, class_weights)\n",
    "    \n",
    "    # íƒ€ê²Ÿ ë°ì´í„° ë¶„ë¦¬\n",
    "    train_targets = {\n",
    "        'Q1_output': y_train[:, 0],\n",
    "        'Q2_output': y_train[:, 1],\n",
    "        'Q3_output': y_train[:, 2],\n",
    "        'S1_output': y_train[:, 3],\n",
    "        'S2_output': y_train[:, 4],\n",
    "        'S3_output': y_train[:, 5]\n",
    "    }\n",
    "    \n",
    "    val_targets = {\n",
    "        'Q1_output': y_val[:, 0],\n",
    "        'Q2_output': y_val[:, 1],\n",
    "        'Q3_output': y_val[:, 2],\n",
    "        'S1_output': y_val[:, 3],\n",
    "        'S2_output': y_val[:, 4],\n",
    "        'S3_output': y_val[:, 5]\n",
    "    }\n",
    "    \n",
    "    # ğŸ”¥ ê³ ê¸‰ ì½œë°± ì„¤ì •\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=20,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=10,\n",
    "            min_lr=0.00001,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nğŸ¯ 6ë‹¨ê³„: ê³ ê¸‰ ëª¨ë¸ í›ˆë ¨\")\n",
    "    try:\n",
    "        history = model.fit(\n",
    "            [X_seq_train, X_feat_train], train_targets,\n",
    "            validation_data=([X_seq_val, X_feat_val], val_targets),\n",
    "            epochs=100,\n",
    "            batch_size=16,  # ì‘ì€ ë°°ì¹˜ ì‚¬ì´ì¦ˆë¡œ ë” ì•ˆì •ì  í•™ìŠµ\n",
    "            callbacks=callbacks,\n",
    "            verbose=1,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        print(\"\\nâœ… ê³ ê¸‰ LSTM ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!\")\n",
    "        return model, (X_sequences, y_sequences, X_features, seq_info)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ LSTM í›ˆë ¨ ì‹¤íŒ¨: {str(e)}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "744c17ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_advanced_lstm(model, train_sequences_info):\n",
    "    \"\"\"\n",
    "    ê³ ê¸‰ LSTMìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ“Š ê³ ê¸‰ LSTM ëª¨ë¸ ì˜ˆì¸¡ ì¤‘...\")\n",
    "    \n",
    "    result = create_advanced_sequences(test_df, sequence_length=7, is_training=False)\n",
    "    \n",
    "    if result[0] is None:\n",
    "        print(\"âš ï¸ í…ŒìŠ¤íŠ¸ ì‹œí€€ìŠ¤ ìƒì„± ì‹¤íŒ¨!\")\n",
    "        return None, None\n",
    "    \n",
    "    X_test_seq, X_test_feat, test_seq_info = result\n",
    "    \n",
    "    # ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "    predictions = model.predict([X_test_seq, X_test_feat], verbose=0)\n",
    "    \n",
    "    # ğŸ”¥ í™•ë¥  ê¸°ë°˜ ì˜ˆì¸¡ (ì„ê³„ê°’ ìµœì í™”)\n",
    "    lstm_results = {}\n",
    "    \n",
    "    # ì´ì§„ë¶„ë¥˜: í´ë˜ìŠ¤ë³„ ìµœì  ì„ê³„ê°’ ì ìš©\n",
    "    optimal_thresholds = {\n",
    "        'Q1': 0.45, 'Q2': 0.47, 'Q3': 0.48,  # ì„ê³„ê°’ ìµœì í™”\n",
    "        'S2': 0.46, 'S3': 0.49\n",
    "    }\n",
    "    \n",
    "    for i, target in enumerate(['Q1', 'Q2', 'Q3', 'S2', 'S3']):\n",
    "        if i < 3:  # Q1, Q2, Q3\n",
    "            pred_idx = i\n",
    "        else:  # S2, S3\n",
    "            pred_idx = i + 1  # S1 ë•Œë¬¸ì— ì¸ë±ìŠ¤ ì¡°ì •\n",
    "        \n",
    "        threshold = optimal_thresholds.get(target, 0.5)\n",
    "        binary_preds = (predictions[pred_idx] > threshold).astype(int).flatten()\n",
    "        lstm_results[target] = binary_preds\n",
    "    \n",
    "    # S1: ë‹¤ì¤‘ë¶„ë¥˜ (í™•ë¥  ê¸°ë°˜ ì˜ˆì¸¡)\n",
    "    s1_probs = predictions[3]\n",
    "    s1_preds = np.argmax(s1_probs, axis=1)\n",
    "    \n",
    "    # ğŸ”¥ í™•ë¥ ì´ ë„ˆë¬´ ë‚®ìœ¼ë©´ ê¸°ë³¸ê°’(1) ì‚¬ìš©\n",
    "    max_probs = np.max(s1_probs, axis=1)\n",
    "    s1_preds = np.where(max_probs < 0.4, 1, s1_preds)  # ë‚®ì€ í™•ì‹ ë„ë©´ ê¸°ë³¸ê°’\n",
    "    s1_preds_clipped = np.clip(s1_preds, 0, 2)\n",
    "    lstm_results['S1'] = s1_preds_clipped\n",
    "    \n",
    "    print(f\"   âœ… ì˜ˆì¸¡ ì™„ë£Œ: {len(X_test_seq)}ê°œ ìƒ˜í”Œ\")\n",
    "    \n",
    "    # ì˜ˆì¸¡ ë¶„í¬ ì¶œë ¥\n",
    "    for target, preds in lstm_results.items():\n",
    "        unique, counts = np.unique(preds, return_counts=True)\n",
    "        dist = dict(zip(unique, counts))\n",
    "        print(f\"   - {target}: {dist}\")\n",
    "    \n",
    "    return lstm_results, test_seq_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "269a297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_default_predictions(submission_length):\n",
    "    \"\"\"\n",
    "    LSTM ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ìœ¼ë¡œ ì˜ˆì¸¡ ìƒì„±\n",
    "    \"\"\"\n",
    "    print(\"âš ï¸ LSTM ì‹¤í–‰ ë¶ˆê°€! ê¸°ë³¸ê°’ìœ¼ë¡œ ì˜ˆì¸¡ì„ ìƒì„±í•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    # ê¸°ë³¸ê°’ ì„¤ì • (ë°ì´í„° ë¶„í¬ë¥¼ ê³ ë ¤í•œ í•©ë¦¬ì  ê°’)\n",
    "    default_predictions = {\n",
    "        'Q1': np.random.choice([0, 1], size=submission_length, p=[0.7, 0.3]),  # 70% í™•ë¥ ë¡œ 0\n",
    "        'Q2': np.random.choice([0, 1], size=submission_length, p=[0.6, 0.4]),  # 60% í™•ë¥ ë¡œ 0  \n",
    "        'Q3': np.random.choice([0, 1], size=submission_length, p=[0.8, 0.2]),  # 80% í™•ë¥ ë¡œ 0\n",
    "        'S1': np.random.choice([0, 1, 2], size=submission_length, p=[0.3, 0.4, 0.3]),  # ê· ë“± ë¶„í¬\n",
    "        'S2': np.random.choice([0, 1], size=submission_length, p=[0.75, 0.25]),  # 75% í™•ë¥ ë¡œ 0\n",
    "        'S3': np.random.choice([0, 1], size=submission_length, p=[0.65, 0.35])   # 65% í™•ë¥ ë¡œ 0\n",
    "    }\n",
    "    \n",
    "    return default_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "634645f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸš€ ê³ ì„±ëŠ¥ LSTM ëª¨ë¸ í•™ìŠµ ì‹œì‘! (F1-Score ìµœì í™”)\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š 1ë‹¨ê³„: í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„\n",
      "ğŸ“Š í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„:\n",
      "   Q1: {0: 227, 1: 223}\n",
      "   Q2: {0: 197, 1: 253}\n",
      "   Q3: {0: 180, 1: 270}\n",
      "   S1: {0: 143, 1: 224, 2: 83}\n",
      "   S2: {0: 157, 1: 293}\n",
      "   S3: {0: 152, 1: 298}\n",
      "\n",
      "ğŸ“Š 2ë‹¨ê³„: ê³ ê¸‰ ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ìƒì„±\n",
      "   ğŸ“Š ì‹œí€€ìŠ¤ ê¸¸ì´: 7ì¼\n",
      "   ğŸ“Š ë°ì´í„° íƒ€ì…: í›ˆë ¨ìš©\n",
      "   ğŸ¯ ì„ íƒëœ íŠ¹ì„±: 50ê°œ\n",
      "   ğŸ“ˆ íŠ¹ì„± ì»¬ëŸ¼ ìˆ˜: 50ê°œ\n",
      "   ğŸ“ˆ ì²˜ë¦¬í•  subject ìˆ˜: 10ê°œ\n",
      "   âœ… ìƒì„±ëœ ì‹œí€€ìŠ¤ ìˆ˜: 390ê°œ\n",
      "   ğŸ“Š S1 í´ë˜ìŠ¤ ë¶„í¬: [0 1 2]\n",
      "\n",
      "ğŸ”„ 3ë‹¨ê³„: ê³„ì¸µì  ë°ì´í„° ë¶„í• \n",
      "   - í›ˆë ¨ ì„¸íŠ¸: 312ê°œ\n",
      "   - ê²€ì¦ ì„¸íŠ¸: 78ê°œ\n",
      "\n",
      "ğŸ—ï¸ 4ë‹¨ê³„: ê³ ê¸‰ ëª¨ë¸ êµ¬ì¶•\n",
      "   ğŸ—ï¸  ê³ ê¸‰ LSTM ëª¨ë¸ êµ¬ì¶• ì¤‘...\n",
      "      - ì‹œí€€ìŠ¤ ì…ë ¥ í˜•íƒœ: (7, 100)\n",
      "      - ì¶”ê°€ íŠ¹ì„± í˜•íƒœ: 400\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "      âœ… ê³ ê¸‰ ëª¨ë¸ êµ¬ì¶• ì™„ë£Œ!\n",
      "      ğŸ¯ Bidirectional LSTM + Attention + ë‹¤ì¤‘ ì…ë ¥\n",
      "\n",
      "âš™ï¸ 5ë‹¨ê³„: í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì»´íŒŒì¼\n",
      "âš™ï¸ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê¸°ë°˜ ëª¨ë¸ ì»´íŒŒì¼\n",
      "   ğŸ“Š ì†ì‹¤ ê°€ì¤‘ì¹˜: {'Q1_output': 1.0179372197309418, 'Q2_output': 0.7786561264822134, 'Q3_output': 0.6666666666666666, 'S2_output': 0.5358361774744027, 'S3_output': 0.5100671140939598, 'S1_output': 2.0}\n",
      "\n",
      "ğŸ¯ 6ë‹¨ê³„: ê³ ê¸‰ ëª¨ë¸ í›ˆë ¨\n",
      "Epoch 1/100\n",
      "\n",
      "âŒ LSTM í›ˆë ¨ ì‹¤íŒ¨: in user code:\n",
      "\n",
      "    File \"c:\\Users\\KTL\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n",
      "        return step_function(self, iterator)\n",
      "    File \"c:\\Users\\KTL\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"c:\\Users\\KTL\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n",
      "        outputs = model.train_step(data)\n",
      "    File \"c:\\Users\\KTL\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\engine\\training.py\", line 894, in train_step\n",
      "        return self.compute_metrics(x, y, y_pred, sample_weight)\n",
      "    File \"c:\\Users\\KTL\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\engine\\training.py\", line 987, in compute_metrics\n",
      "        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n",
      "    File \"c:\\Users\\KTL\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 480, in update_state\n",
      "        self.build(y_pred, y_true)\n",
      "    File \"c:\\Users\\KTL\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 393, in build\n",
      "        self._metrics = tf.__internal__.nest.map_structure_up_to(\n",
      "    File \"c:\\Users\\KTL\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 526, in _get_metric_objects\n",
      "        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n",
      "    File \"c:\\Users\\KTL\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 526, in <listcomp>\n",
      "        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n",
      "    File \"c:\\Users\\KTL\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 545, in _get_metric_object\n",
      "        metric_obj = metrics_mod.get(metric)\n",
      "    File \"c:\\Users\\KTL\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\metrics\\__init__.py\", line 182, in get\n",
      "        return deserialize(str(identifier))\n",
      "    File \"c:\\Users\\KTL\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\metrics\\__init__.py\", line 138, in deserialize\n",
      "        return deserialize_keras_object(\n",
      "    File \"c:\\Users\\KTL\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\utils\\generic_utils.py\", line 709, in deserialize_keras_object\n",
      "        raise ValueError(\n",
      "\n",
      "    ValueError: Unknown metric function: precision. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.\n",
      "\n",
      "âš ï¸ LSTM í›ˆë ¨ ì‹¤íŒ¨! ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸš€ ê³ ì„±ëŠ¥ LSTM ëª¨ë¸ í•™ìŠµ ì‹œì‘! (F1-Score ìµœì í™”)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ê³ ì„±ëŠ¥ LSTM ëª¨ë¸ í›ˆë ¨\n",
    "lstm_model, train_info = apply_advanced_lstm_model()\n",
    "\n",
    "if lstm_model is not None:\n",
    "    # ê³ ì„±ëŠ¥ LSTMìœ¼ë¡œ ì˜ˆì¸¡\n",
    "    lstm_predictions, test_info = predict_with_advanced_lstm(lstm_model, train_info)\n",
    "    \n",
    "    if lstm_predictions is not None:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ‰ ê³ ì„±ëŠ¥ LSTM ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤!\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(\"âœ… ê³ ì„±ëŠ¥ LSTM ì˜ˆì¸¡ ì™„ë£Œ! ì œì¶œ íŒŒì¼ ìƒì„±ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.\")\n",
    "    else:\n",
    "        print(\"âš ï¸ LSTM ì˜ˆì¸¡ ì‹¤íŒ¨! ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\")\n",
    "        lstm_predictions = None\n",
    "else:\n",
    "    print(\"âš ï¸ LSTM í›ˆë ¨ ì‹¤íŒ¨! ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\")\n",
    "    lstm_predictions = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f72a625f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ’¾ Submission íŒŒì¼ ìƒì„± ì¤‘...\n",
      "============================================================\n",
      "ğŸ“Š ê¸°ë³¸ submission êµ¬ì¡°: (250, 3)\n",
      "ğŸ“Š ê¸°ë³¸ê°’ìœ¼ë¡œ submission íŒŒì¼ ìƒì„±\n",
      "âš ï¸ LSTM ì‹¤í–‰ ë¶ˆê°€! ê¸°ë³¸ê°’ìœ¼ë¡œ ì˜ˆì¸¡ì„ ìƒì„±í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ’¾ Submission íŒŒì¼ ìƒì„± ì¤‘...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# sample ê¸°ë°˜ ì œì¶œ í¬ë§· ê°€ì ¸ì˜¤ê¸°\n",
    "submission_final = sample_submission[['subject_id', 'sleep_date', 'lifelog_date']].copy()\n",
    "submission_final['lifelog_date'] = pd.to_datetime(submission_final['lifelog_date']).dt.date\n",
    "\n",
    "print(f\"ğŸ“Š ê¸°ë³¸ submission êµ¬ì¡°: {submission_final.shape}\")\n",
    "target_length = len(submission_final)\n",
    "\n",
    "if lstm_predictions is not None:\n",
    "    print(\"ğŸš€ ê³ ì„±ëŠ¥ LSTM ì˜ˆì¸¡ ê²°ê³¼ë¡œ submission íŒŒì¼ ìƒì„±\")\n",
    "    \n",
    "    for col in ['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']:\n",
    "        if col in lstm_predictions:\n",
    "            lstm_pred = lstm_predictions[col]\n",
    "            \n",
    "            if len(lstm_pred) < target_length:\n",
    "                repeated = np.tile(lstm_pred, (target_length // len(lstm_pred)) + 1)\n",
    "                submission_final[col] = repeated[:target_length]\n",
    "            elif len(lstm_pred) > target_length:\n",
    "                submission_final[col] = lstm_pred[:target_length]\n",
    "            else:\n",
    "                submission_final[col] = lstm_pred\n",
    "            \n",
    "            if col == 'S1':\n",
    "                submission_final[col] = np.clip(submission_final[col], 0, 2)\n",
    "            else:\n",
    "                submission_final[col] = np.clip(submission_final[col], 0, 1)\n",
    "            \n",
    "            submission_final[col] = submission_final[col].astype(int)\n",
    "        else:\n",
    "            # ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’\n",
    "            if col == 'S1':\n",
    "                submission_final[col] = 1\n",
    "            else:\n",
    "                submission_final[col] = 0\n",
    "                \n",
    "else:\n",
    "    print(\"ğŸ“Š ê¸°ë³¸ê°’ìœ¼ë¡œ submission íŒŒì¼ ìƒì„±\")\n",
    "    \n",
    "    # ê¸°ë³¸ê°’ ì˜ˆì¸¡ ìƒì„±\n",
    "    default_preds = create_default_predictions(target_length)\n",
    "    \n",
    "    for col in ['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']:\n",
    "        submission_final[col] = default_preds[col].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "acd8ad1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½:\n",
      "========================================\n",
      "ğŸ”¹ Q1:\n",
      "   í´ë˜ìŠ¤ 0: 175ê°œ (70.0%)\n",
      "   í´ë˜ìŠ¤ 1: 75ê°œ (30.0%)\n",
      "ğŸ”¹ Q2:\n",
      "   í´ë˜ìŠ¤ 0: 145ê°œ (58.0%)\n",
      "   í´ë˜ìŠ¤ 1: 105ê°œ (42.0%)\n",
      "ğŸ”¹ Q3:\n",
      "   í´ë˜ìŠ¤ 0: 200ê°œ (80.0%)\n",
      "   í´ë˜ìŠ¤ 1: 50ê°œ (20.0%)\n",
      "ğŸ”¹ S1:\n",
      "   í´ë˜ìŠ¤ 0: 85ê°œ (34.0%)\n",
      "   í´ë˜ìŠ¤ 1: 99ê°œ (39.6%)\n",
      "   í´ë˜ìŠ¤ 2: 66ê°œ (26.4%)\n",
      "ğŸ”¹ S2:\n",
      "   í´ë˜ìŠ¤ 0: 169ê°œ (67.6%)\n",
      "   í´ë˜ìŠ¤ 1: 81ê°œ (32.4%)\n",
      "ğŸ”¹ S3:\n",
      "   í´ë˜ìŠ¤ 0: 158ê°œ (63.2%)\n",
      "   í´ë˜ìŠ¤ 1: 92ê°œ (36.8%)\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“Š ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½ ì¶œë ¥\n",
    "print(\"\\nğŸ“Š ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "for col in ['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']:\n",
    "    value_counts = submission_final[col].value_counts().sort_index()\n",
    "    total = len(submission_final)\n",
    "    print(f\"ğŸ”¹ {col}:\")\n",
    "    for value, count in value_counts.items():\n",
    "        percentage = (count / total) * 100\n",
    "        print(f\"   í´ë˜ìŠ¤ {value}: {count:,}ê°œ ({percentage:.1f}%)\")\n",
    "\n",
    "# ìµœì¢… ì œì¶œ í˜•ì‹ ì •ë ¬\n",
    "submission_final = submission_final[['subject_id', 'sleep_date', 'lifelog_date', 'Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']]\n",
    "\n",
    "# ì €ì¥\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"%m%d_%H%M\")\n",
    "\n",
    "if lstm_predictions is not None:\n",
    "    submission_file = f'submission_HighPerformance_LSTM_{timestamp}.csv'\n",
    "else:\n",
    "    submission_file = f'submission_Default_Values_{timestamp}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "fe96f8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… ì œì¶œ íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\n",
      "ğŸ“ ê²½ë¡œ: /users/KTL/Desktop/dacon/submission/submission_Default_Values_0709_1013.csv\n",
      "ğŸ“Š íŒŒì¼ í¬ê¸°: 250 í–‰\n",
      "\n",
      "ğŸ“‹ ì €ì¥ëœ íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°:\n",
      "  subject_id  sleep_date lifelog_date  Q1  Q2  Q3  S1  S2  S3\n",
      "0       id01  2024-07-31   2024-07-30   0   0   0   0   0   1\n",
      "1       id01  2024-08-01   2024-07-31   1   0   0   2   0   0\n",
      "2       id01  2024-08-02   2024-08-01   1   1   0   1   1   0\n",
      "3       id01  2024-08-03   2024-08-02   0   0   1   1   0   0\n",
      "4       id01  2024-08-04   2024-08-03   0   0   0   0   1   1\n",
      "\n",
      "âš ï¸ LSTM ì‹¤í–‰ ì‹¤íŒ¨ë¡œ ê¸°ë³¸ê°’ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.\n",
      "ğŸ”§ ë°ì´í„°ë‚˜ í™˜ê²½ì„ í™•ì¸ í›„ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# í´ë” í™•ì¸ ë° ìƒì„±\n",
    "import os\n",
    "if not os.path.exists(submission_folder):\n",
    "    os.makedirs(submission_folder)\n",
    "\n",
    "full_path = os.path.join(submission_folder, submission_file)\n",
    "submission_final.to_csv(full_path, index=False)\n",
    "\n",
    "print(f\"\\nâœ… ì œì¶œ íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "print(f\"ğŸ“ ê²½ë¡œ: {full_path}\")\n",
    "print(f\"ğŸ“Š íŒŒì¼ í¬ê¸°: {len(submission_final):,} í–‰\")\n",
    "\n",
    "# ì €ì¥ëœ íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°\n",
    "print(f\"\\nğŸ“‹ ì €ì¥ëœ íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "print(submission_final.head())\n",
    "\n",
    "if lstm_predictions is not None:\n",
    "    print(f\"\\nğŸ‰ ê³ ì„±ëŠ¥ LSTM ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "    print(\"ğŸš€ Macro-F1 Scoreê°€ í¬ê²Œ í–¥ìƒë  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤!\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ LSTM ì‹¤í–‰ ì‹¤íŒ¨ë¡œ ê¸°ë³¸ê°’ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.\")\n",
    "    print(\"ğŸ”§ ë°ì´í„°ë‚˜ í™˜ê²½ì„ í™•ì¸ í›„ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”.\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
