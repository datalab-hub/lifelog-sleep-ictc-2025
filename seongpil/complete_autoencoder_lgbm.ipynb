{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6536d54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### feature ëŒ€ìƒìœ¼ë¡œ LGBM í•™ìŠµ with Autoencoder Enhancement\n",
    "# (Developed from dacon_etri_base_mod1.ipynb)\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# í˜„ì¬ ë‚ ì§œ ë° ì‹œê°„ ê°€ì ¸ì˜¤ê¸°\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"%m%d_%H%M\")  # ì˜ˆ: 0517_1530\n",
    "\n",
    "submission_folder = '/users/KTL/Desktop/dacon/submission/'\n",
    "submission_file = f'submission_autoencoder_final_mod4_1_6_{timestamp}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdfb2832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================== ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ===========================\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import glob \n",
    "import random \n",
    "import os \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import ast \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from lightgbm import LGBMClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "791fe7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================== ì‹œë“œ ê³ ì • ===========================\n",
    "\n",
    "# seed ê³ ì • \n",
    "SD = 42 \n",
    "random.seed(SD) \n",
    "np.random.seed(SD) \n",
    "os.environ['PYTHONHASHSEED'] = str(SD)\n",
    "tf.random.set_seed(SD)  # TensorFlow ì‹œë“œ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6906b814",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================== íŒŒì¼ ê²½ë¡œ ì„¤ì • ===========================\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ ì„¤ì • - VSCode ìƒëŒ€ê²½ë¡œë¡œ ë³€ê²½\n",
    "# ì‹¤ì œ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì • í•„ìš”\n",
    "base_folder =  '/users/KTL/Desktop/ETRI_lifelog_dataset'\n",
    "folder = '/ch2025_data_items'\n",
    "\n",
    "data_dir = base_folder + folder \n",
    "\n",
    "# Parquet íŒŒì¼ ì „ì²´ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ \n",
    "parquet_files = glob.glob(os.path.join(data_dir, 'ch2025_*.parquet')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ee3b1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded: mACStatus, shape = (939896, 3)\n",
      "âœ… Loaded: mActivity, shape = (961062, 3)\n",
      "âœ… Loaded: mAmbience, shape = (476577, 3)\n",
      "âœ… Loaded: mBle, shape = (21830, 3)\n",
      "âœ… Loaded: mGps, shape = (800611, 3)\n",
      "âœ… Loaded: mLight, shape = (96258, 3)\n",
      "âœ… Loaded: mScreenStatus, shape = (939653, 3)\n",
      "âœ… Loaded: mUsageStats, shape = (45197, 3)\n",
      "âœ… Loaded: mWifi, shape = (76336, 3)\n",
      "âœ… Loaded: wHr, shape = (382918, 3)\n",
      "âœ… Loaded: wLight, shape = (633741, 3)\n",
      "âœ… Loaded: wPedo, shape = (748100, 9)\n",
      "â³ mACStatus ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mACStatus_test â†’ (335849, 3), mACStatus_train â†’ (604047, 3)\n",
      "â³ mActivity ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mActivity_test â†’ (343579, 3), mActivity_train â†’ (617483, 3)\n",
      "â³ mAmbience ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mAmbience_test â†’ (170453, 3), mAmbience_train â†’ (306124, 3)\n",
      "â³ mBle ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mBle_test â†’ (8140, 3), mBle_train â†’ (13690, 3)\n",
      "â³ mGps ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mGps_test â†’ (287386, 3), mGps_train â†’ (513225, 3)\n",
      "â³ mLight ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mLight_test â†’ (34439, 3), mLight_train â†’ (61819, 3)\n",
      "â³ mScreenStatus ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mScreenStatus_test â†’ (336160, 3), mScreenStatus_train â†’ (603493, 3)\n",
      "â³ mUsageStats ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mUsageStats_test â†’ (16499, 3), mUsageStats_train â†’ (28698, 3)\n",
      "â³ mWifi ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mWifi_test â†’ (27467, 3), mWifi_train â†’ (48869, 3)\n",
      "â³ wHr ë¶„ë¦¬ ì¤‘...\n",
      "âœ… wHr_test â†’ (143311, 3), wHr_train â†’ (239607, 3)\n",
      "â³ wLight ë¶„ë¦¬ ì¤‘...\n",
      "âœ… wLight_test â†’ (233809, 3), wLight_train â†’ (399932, 3)\n",
      "â³ wPedo ë¶„ë¦¬ ì¤‘...\n",
      "âœ… wPedo_test â†’ (288832, 9), wPedo_train â†’ (459268, 9)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =========================== ë°ì´í„° ë¡œë”© ===========================\n",
    "\n",
    "# íŒŒì¼ ì´ë¦„ì„ í‚¤ë¡œ, DataFrameì„ ê°’ìœ¼ë¡œ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ \n",
    "lifelog_data = {} \n",
    "\n",
    "# íŒŒì¼ë³„ë¡œ ì½ê¸° \n",
    "for file_path in parquet_files: \n",
    "    name = os.path.basename(file_path).replace('.parquet', '').replace('ch2025_', '') \n",
    "    lifelog_data[name] = pd.read_parquet(file_path) \n",
    "    print(f\"âœ… Loaded: {name}, shape = {lifelog_data[name].shape}\") \n",
    "\n",
    "# ë”•ì…”ë„ˆë¦¬ì— ìˆëŠ” ëª¨ë“  í•­ëª©ì„ ë…ë¦½ì ì¸ ë³€ìˆ˜ë¡œ í• ë‹¹ \n",
    "for key, df in lifelog_data.items(): \n",
    "    globals()[f\"{key}_df\"] = df \n",
    "\n",
    "# ë©”íŠ¸ë¦­ìŠ¤ íŒŒì¼ ì½ê¸°\n",
    "metrics_train = pd.read_csv(base_folder + '/ch2025_metrics_train.csv')\n",
    "sample_submission = pd.read_csv(base_folder+'/ch2025_submission_sample.csv')\n",
    "\n",
    "# âœ… ê¸°ì¤€ ìŒ (subject_id, lifelog_date) \n",
    "sample_submission['lifelog_date'] = pd.to_datetime(sample_submission['lifelog_date']) \n",
    "test_keys = set(zip(sample_submission['subject_id'], sample_submission['lifelog_date'].dt.date)) \n",
    "\n",
    "# âœ… DataFrame ë³„ timestamp ì»¬ëŸ¼ ìˆ˜ë™ ì§€ì • \n",
    "dataframes = { \n",
    "    'mACStatus': (mACStatus_df, 'timestamp'), \n",
    "    'mActivity': (mActivity_df, 'timestamp'), \n",
    "    'mAmbience': (mAmbience_df, 'timestamp'), \n",
    "    'mBle': (mBle_df, 'timestamp'), \n",
    "    'mGps': (mGps_df, 'timestamp'), \n",
    "    'mLight': (mLight_df, 'timestamp'), \n",
    "    'mScreenStatus': (mScreenStatus_df, 'timestamp'), \n",
    "    'mUsageStats': (mUsageStats_df, 'timestamp'), \n",
    "    'mWifi': (mWifi_df, 'timestamp'), \n",
    "    'wHr': (wHr_df, 'timestamp'), \n",
    "    'wLight': (wLight_df, 'timestamp'), \n",
    "    'wPedo': (wPedo_df, 'timestamp'), \n",
    "} \n",
    "\n",
    "# âœ… ë¶„ë¦¬ í•¨ìˆ˜ \n",
    "def split_test_train(df, subject_col='subject_id', timestamp_col='timestamp'): \n",
    "    df[timestamp_col] = pd.to_datetime(df[timestamp_col], errors='coerce') \n",
    "    df = df.dropna(subset=[timestamp_col]) \n",
    "    df['date_only'] = df[timestamp_col].dt.date \n",
    "    df['key'] = list(zip(df[subject_col], df['date_only'])) \n",
    "    test_df = df[df['key'].isin(test_keys)].drop(columns=['date_only', 'key']) \n",
    "    train_df = df[~df['key'].isin(test_keys)].drop(columns=['date_only', 'key']) \n",
    "    return test_df, train_df \n",
    "\n",
    "# âœ… ê²°ê³¼ ì €ì¥ \n",
    "for name, (df, ts_col) in dataframes.items(): \n",
    "    print(f\"â³ {name} ë¶„ë¦¬ ì¤‘...\") \n",
    "    test_df, train_df = split_test_train(df.copy(), subject_col='subject_id', timestamp_col=ts_col) \n",
    "    globals()[f\"{name}_test\"] = test_df \n",
    "    globals()[f\"{name}_train\"] = train_df \n",
    "    print(f\"âœ… {name}_test â†’ {test_df.shape}, {name}_train â†’ {train_df.shape}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02c832f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Feature Engineering ì‹œì‘ ===\n",
      "âœ… mACStatus ì²˜ë¦¬ ì™„ë£Œ\n",
      "âœ… mActivity ì²˜ë¦¬ ì™„ë£Œ\n",
      "âœ… mAmbience ì²˜ë¦¬ ì™„ë£Œ\n",
      "âœ… mBle ì²˜ë¦¬ ì™„ë£Œ\n",
      "âœ… mGps ì²˜ë¦¬ ì™„ë£Œ\n",
      "âœ… mLight ì²˜ë¦¬ ì™„ë£Œ\n",
      "âœ… mScreenStatus ì²˜ë¦¬ ì™„ë£Œ\n",
      "âœ… mUsageStats ì²˜ë¦¬ ì™„ë£Œ\n",
      "âœ… mWifi ì²˜ë¦¬ ì™„ë£Œ\n",
      "âœ… wHr ì²˜ë¦¬ ì™„ë£Œ\n",
      "âœ… wLight ì²˜ë¦¬ ì™„ë£Œ\n",
      "âœ… wPedo ì²˜ë¦¬ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# =========================== FEATURE ENGINEERING ===========================\n",
    "\n",
    "def process_mACStatus(df): \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    df = df.sort_values(['subject_id', 'timestamp']) \n",
    "    results = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        status = group['m_charging'].values # 0/1 ìƒíƒœ \n",
    "        times = group['timestamp'].values # ì¶©ì „ ìƒíƒœ ë¹„ìœ¨ \n",
    "        ratio_charging = status.mean() \n",
    "        # ìƒíƒœ ì „ì´ íšŸìˆ˜ \n",
    "        transitions = (status[1:] != status[:-1]).sum() \n",
    "        # ì—°ì†ëœ 1 ìƒíƒœ ê¸¸ì´ë“¤ \n",
    "        lengths = [] \n",
    "        current_len = 0 \n",
    "        for val in status: \n",
    "            if val == 1: \n",
    "                current_len += 1 \n",
    "            elif current_len > 0: \n",
    "                lengths.append(current_len) \n",
    "                current_len = 0 \n",
    "        if current_len > 0: \n",
    "            lengths.append(current_len) \n",
    "        avg_charging_duration = np.mean(lengths) if lengths else 0 \n",
    "        max_charging_duration = np.max(lengths) if lengths else 0 \n",
    "        results.append({ \n",
    "            'subject_id': subj, \n",
    "            'date': date, \n",
    "            'charging_ratio': ratio_charging, \n",
    "            'charging_transitions': transitions, \n",
    "            'avg_charging_duration': avg_charging_duration, \n",
    "            'max_charging_duration': max_charging_duration, \n",
    "        }) \n",
    "    return pd.DataFrame(results) \n",
    "\n",
    "def process_mActivity(df): \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    summary = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        counts = group['m_activity'].value_counts(normalize=True) # ë¹„ìœ¨ \n",
    "        row = {'subject_id': subj, 'date': date} \n",
    "        # 0~8 ë¹„ìœ¨ ì €ì¥ \n",
    "        for i in range(9): \n",
    "            row[f'activity_{i}_ratio'] = counts.get(i, 0) \n",
    "        # ì£¼ìš” í™œë™ ì •ë³´ \n",
    "        row['dominant_activity'] = group['m_activity'].mode()[0] \n",
    "        row['num_unique_activities'] = group['m_activity'].nunique() \n",
    "        summary.append(row) \n",
    "    return pd.DataFrame(summary) \n",
    "\n",
    "# ì§€ì •ëœ 10ê°œ ë¼ë²¨ \n",
    "top_10_labels = [ \n",
    "    \"Inside, small room\", \"Speech\", \"Silence\", \"Music\", \"Narration, monologue\", \n",
    "    \"Child speech, kid speaking\", \"Conversation\", \"Speech synthesizer\", \"Shout\", \"Babbling\" \n",
    "] \n",
    "\n",
    "def process_mAmbience_top10(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    # ì´ˆê¸°í™” \n",
    "    for label in top_10_labels + ['others']: \n",
    "        df[label] = 0.0 \n",
    "    for idx, row in df.iterrows(): \n",
    "        parsed = ast.literal_eval(row['m_ambience']) if isinstance(row['m_ambience'], str) else row['m_ambience'] \n",
    "        others_prob = 0.0 \n",
    "        for label, prob in parsed: \n",
    "            prob = float(prob) \n",
    "            if label in top_10_labels: \n",
    "                df.at[idx, label] = prob \n",
    "            else: \n",
    "                others_prob += prob \n",
    "        df.at[idx, 'others'] = others_prob \n",
    "    return df.drop(columns=['m_ambience']) \n",
    "\n",
    "def summarize_mAmbience_daily(df): \n",
    "    prob_cols = [col for col in df.columns if col not in ['subject_id', 'timestamp', 'date']] \n",
    "    # í•˜ë£¨ ë‹¨ìœ„ë¡œ í‰ê· ê°’ ìš”ì•½ \n",
    "    daily_summary = df.groupby(['subject_id', 'date'])[prob_cols].mean().reset_index() \n",
    "    return daily_summary \n",
    "\n",
    "def process_mBle(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    features = [] \n",
    "    for idx, row in df.iterrows(): \n",
    "        entry = ast.literal_eval(row['m_ble']) if isinstance(row['m_ble'], str) else row['m_ble'] \n",
    "        rssi_list = [] \n",
    "        class_0_cnt = 0 \n",
    "        class_other_cnt = 0 \n",
    "        for device in entry: \n",
    "            try: \n",
    "                rssi = int(device['rssi']) \n",
    "                rssi_list.append(rssi) \n",
    "                if str(device['device_class']) == '0': \n",
    "                    class_0_cnt += 1 \n",
    "                else: \n",
    "                    class_other_cnt += 1 \n",
    "            except: \n",
    "                continue # malformed record \n",
    "        feature = { \n",
    "            'subject_id': row['subject_id'], \n",
    "            'date': row['date'], \n",
    "            'device_class_0_cnt': class_0_cnt, \n",
    "            'device_class_others_cnt': class_other_cnt, \n",
    "            'device_count': len(rssi_list), \n",
    "            'rssi_mean': np.mean(rssi_list) if rssi_list else np.nan, \n",
    "            'rssi_min': np.min(rssi_list) if rssi_list else np.nan, \n",
    "            'rssi_max': np.max(rssi_list) if rssi_list else np.nan, \n",
    "        } \n",
    "        features.append(feature) \n",
    "    return pd.DataFrame(features) \n",
    "\n",
    "def summarize_mBle_daily(df): \n",
    "    # row ë‹¨ìœ„ BLE feature ì¶”ì¶œ \n",
    "    df = process_mBle(df) \n",
    "    # í•˜ë£¨ ë‹¨ìœ„ë¡œ cnt í•©ì¹˜ê¸° \n",
    "    grouped = df.groupby(['subject_id', 'date']).agg({ \n",
    "        'device_class_0_cnt': 'sum', \n",
    "        'device_class_others_cnt': 'sum', \n",
    "        'rssi_mean': 'mean', \n",
    "        'rssi_min': 'min', \n",
    "        'rssi_max': 'max', \n",
    "    }).reset_index() \n",
    "    # ì´í•© êµ¬í•´ì„œ ë¹„ìœ¨ ê³„ì‚° \n",
    "    total_cnt = grouped['device_class_0_cnt'] + grouped['device_class_others_cnt'] \n",
    "    grouped['device_class_0_ratio'] = grouped['device_class_0_cnt'] / total_cnt.replace(0, np.nan) \n",
    "    grouped['device_class_others_ratio'] = grouped['device_class_others_cnt'] / total_cnt.replace(0, np.nan) \n",
    "    # í•„ìš” ì—†ëŠ” ì›ë˜ cnt ì»¬ëŸ¼ ì œê±° \n",
    "    grouped.drop(columns=['device_class_0_cnt', 'device_class_others_cnt'], inplace=True) \n",
    "    return grouped \n",
    "\n",
    "def process_mGps(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    features = [] \n",
    "    for idx, row in df.iterrows(): \n",
    "        gps_list = ast.literal_eval(row['m_gps']) if isinstance(row['m_gps'], str) else row['m_gps'] \n",
    "        altitudes = [] \n",
    "        latitudes = [] \n",
    "        longitudes = [] \n",
    "        speeds = [] \n",
    "        for entry in gps_list: \n",
    "            try: \n",
    "                altitudes.append(float(entry['altitude'])) \n",
    "                latitudes.append(float(entry['latitude'])) \n",
    "                longitudes.append(float(entry['longitude'])) \n",
    "                speeds.append(float(entry['speed'])) \n",
    "            except: \n",
    "                continue \n",
    "        features.append({ \n",
    "            'subject_id': row['subject_id'], \n",
    "            'date': row['date'], \n",
    "            'altitude_mean': np.mean(altitudes) if altitudes else np.nan, \n",
    "            'latitude_std': np.std(latitudes) if latitudes else np.nan, \n",
    "            'longitude_std': np.std(longitudes) if longitudes else np.nan, \n",
    "            'speed_mean': np.mean(speeds) if speeds else np.nan, \n",
    "            'speed_max': np.max(speeds) if speeds else np.nan, \n",
    "            'speed_std': np.std(speeds) if speeds else np.nan, \n",
    "        }) \n",
    "    return pd.DataFrame(features) \n",
    "\n",
    "def process_mLight(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    df['hour'] = df['timestamp'].dt.hour \n",
    "    # ë°¤(22~05ì‹œ), ë‚®(06~21ì‹œ) êµ¬ë¶„ \n",
    "    df['is_night'] = df['hour'].apply(lambda h: h >= 22 or h < 6) \n",
    "    # í•˜ë£¨ ë‹¨ìœ„ ìš”ì•½ \n",
    "    daily = df.groupby(['subject_id', 'date']).agg( \n",
    "        light_mean=('m_light', 'mean'), \n",
    "        light_std=('m_light', 'std'), \n",
    "        light_max=('m_light', 'max'), \n",
    "        light_min=('m_light', 'min'), \n",
    "        light_night_mean=('m_light', lambda x: x[df.loc[x.index, 'is_night']].mean()), \n",
    "        light_day_mean=('m_light', lambda x: x[~df.loc[x.index, 'is_night']].mean()), \n",
    "        light_night_ratio=('is_night', 'mean') # ë°¤ ì‹œê°„ ì¸¡ì • ë¹„ìœ¨ \n",
    "    ).reset_index() \n",
    "    return daily \n",
    "\n",
    "def process_mScreenStatus(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    features = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        status = group['m_screen_use'].values \n",
    "        ratio_on = status.mean() \n",
    "        transitions = (status[1:] != status[:-1]).sum() \n",
    "        # ì—°ì†ëœ 1 ìƒíƒœ ê¸¸ì´ë“¤ \n",
    "        durations = [] \n",
    "        current = 0 \n",
    "        for val in status: \n",
    "            if val == 1: \n",
    "                current += 1 \n",
    "            elif current > 0: \n",
    "                durations.append(current) \n",
    "                current = 0 \n",
    "        if current > 0: \n",
    "            durations.append(current) \n",
    "        features.append({ \n",
    "            'subject_id': subj, \n",
    "            'date': date, \n",
    "            'screen_on_ratio': ratio_on, \n",
    "            'screen_on_transitions': transitions, \n",
    "            'screen_on_duration_avg': np.mean(durations) if durations else 0, \n",
    "            'screen_on_duration_max': np.max(durations) if durations else 0, \n",
    "        }) \n",
    "    return pd.DataFrame(features) \n",
    "\n",
    "top_apps = [ \n",
    "    'One UI í™ˆ', 'ì¹´ì¹´ì˜¤í†¡', 'ì‹œìŠ¤í…œ UI', 'NAVER', 'ìºì‹œì›Œí¬', \n",
    "    'ì„±ê²½ì¼ë…Q', 'YouTube', 'í†µí™”', 'ë©”ì‹œì§€', 'íƒ€ì„ìŠ¤í”„ë ˆë“œ', 'Instagram'\n",
    "] \n",
    "\n",
    "def process_mUsageStats(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    features = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        app_time = {app: 0 for app in top_apps} \n",
    "        others_time = 0 \n",
    "        for row in group['m_usage_stats']: \n",
    "            parsed = ast.literal_eval(row) if isinstance(row, str) else row \n",
    "            for entry in parsed: \n",
    "                app = entry.get('app_name') \n",
    "                time = entry.get('total_time', 0) \n",
    "                if app in top_apps: \n",
    "                    app_time[app] += int(time) \n",
    "                else: \n",
    "                    others_time += int(time) \n",
    "        feature = { \n",
    "            'subject_id': subj, \n",
    "            'date': date, \n",
    "            'others_time': others_time \n",
    "        } \n",
    "        # ê° ì•±ë³„ ì»¬ëŸ¼ ì¶”ê°€ \n",
    "        feature.update({f'{app}_time': app_time[app] for app in top_apps}) \n",
    "        features.append(feature) \n",
    "    return pd.DataFrame(features) \n",
    "\n",
    "def process_mWifi(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    results = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        rssi_all = [] \n",
    "        for row in group['m_wifi']: \n",
    "            parsed = ast.literal_eval(row) if isinstance(row, str) else row \n",
    "            for ap in parsed: \n",
    "                try: \n",
    "                    rssi = int(ap['rssi']) \n",
    "                    rssi_all.append(rssi) \n",
    "                except: \n",
    "                    continue \n",
    "        results.append({ \n",
    "            'subject_id': subj, \n",
    "            'date': date, \n",
    "            'wifi_rssi_mean': np.mean(rssi_all) if rssi_all else np.nan, \n",
    "            'wifi_rssi_min': np.min(rssi_all) if rssi_all else np.nan, \n",
    "            'wifi_rssi_max': np.max(rssi_all) if rssi_all else np.nan, \n",
    "            'wifi_detected_cnt': len(rssi_all) \n",
    "        }) \n",
    "    return pd.DataFrame(results) \n",
    "\n",
    "def get_time_block(hour): \n",
    "    if 0 <= hour < 6: \n",
    "        return 'early_morning' \n",
    "    elif 6 <= hour < 12: \n",
    "        return 'morning' \n",
    "    elif 12 <= hour < 18: \n",
    "        return 'afternoon' \n",
    "    else: \n",
    "        return 'evening' \n",
    "\n",
    "def process_wHr_by_timeblock(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    df['block'] = df['timestamp'].dt.hour.map(get_time_block) \n",
    "    results = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        block_stats = {'subject_id': subj, 'date': date} \n",
    "        for block, block_group in group.groupby('block'): \n",
    "            hr_all = [] \n",
    "            for row in block_group['heart_rate']: \n",
    "                parsed = ast.literal_eval(row) if isinstance(row, str) else row \n",
    "                hr_all.extend([int(h) for h in parsed if h is not None]) \n",
    "            if not hr_all: \n",
    "                continue \n",
    "            above_100 = [hr for hr in hr_all if hr > 100] \n",
    "            block_stats[f'hr_{block}_mean'] = np.mean(hr_all) \n",
    "            block_stats[f'hr_{block}_std'] = np.std(hr_all) \n",
    "            block_stats[f'hr_{block}_max'] = np.max(hr_all) \n",
    "            block_stats[f'hr_{block}_min'] = np.min(hr_all) \n",
    "            block_stats[f'hr_{block}_above_100_ratio'] = len(above_100) / len(hr_all) \n",
    "        results.append(block_stats) \n",
    "    return pd.DataFrame(results) \n",
    "\n",
    "def process_wLight_by_timeblock(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    df['block'] = df['timestamp'].dt.hour.map(get_time_block) \n",
    "    results = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        block_stats = {'subject_id': subj, 'date': date} \n",
    "        for block, block_group in group.groupby('block'): \n",
    "            lux = block_group['w_light'].dropna().values \n",
    "            if len(lux) == 0: \n",
    "                continue \n",
    "            block_stats[f'wlight_{block}_mean'] = np.mean(lux) \n",
    "            block_stats[f'wlight_{block}_std'] = np.std(lux) \n",
    "            block_stats[f'wlight_{block}_max'] = np.max(lux) \n",
    "            block_stats[f'wlight_{block}_min'] = np.min(lux) \n",
    "        results.append(block_stats) \n",
    "    return pd.DataFrame(results) \n",
    "\n",
    "def process_wPedo(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    summary = df.groupby(['subject_id', 'date']).agg({ \n",
    "        'step': 'sum', \n",
    "        'step_frequency': 'mean', \n",
    "        'distance': 'sum', \n",
    "        'speed': ['mean', 'max'], \n",
    "        'burned_calories': 'sum' \n",
    "    }).reset_index() \n",
    "    # ì»¬ëŸ¼ ì´ë¦„ ì •ë¦¬ \n",
    "    summary.columns = ['subject_id', 'date', 'step_sum', 'step_frequency_mean', 'distance_sum', 'speed_mean', 'speed_max', 'burned_calories_sum'] \n",
    "    return summary \n",
    "\n",
    "# ëª¨ë“  feature processing ì‹¤í–‰\n",
    "print(\"=== Feature Engineering ì‹œì‘ ===\")\n",
    "\n",
    "mACStatus_df2 = process_mACStatus(mACStatus_df)\n",
    "print(\"âœ… mACStatus ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "mActivity_df2 = process_mActivity(mActivity_df)\n",
    "print(\"âœ… mActivity ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "mAmbience_df2= process_mAmbience_top10(mAmbience_df)\n",
    "mAmbience_df2 = summarize_mAmbience_daily(mAmbience_df2)\n",
    "print(\"âœ… mAmbience ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "mBle_df2 = summarize_mBle_daily(mBle_df)\n",
    "print(\"âœ… mBle ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "m_Gps_df2 = process_mGps(mGps_df)\n",
    "m_Gps_df2 = m_Gps_df2.groupby(['subject_id', 'date']).agg({\n",
    "    'altitude_mean': 'mean',\n",
    "    'latitude_std': 'mean',\n",
    "    'longitude_std': 'mean',\n",
    "    'speed_mean': 'mean',\n",
    "    'speed_max': 'max',\n",
    "    'speed_std': 'mean'\n",
    "}).reset_index()\n",
    "print(\"âœ… mGps ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "mLight_df2 = process_mLight(mLight_df)\n",
    "print(\"âœ… mLight ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "mScreenStatus_df2 = process_mScreenStatus(mScreenStatus_df)\n",
    "print(\"âœ… mScreenStatus ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "mUsageStats_df2 = process_mUsageStats(mUsageStats_df)\n",
    "print(\"âœ… mUsageStats ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "mWifi_df2 = process_mWifi(mWifi_df)\n",
    "print(\"âœ… mWifi ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "wHr_df2 = process_wHr_by_timeblock(wHr_df)\n",
    "print(\"âœ… wHr ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "wLight_df2 = process_wLight_by_timeblock(wLight_df)\n",
    "print(\"âœ… wLight ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "wPedo_df2 = process_wPedo(wPedo_df)\n",
    "print(\"âœ… wPedo ì²˜ë¦¬ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2db554b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë°ì´í„° ë³‘í•© ì™„ë£Œ: train_df=(450, 115), test_df=(250, 108)\n"
     ]
    }
   ],
   "source": [
    "# =========================== ë°ì´í„° ë³‘í•© ===========================\n",
    "\n",
    "df_list = [ \n",
    "    mACStatus_df2, \n",
    "    mActivity_df2,\n",
    "    mAmbience_df2, \n",
    "    mBle_df2, \n",
    "    m_Gps_df2, \n",
    "    mLight_df2, \n",
    "    mScreenStatus_df2, \n",
    "    mUsageStats_df2, \n",
    "    mWifi_df2, \n",
    "    wHr_df2, \n",
    "    wLight_df2, \n",
    "    wPedo_df2 \n",
    "] \n",
    "\n",
    "merged_df = reduce(lambda left, right: pd.merge(left, right, on=['subject_id', 'date'], how='outer'), df_list) \n",
    "\n",
    "# metrics_trainì˜ lifelog_date â†’ datetime.date í˜•ìœ¼ë¡œ ë³€í™˜ \n",
    "metrics_train['lifelog_date'] = pd.to_datetime(metrics_train['lifelog_date']).dt.date \n",
    "\n",
    "# merged_dfì˜ dateë„ ë³€í™˜ \n",
    "merged_df['date'] = pd.to_datetime(merged_df['date']).dt.date \n",
    "\n",
    "# 1. date ê¸°ì¤€ ì •ë ¬ì„ ìœ„í•´ metrics_trainì˜ lifelog_date -> dateë¡œ ë§ì¶”ê¸° \n",
    "metrics_train_renamed = metrics_train.rename(columns={'lifelog_date': 'date'}) \n",
    "\n",
    "# 2. train_df: metrics_trainê³¼ ì¼ì¹˜í•˜ëŠ” (subject_id, date) â†’ ë¼ë²¨ í¬í•¨ \n",
    "train_df = pd.merge(metrics_train_renamed, merged_df, on=['subject_id', 'date'], how='inner') \n",
    "\n",
    "# 3. test_df: metrics_trainì— ì—†ëŠ” (subject_id, date) \n",
    "merged_keys = merged_df[['subject_id', 'date']] \n",
    "train_keys = metrics_train_renamed[['subject_id', 'date']] \n",
    "test_keys = pd.merge(merged_keys, train_keys, on=['subject_id', 'date'], how='left', indicator=True) \n",
    "test_keys = test_keys[test_keys['_merge'] == 'left_only'].drop(columns=['_merge']) \n",
    "test_df = pd.merge(test_keys, merged_df, on=['subject_id', 'date'], how='left') \n",
    "\n",
    "print(f\"âœ… ë°ì´í„° ë³‘í•© ì™„ë£Œ: train_df={train_df.shape}, test_df={test_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7793fda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: X_selected=(450, 106), test_X_selected=(250, 106)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =========================== ë°ì´í„° ì „ì²˜ë¦¬ ===========================\n",
    "\n",
    "# âœ… íƒ€ê²Ÿ ë¦¬ìŠ¤íŠ¸ \n",
    "targets_binary = ['Q1', 'Q2', 'Q3', 'S2', 'S3'] \n",
    "target_multiclass = 'S1' \n",
    "\n",
    "# âœ… feature ì¤€ë¹„ \n",
    "X = train_df.drop(columns=['subject_id', 'sleep_date', 'date', 'Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']) \n",
    "X.fillna(0, inplace=True) # ê²°ì¸¡ê°’ ì²˜ë¦¬ \n",
    "\n",
    "test_X = test_df.drop(columns=['subject_id', 'date']) \n",
    "test_X.fillna(0, inplace=True) \n",
    "\n",
    "# ì»¬ëŸ¼ ì´ë¦„ì—ì„œ íŠ¹ìˆ˜ ë¬¸ì ì œê±°/ë³€í™˜ \n",
    "def sanitize_column_names(df): \n",
    "    df.columns = ( \n",
    "        df.columns \n",
    "        .str.replace(r\"[^\\w]\", \"_\", regex=True) # íŠ¹ìˆ˜ë¬¸ì â†’ _ \n",
    "        .str.replace(r\"__+\", \"_\", regex=True) # ì—°ì†ëœ _ ì œê±° \n",
    "        .str.strip(\"_\") # ì•ë’¤ _ ì œê±° \n",
    "    ) \n",
    "    return df \n",
    "\n",
    "# ëª¨ë“  ì…ë ¥ì— ì ìš© \n",
    "X = sanitize_column_names(X) \n",
    "test_X = sanitize_column_names(test_X) \n",
    "\n",
    "# ëª¨ë“  íŠ¹ì„±ì— ëŒ€í•´ ëª¨ë¸ë§ ì§„í–‰ \n",
    "X_selected = X.copy()\n",
    "test_X_selected = test_X.copy()\n",
    "\n",
    "print(f\"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: X_selected={X_selected.shape}, test_X_selected={test_X_selected.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56804503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ê¸°ë³¸ Feature Importance ì¶”ì¶œ ===\n",
      ">> Q1 ë³€ìˆ˜ Feature Importance ì¶”ì¶œ ì¤‘..\n",
      ">> Q2 ë³€ìˆ˜ Feature Importance ì¶”ì¶œ ì¤‘..\n",
      ">> Q3 ë³€ìˆ˜ Feature Importance ì¶”ì¶œ ì¤‘..\n",
      ">> S2 ë³€ìˆ˜ Feature Importance ì¶”ì¶œ ì¤‘..\n",
      ">> S3 ë³€ìˆ˜ Feature Importance ì¶”ì¶œ ì¤‘..\n",
      ">> S1 ë³€ìˆ˜ Feature Importance ì¶”ì¶œ ì¤‘..\n",
      "âœ… Feature Importance ì¶”ì¶œ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# =========================== ê¸°ë³¸ LGBMìœ¼ë¡œ Feature Importance êµ¬í•˜ê¸° ===========================\n",
    "\n",
    "print(\"=== ê¸°ë³¸ Feature Importance ì¶”ì¶œ ===\")\n",
    "\n",
    "# LightGBM ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "lgbm_params = {\n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate': 0.03,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "# ì´ì§„ ë¶„ë¥˜\n",
    "binary_models_selected = {}\n",
    "for col in targets_binary:\n",
    "    print(f\">> {col} ë³€ìˆ˜ Feature Importance ì¶”ì¶œ ì¤‘..\")\n",
    "    y = train_df[col]\n",
    "    binary_params = lgbm_params.copy()\n",
    "    binary_params['objective'] = 'binary'\n",
    "    \n",
    "    model = LGBMClassifier(**binary_params)\n",
    "    model.fit(X_selected, y)\n",
    "    binary_models_selected[col] = model\n",
    "\n",
    "# ë‹¤ì¤‘ ë¶„ë¥˜ (S1)\n",
    "print(\">> S1 ë³€ìˆ˜ Feature Importance ì¶”ì¶œ ì¤‘..\")\n",
    "y_multi = train_df['S1']\n",
    "multiclass_params = lgbm_params.copy()\n",
    "multiclass_params['num_class'] = len(y_multi.unique())\n",
    "\n",
    "model_s1_selected = LGBMClassifier(**multiclass_params)\n",
    "model_s1_selected.fit(X_selected, y_multi)\n",
    "\n",
    "# Feature Importance ì¶”ì¶œ\n",
    "feature_importance_dict = {}\n",
    "\n",
    "# ì´ì§„ë¶„ë¥˜ ëª¨ë¸ë“¤ì˜ feature importance\n",
    "for col_name, model in binary_models_selected.items():\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': X_selected.columns,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False).head(10)  # ìƒìœ„ 10ê°œ\n",
    "    feature_importance_dict[col_name] = importance_df.copy()\n",
    "\n",
    "# S1 ë‹¤ì¤‘ë¶„ë¥˜ ëª¨ë¸ì˜ feature importance\n",
    "fi_multi_20 = pd.DataFrame({\n",
    "    'feature': X_selected.columns,\n",
    "    'importance': model_s1_selected.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(20)\n",
    "feature_importance_dict['S1'] = fi_multi_20.copy()\n",
    "\n",
    "print(\"âœ… Feature Importance ì¶”ì¶œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0115a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== ê° ëª¨ë¸ë³„ ì˜¤í† ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•œ ê³ ê¸‰ íŠ¹ì„± ì¶”ì¶œ =====\n",
      "\n",
      "ğŸ”¹ Q1 ëª¨ë¸ ì˜¤í† ì¸ì½”ë” ì²˜ë¦¬ ì‹œì‘...\n",
      "   - ì„ íƒëœ í”¼ì²˜ ìˆ˜: 20ê°œ\n",
      "   - ìµœê³  ì¤‘ìš”ë„ í”¼ì²˜: light_night_mean\n",
      "   - ì…ë ¥ ì°¨ì›: 20ê°œ\n",
      "   - ì¸ì½”ë”© ì°¨ì›: 10ê°œ\n",
      "   - Q1 ì˜¤í† ì¸ì½”ë” í•™ìŠµ ì¤‘...\n",
      "   - ì¸ì½”ë”©ëœ íŠ¹ì„± ì¶”ì¶œ ì¤‘...\n",
      "   âœ… Q1 ì™„ë£Œ!\n",
      "      - ì›ë³¸ íŠ¹ì„± ìˆ˜: 20ê°œ\n",
      "      - ì¸ì½”ë”©ëœ íŠ¹ì„± ìˆ˜: 10ê°œ\n",
      "      - ê²°í•©ëœ íŠ¹ì„± ìˆ˜: 30ê°œ\n",
      "      - ìµœì¢… ê²€ì¦ ì†ì‹¤: 0.659387\n",
      "      - í•™ìŠµ ì—í¬í¬: 150íšŒ\n",
      "\n",
      "ğŸ”¹ Q2 ëª¨ë¸ ì˜¤í† ì¸ì½”ë” ì²˜ë¦¬ ì‹œì‘...\n",
      "   - ì„ íƒëœ í”¼ì²˜ ìˆ˜: 20ê°œ\n",
      "   - ìµœê³  ì¤‘ìš”ë„ í”¼ì²˜: activity_7_ratio\n",
      "   - ì…ë ¥ ì°¨ì›: 20ê°œ\n",
      "   - ì¸ì½”ë”© ì°¨ì›: 10ê°œ\n",
      "   - Q2 ì˜¤í† ì¸ì½”ë” í•™ìŠµ ì¤‘...\n",
      "   - ì¸ì½”ë”©ëœ íŠ¹ì„± ì¶”ì¶œ ì¤‘...\n",
      "   âœ… Q2 ì™„ë£Œ!\n",
      "      - ì›ë³¸ íŠ¹ì„± ìˆ˜: 20ê°œ\n",
      "      - ì¸ì½”ë”©ëœ íŠ¹ì„± ìˆ˜: 10ê°œ\n",
      "      - ê²°í•©ëœ íŠ¹ì„± ìˆ˜: 30ê°œ\n",
      "      - ìµœì¢… ê²€ì¦ ì†ì‹¤: 0.789956\n",
      "      - í•™ìŠµ ì—í¬í¬: 150íšŒ\n",
      "\n",
      "ğŸ”¹ Q3 ëª¨ë¸ ì˜¤í† ì¸ì½”ë” ì²˜ë¦¬ ì‹œì‘...\n",
      "   - ì„ íƒëœ í”¼ì²˜ ìˆ˜: 20ê°œ\n",
      "   - ìµœê³  ì¤‘ìš”ë„ í”¼ì²˜: others_time\n",
      "   - ì…ë ¥ ì°¨ì›: 20ê°œ\n",
      "   - ì¸ì½”ë”© ì°¨ì›: 10ê°œ\n",
      "   - Q3 ì˜¤í† ì¸ì½”ë” í•™ìŠµ ì¤‘...\n",
      "   - ì¸ì½”ë”©ëœ íŠ¹ì„± ì¶”ì¶œ ì¤‘...\n",
      "   âœ… Q3 ì™„ë£Œ!\n",
      "      - ì›ë³¸ íŠ¹ì„± ìˆ˜: 20ê°œ\n",
      "      - ì¸ì½”ë”©ëœ íŠ¹ì„± ìˆ˜: 10ê°œ\n",
      "      - ê²°í•©ëœ íŠ¹ì„± ìˆ˜: 30ê°œ\n",
      "      - ìµœì¢… ê²€ì¦ ì†ì‹¤: 0.703759\n",
      "      - í•™ìŠµ ì—í¬í¬: 150íšŒ\n",
      "\n",
      "ğŸ”¹ S1 ëª¨ë¸ ì˜¤í† ì¸ì½”ë” ì²˜ë¦¬ ì‹œì‘...\n",
      "   - ì„ íƒëœ í”¼ì²˜ ìˆ˜: 20ê°œ\n",
      "   - ìµœê³  ì¤‘ìš”ë„ í”¼ì²˜: others_time\n",
      "   - ì…ë ¥ ì°¨ì›: 20ê°œ\n",
      "   - ì¸ì½”ë”© ì°¨ì›: 10ê°œ\n",
      "   - S1 ì˜¤í† ì¸ì½”ë” í•™ìŠµ ì¤‘...\n",
      "   - ì¸ì½”ë”©ëœ íŠ¹ì„± ì¶”ì¶œ ì¤‘...\n",
      "   âœ… S1 ì™„ë£Œ!\n",
      "      - ì›ë³¸ íŠ¹ì„± ìˆ˜: 20ê°œ\n",
      "      - ì¸ì½”ë”©ëœ íŠ¹ì„± ìˆ˜: 10ê°œ\n",
      "      - ê²°í•©ëœ íŠ¹ì„± ìˆ˜: 30ê°œ\n",
      "      - ìµœì¢… ê²€ì¦ ì†ì‹¤: 0.687848\n",
      "      - í•™ìŠµ ì—í¬í¬: 150íšŒ\n",
      "\n",
      "ğŸ”¹ S2 ëª¨ë¸ ì˜¤í† ì¸ì½”ë” ì²˜ë¦¬ ì‹œì‘...\n",
      "   - ì„ íƒëœ í”¼ì²˜ ìˆ˜: 20ê°œ\n",
      "   - ìµœê³  ì¤‘ìš”ë„ í”¼ì²˜: light_night_mean\n",
      "   - ì…ë ¥ ì°¨ì›: 20ê°œ\n",
      "   - ì¸ì½”ë”© ì°¨ì›: 10ê°œ\n",
      "   - S2 ì˜¤í† ì¸ì½”ë” í•™ìŠµ ì¤‘...\n",
      "   - ì¸ì½”ë”©ëœ íŠ¹ì„± ì¶”ì¶œ ì¤‘...\n",
      "   âœ… S2 ì™„ë£Œ!\n",
      "      - ì›ë³¸ íŠ¹ì„± ìˆ˜: 20ê°œ\n",
      "      - ì¸ì½”ë”©ëœ íŠ¹ì„± ìˆ˜: 10ê°œ\n",
      "      - ê²°í•©ëœ íŠ¹ì„± ìˆ˜: 30ê°œ\n",
      "      - ìµœì¢… ê²€ì¦ ì†ì‹¤: 0.702455\n",
      "      - í•™ìŠµ ì—í¬í¬: 150íšŒ\n",
      "\n",
      "ğŸ”¹ S3 ëª¨ë¸ ì˜¤í† ì¸ì½”ë” ì²˜ë¦¬ ì‹œì‘...\n",
      "   - ì„ íƒëœ í”¼ì²˜ ìˆ˜: 20ê°œ\n",
      "   - ìµœê³  ì¤‘ìš”ë„ í”¼ì²˜: light_night_mean\n",
      "   - ì…ë ¥ ì°¨ì›: 20ê°œ\n",
      "   - ì¸ì½”ë”© ì°¨ì›: 10ê°œ\n",
      "   - S3 ì˜¤í† ì¸ì½”ë” í•™ìŠµ ì¤‘...\n",
      "   - ì¸ì½”ë”©ëœ íŠ¹ì„± ì¶”ì¶œ ì¤‘...\n",
      "   âœ… S3 ì™„ë£Œ!\n",
      "      - ì›ë³¸ íŠ¹ì„± ìˆ˜: 20ê°œ\n",
      "      - ì¸ì½”ë”©ëœ íŠ¹ì„± ìˆ˜: 10ê°œ\n",
      "      - ê²°í•©ëœ íŠ¹ì„± ìˆ˜: 30ê°œ\n",
      "      - ìµœì¢… ê²€ì¦ ì†ì‹¤: 0.690251\n",
      "      - í•™ìŠµ ì—í¬í¬: 150íšŒ\n",
      "\n",
      "============================================================\n",
      "ğŸ‰ ëª¨ë“  ëª¨ë¸ë³„ ì˜¤í† ì¸ì½”ë” ì²˜ë¦¬ ì™„ë£Œ!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =========================== ì˜¤í† ì¸ì½”ë” íŠ¹ì„± ì¶”ì¶œ ===========================\n",
    "\n",
    "print(\"===== ê° ëª¨ë¸ë³„ ì˜¤í† ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•œ ê³ ê¸‰ íŠ¹ì„± ì¶”ì¶œ =====\")\n",
    "\n",
    "# ê° ëª¨ë¸ë³„ ê²°ê³¼ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ë“¤\n",
    "autoencoder_results = {}\n",
    "encoder_results = {}\n",
    "combined_features = {}\n",
    "scalers = {}\n",
    "\n",
    "# ëª¨ë¸ ë¦¬ìŠ¤íŠ¸\n",
    "model_names = ['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']\n",
    "\n",
    "def create_autoencoder(input_dim, encoding_dim=32):\n",
    "    \"\"\"ê°œì„ ëœ ì˜¤í† ì¸ì½”ë” ëª¨ë¸ ìƒì„±\"\"\"\n",
    "    # ì…ë ¥ ë ˆì´ì–´\n",
    "    input_layer = Input(shape=(input_dim,), name='input')\n",
    "    \n",
    "    # ì¸ì½”ë” ë¶€ë¶„ (ì ì§„ì  ì°¨ì› ì¶•ì†Œ)\n",
    "    x = Dense(min(128, input_dim * 2), activation='relu', name='encoder_1')(input_layer)\n",
    "    x = BatchNormalization(name='bn_encoder_1')(x)\n",
    "    x = Dropout(0.2, name='dropout_encoder_1')(x)\n",
    "    \n",
    "    x = Dense(min(64, input_dim), activation='relu', name='encoder_2')(x)\n",
    "    x = BatchNormalization(name='bn_encoder_2')(x)\n",
    "    x = Dropout(0.2, name='dropout_encoder_2')(x)\n",
    "    \n",
    "    # ë³‘ëª©ì¸µ (ì ì¬ ê³µê°„)\n",
    "    encoded = Dense(encoding_dim, activation='relu', name='latent_space')(x)\n",
    "    \n",
    "    # ë””ì½”ë” ë¶€ë¶„ (ì ì§„ì  ì°¨ì› ë³µì›)\n",
    "    x = Dense(min(64, input_dim), activation='relu', name='decoder_1')(encoded)\n",
    "    x = BatchNormalization(name='bn_decoder_1')(x)\n",
    "    x = Dropout(0.2, name='dropout_decoder_1')(x)\n",
    "    \n",
    "    x = Dense(min(128, input_dim * 2), activation='relu', name='decoder_2')(x)\n",
    "    x = BatchNormalization(name='bn_decoder_2')(x)\n",
    "    x = Dropout(0.2, name='dropout_decoder_2')(x)\n",
    "    \n",
    "    # ì¶œë ¥ ë ˆì´ì–´\n",
    "    decoded = Dense(input_dim, activation='sigmoid', name='output')(x)\n",
    "    \n",
    "    # ëª¨ë¸ ìƒì„±\n",
    "    autoencoder = Model(input_layer, decoded, name='autoencoder')\n",
    "    encoder = Model(input_layer, encoded, name='encoder')\n",
    "    \n",
    "    return autoencoder, encoder\n",
    "\n",
    "# ê° ëª¨ë¸ë³„ ì˜¤í† ì¸ì½”ë” ì²˜ë¦¬\n",
    "for model_name in model_names:\n",
    "    print(f\"\\nğŸ”¹ {model_name} ëª¨ë¸ ì˜¤í† ì¸ì½”ë” ì²˜ë¦¬ ì‹œì‘...\")\n",
    "    \n",
    "    # í•´ë‹¹ ëª¨ë¸ì˜ ìƒìœ„ 20ê°œ ì¤‘ìš” í”¼ì²˜ ì„ íƒ\n",
    "    top_features = feature_importance_dict[model_name]['feature'].tolist()\n",
    "    \n",
    "    # ì¤‘ìš” í”¼ì²˜ë“¤ë¡œ ë°ì´í„° í•„í„°ë§\n",
    "    X_model_selected = X_selected[top_features]\n",
    "    test_X_model_selected = test_X_selected[top_features]\n",
    "    \n",
    "    # ë°ì´í„° ì •ê·œí™” (ì˜¤í† ì¸ì½”ë” ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´)\n",
    "    scaler = StandardScaler()\n",
    "    X_model_scaled = scaler.fit_transform(X_model_selected)\n",
    "    test_X_model_scaled = scaler.transform(test_X_model_selected)\n",
    "    \n",
    "    print(f\"   - ì„ íƒëœ í”¼ì²˜ ìˆ˜: {len(top_features)}ê°œ\")\n",
    "    print(f\"   - ìµœê³  ì¤‘ìš”ë„ í”¼ì²˜: {feature_importance_dict[model_name].iloc[0]['feature']}\")\n",
    "    \n",
    "    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "    input_dim = len(top_features)\n",
    "    encoding_dim = max(8, min(32, input_dim))  # ì ì‘ì  ì¸ì½”ë”© ì°¨ì›\n",
    "    batch_size = 64\n",
    "    epochs = 150\n",
    "    \n",
    "    print(f\"   - ì…ë ¥ ì°¨ì›: {input_dim}ê°œ\")\n",
    "    print(f\"   - ì¸ì½”ë”© ì°¨ì›: {encoding_dim}ê°œ\")\n",
    "    \n",
    "    # ì˜¤í† ì¸ì½”ë” ëª¨ë¸ ìƒì„±\n",
    "    autoencoder, encoder = create_autoencoder(input_dim, encoding_dim)\n",
    "    \n",
    "    # ëª¨ë¸ ì»´íŒŒì¼\n",
    "    autoencoder.compile(\n",
    "        optimizer='adam',\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    # í›ˆë ¨/ê²€ì¦ ì„¸íŠ¸ ë¶„ë¦¬\n",
    "    X_train_auto, X_val_auto = train_test_split(\n",
    "        X_model_scaled, \n",
    "        test_size=0.2, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # ì½œë°± ì„¤ì •\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=0\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=8,\n",
    "            min_lr=1e-6,\n",
    "            verbose=0\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # ì˜¤í† ì¸ì½”ë” ëª¨ë¸ í•™ìŠµ\n",
    "    print(f\"   - {model_name} ì˜¤í† ì¸ì½”ë” í•™ìŠµ ì¤‘...\")\n",
    "    history = autoencoder.fit(\n",
    "        X_train_auto, X_train_auto,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        validation_data=(X_val_auto, X_val_auto),\n",
    "        callbacks=callbacks,\n",
    "        verbose=0  # ì¶œë ¥ ìµœì†Œí™”\n",
    "    )\n",
    "    \n",
    "    # ì¸ì½”ë”©ëœ íŠ¹ì„± ì¶”ì¶œ\n",
    "    print(f\"   - ì¸ì½”ë”©ëœ íŠ¹ì„± ì¶”ì¶œ ì¤‘...\")\n",
    "    encoded_features_train = encoder.predict(X_model_scaled, verbose=0)\n",
    "    encoded_features_test = encoder.predict(test_X_model_scaled, verbose=0)\n",
    "    \n",
    "    # ì›ë³¸ íŠ¹ì„±ê³¼ ì¸ì½”ë”©ëœ íŠ¹ì„± ê²°í•©\n",
    "    X_combined_model = np.hstack([X_model_scaled, encoded_features_train])\n",
    "    test_X_combined_model = np.hstack([test_X_model_scaled, encoded_features_test])\n",
    "    \n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    autoencoder_results[model_name] = autoencoder\n",
    "    encoder_results[model_name] = encoder\n",
    "    scalers[model_name] = scaler\n",
    "    combined_features[model_name] = {\n",
    "        'train': X_combined_model,\n",
    "        'test': test_X_combined_model,\n",
    "        'original_features': top_features,\n",
    "        'original_shape': X_model_selected.shape,\n",
    "        'encoded_shape': encoded_features_train.shape,\n",
    "        'combined_shape': X_combined_model.shape,\n",
    "        'scaler': scaler\n",
    "    }\n",
    "    \n",
    "    # ìµœì¢… ì†ì‹¤ê°’ ê³„ì‚°\n",
    "    final_loss = min(history.history['val_loss'])\n",
    "    \n",
    "    print(f\"   âœ… {model_name} ì™„ë£Œ!\")\n",
    "    print(f\"      - ì›ë³¸ íŠ¹ì„± ìˆ˜: {len(top_features)}ê°œ\")\n",
    "    print(f\"      - ì¸ì½”ë”©ëœ íŠ¹ì„± ìˆ˜: {encoding_dim}ê°œ\")\n",
    "    print(f\"      - ê²°í•©ëœ íŠ¹ì„± ìˆ˜: {X_combined_model.shape[1]}ê°œ\")\n",
    "    print(f\"      - ìµœì¢… ê²€ì¦ ì†ì‹¤: {final_loss:.6f}\")\n",
    "    print(f\"      - í•™ìŠµ ì—í¬í¬: {len(history.history['loss'])}íšŒ\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ‰ ëª¨ë“  ëª¨ë¸ë³„ ì˜¤í† ì¸ì½”ë” ì²˜ë¦¬ ì™„ë£Œ!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ecb4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸš€ ì˜¤í† ì¸ì½”ë” íŠ¹ì„±ì„ ì‚¬ìš©í•œ LightGBM ëª¨ë¸ í•™ìŠµ ì‹œì‘\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =========================== ì˜¤í† ì¸ì½”ë” íŠ¹ì„±ì„ ì‚¬ìš©í•œ LightGBM í•™ìŠµ ===========================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸš€ ì˜¤í† ì¸ì½”ë” íŠ¹ì„±ì„ ì‚¬ìš©í•œ LightGBM ëª¨ë¸ í•™ìŠµ ì‹œì‘\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ê° ëª¨ë¸ë³„ ê²°ê³¼ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ (ì˜¤í† ì¸ì½”ë” íŠ¹ì„± ì‚¬ìš©)\n",
    "binary_models_autoencoder = {}\n",
    "binary_preds_autoencoder = {}\n",
    "multiclass_s1_autoencoder = {}\n",
    "\n",
    "# S1ì„ ì œì™¸í•œ ì´ì§„ë¶„ë¥˜ ëª¨ë¸ë“¤\n",
    "binary_model_names = ['Q1', 'Q2', 'Q3', 'S2', 'S3']\n",
    "\n",
    "# ê°œì„ ëœ LightGBM íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ\n",
    "improved_binary_param_grid = {\n",
    "    'learning_rate': [0.01, 0.03],\n",
    "    'n_estimators': [1000, 1500],\n",
    "    'num_leaves': [31, 50],\n",
    "    'max_depth': [-1, 6],\n",
    "    'min_child_samples': [10, 20],\n",
    "    # 'subsample': [0.9, 1.0],\n",
    "    # 'colsample_bytree': [0.9, 1.0],\n",
    "    # 'reg_alpha': [0, 0.01, 0.1],\n",
    "    # 'reg_lambda': [0, 0.01, 0.1],\n",
    "}\n",
    "\n",
    "# ê¸°ë³¸ LightGBM íŒŒë¼ë¯¸í„°\n",
    "base_lgbm_params = {\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': -1,\n",
    "    'force_row_wise': True  # ì•ˆì •ì„±ì„ ìœ„í•´ ì¶”ê°€\n",
    "}\n",
    "\n",
    "# êµì°¨ ê²€ì¦ ì„¤ì •\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "def create_smote_pipeline(model, k_neighbors=3):\n",
    "    \"\"\"SMOTE + LightGBM íŒŒì´í”„ë¼ì¸ ìƒì„±\"\"\"\n",
    "    return ImbPipeline([\n",
    "        ('smote', SMOTE(random_state=42, k_neighbors=k_neighbors)),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "\n",
    "def get_optimal_k_neighbors(y):\n",
    "    \"\"\"í´ë˜ìŠ¤ ë¶„í¬ì— ë”°ë¥¸ ìµœì  k_neighbors ê³„ì‚°\"\"\"\n",
    "    min_class_count = min(y.value_counts())\n",
    "    if min_class_count <= 3:\n",
    "        return max(1, min_class_count - 1)\n",
    "    elif min_class_count <= 5:\n",
    "        return 3\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "def train_binary_model_with_autoencoder(model_name, target_col):\n",
    "    \"\"\"ì˜¤í† ì¸ì½”ë” íŠ¹ì„±ì„ ì‚¬ìš©í•œ ì´ì§„ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ\"\"\"\n",
    "    print(f\"\\nğŸ¯ {model_name} ëª¨ë¸ë¡œ {target_col} íƒ€ê²Ÿ ì˜ˆì¸¡ (ì˜¤í† ì¸ì½”ë” íŠ¹ì„± ì‚¬ìš©)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # íƒ€ê²Ÿ ë°ì´í„° ì¤€ë¹„\n",
    "    y = train_df[target_col]\n",
    "    \n",
    "    # ì˜¤í† ì¸ì½”ë”ë¡œ ìƒì„±ëœ ê²°í•© íŠ¹ì„± ì‚¬ìš©\n",
    "    X_train_model = combined_features[model_name]['train']\n",
    "    X_test_model = combined_features[model_name]['test']\n",
    "    \n",
    "    print(f\"   - ì‚¬ìš© íŠ¹ì„± ìˆ˜: {X_train_model.shape[1]}ê°œ (ì˜¤í† ì¸ì½”ë” ê²°í•©)\")\n",
    "    print(f\"   - í›ˆë ¨ ìƒ˜í”Œ ìˆ˜: {X_train_model.shape[0]}ê°œ\")\n",
    "    print(f\"   - íƒ€ê²Ÿ: {target_col}\")\n",
    "    print(f\"   - í´ë˜ìŠ¤ ìˆ˜: {len(y.unique())}ê°œ\")\n",
    "    print(f\"   - ì›ë³¸ í´ë˜ìŠ¤ ë¶„í¬: {dict(y.value_counts().sort_index())}\")\n",
    "    \n",
    "    # ìµœì  k_neighbors ê³„ì‚°\n",
    "    k_neighbors = get_optimal_k_neighbors(y)\n",
    "    print(f\"   - SMOTE k_neighbors: {k_neighbors}\")\n",
    "    \n",
    "    # SMOTE ì ìš© í›„ í´ë˜ìŠ¤ ë¶„í¬ ë¯¸ë¦¬ë³´ê¸°\n",
    "    try:\n",
    "        smote_preview = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
    "        X_resampled, y_resampled = smote_preview.fit_resample(X_train_model, y)\n",
    "        print(f\"   - SMOTE í›„ í´ë˜ìŠ¤ ë¶„í¬: {dict(pd.Series(y_resampled).value_counts().sort_index())}\")\n",
    "        print(f\"   - ìƒ˜í”Œ ìˆ˜ ë³€í™”: {len(y)} â†’ {len(y_resampled)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸  SMOTE ë¯¸ë¦¬ë³´ê¸° ì‹¤íŒ¨: {str(e)}\")\n",
    "        k_neighbors = 1\n",
    "        print(f\"   - k_neighborsë¥¼ 1ë¡œ ì¬ì¡°ì •\")\n",
    "    \n",
    "    # LightGBM ëª¨ë¸ ìƒì„±\n",
    "    lgbm_model = LGBMClassifier(\n",
    "        objective='binary',\n",
    "        **base_lgbm_params\n",
    "    )\n",
    "    \n",
    "    # SMOTE + LightGBM íŒŒì´í”„ë¼ì¸ ìƒì„±\n",
    "    pipeline = create_smote_pipeline(lgbm_model, k_neighbors)\n",
    "    \n",
    "    # íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ (classifier__ ì ‘ë‘ì‚¬ ì¶”ê°€)\n",
    "    param_grid_pipeline = {}\n",
    "    for key, value in improved_binary_param_grid.items():\n",
    "        param_grid_pipeline[f'classifier__{key}'] = value\n",
    "    \n",
    "    # ê·¸ë¦¬ë“œ ì„œì¹˜\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid_pipeline,\n",
    "        scoring='f1_macro',\n",
    "        cv=cv,\n",
    "        verbose=0,\n",
    "        n_jobs=-1,\n",
    "        error_score='raise'  # ì—ëŸ¬ ë°œìƒ ì‹œ ëª…í™•í•œ ë©”ì‹œì§€\n",
    "    )\n",
    "    \n",
    "    print(f\"   - ëª¨ë¸ ìµœì í™” ì¤‘...\")\n",
    "    \n",
    "    try:\n",
    "        grid_search.fit(X_train_model, y)\n",
    "        \n",
    "        print(f\"   âœ… í•™ìŠµ ì™„ë£Œ!\")\n",
    "        print(f\"      - ìµœì  F1-Macro ì ìˆ˜: {grid_search.best_score_:.6f}\")\n",
    "        \n",
    "        # ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "        best_model = grid_search.best_estimator_\n",
    "        predictions = best_model.predict(X_test_model)\n",
    "        \n",
    "        # ê²°ê³¼ ì €ì¥\n",
    "        binary_models_autoencoder[model_name] = best_model\n",
    "        binary_preds_autoencoder[model_name] = predictions\n",
    "        \n",
    "        print(f\"      - ì˜ˆì¸¡ ì™„ë£Œ: {len(predictions)}ê°œ ìƒ˜í”Œ\")\n",
    "        print(f\"      - ì˜ˆì¸¡ í´ë˜ìŠ¤ ë¶„í¬: {dict(pd.Series(predictions).value_counts().sort_index())}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ í•™ìŠµ ì‹¤íŒ¨: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def train_multiclass_s1_with_autoencoder():\n",
    "    \"\"\"ì˜¤í† ì¸ì½”ë” íŠ¹ì„±ì„ ì‚¬ìš©í•œ S1 ë‹¤ì¤‘ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ\"\"\"\n",
    "    print(f\"\\nğŸ¯ S1 ëª¨ë¸ë¡œ S1 ë‹¤ì¤‘ë¶„ë¥˜ íƒ€ê²Ÿ ì˜ˆì¸¡ (ì˜¤í† ì¸ì½”ë” íŠ¹ì„± ì‚¬ìš©)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # S1 íƒ€ê²Ÿ ë°ì´í„° ì¤€ë¹„\n",
    "    target_col = 'S1'\n",
    "    y_multiclass = train_df[target_col]\n",
    "    \n",
    "    # ì˜¤í† ì¸ì½”ë”ë¡œ ìƒì„±ëœ ê²°í•© íŠ¹ì„± ì‚¬ìš©\n",
    "    X_train_s1 = combined_features[target_col]['train']\n",
    "    X_test_s1 = combined_features[target_col]['test']\n",
    "    \n",
    "    print(f\"   - ì‚¬ìš© íŠ¹ì„± ìˆ˜: {X_train_s1.shape[1]}ê°œ (ì˜¤í† ì¸ì½”ë” ê²°í•©)\")\n",
    "    print(f\"   - í›ˆë ¨ ìƒ˜í”Œ ìˆ˜: {X_train_s1.shape[0]}ê°œ\")\n",
    "    print(f\"   - íƒ€ê²Ÿ: {target_col} (ë‹¤ì¤‘ë¶„ë¥˜)\")\n",
    "    print(f\"   - í´ë˜ìŠ¤ ìˆ˜: {len(y_multiclass.unique())}ê°œ\")\n",
    "    print(f\"   - ì›ë³¸ í´ë˜ìŠ¤ ë¶„í¬: {dict(y_multiclass.value_counts().sort_index())}\")\n",
    "    \n",
    "    # ë‹¤ì¤‘ë¶„ë¥˜ìš© SMOTE ì ìš©\n",
    "    k_neighbors = get_optimal_k_neighbors(y_multiclass)\n",
    "    print(f\"   - SMOTE k_neighbors: {k_neighbors}\")\n",
    "    \n",
    "    try:\n",
    "        smote = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
    "        X_train_s1_smote, y_multiclass_smote = smote.fit_resample(X_train_s1, y_multiclass)\n",
    "        print(f\"   - SMOTE í›„ í´ë˜ìŠ¤ ë¶„í¬: {dict(pd.Series(y_multiclass_smote).value_counts().sort_index())}\")\n",
    "        print(f\"   - ìƒ˜í”Œ ìˆ˜ ë³€í™”: {len(y_multiclass)} â†’ {len(y_multiclass_smote)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸  SMOTE ì ìš© ì‹¤íŒ¨: {str(e)}\")\n",
    "        X_train_s1_smote, y_multiclass_smote = X_train_s1, y_multiclass\n",
    "        print(f\"   - ì›ë³¸ ë°ì´í„°ë¡œ ì§„í–‰\")\n",
    "    \n",
    "    # ë‹¤ì¤‘ë¶„ë¥˜ìš© ëª¨ë¸ ì„¤ì •\n",
    "    multiclass_model = LGBMClassifier(\n",
    "        objective='multiclass',\n",
    "        metric='multi_logloss',\n",
    "        **base_lgbm_params\n",
    "    )\n",
    "    \n",
    "    # ë‹¤ì¤‘ë¶„ë¥˜ìš© íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ (ê°„ì†Œí™”)\n",
    "    multiclass_param_grid = {\n",
    "        'learning_rate': [0.01, 0.03, 0.05],\n",
    "        'n_estimators': [1000, 1500],\n",
    "        'num_leaves': [31, 50, 100],\n",
    "        'max_depth': [-1, 6],\n",
    "        'min_child_samples': [10, 20],\n",
    "    }\n",
    "    \n",
    "    # ê·¸ë¦¬ë“œ ì„œì¹˜\n",
    "    grid_search_multiclass = GridSearchCV(\n",
    "        estimator=multiclass_model,\n",
    "        param_grid=multiclass_param_grid,\n",
    "        scoring='f1_macro',\n",
    "        cv=cv,\n",
    "        verbose=0,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print(f\"   - S1 ë‹¤ì¤‘ë¶„ë¥˜ ëª¨ë¸ ìµœì í™” ì¤‘...\")\n",
    "    \n",
    "    try:\n",
    "        grid_search_multiclass.fit(X_train_s1_smote, y_multiclass_smote)\n",
    "        \n",
    "        print(f\"   âœ… í•™ìŠµ ì™„ë£Œ!\")\n",
    "        print(f\"      - ìµœì  F1-Macro ì ìˆ˜: {grid_search_multiclass.best_score_:.6f}\")\n",
    "        \n",
    "        # ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "        best_multiclass_model = grid_search_multiclass.best_estimator_\n",
    "        multiclass_predictions = best_multiclass_model.predict(X_test_s1)\n",
    "        multiclass_probabilities = best_multiclass_model.predict_proba(X_test_s1)\n",
    "        \n",
    "        # ê²°ê³¼ ì €ì¥\n",
    "        multiclass_s1_autoencoder['model'] = best_multiclass_model\n",
    "        multiclass_s1_autoencoder['predictions'] = multiclass_predictions\n",
    "        multiclass_s1_autoencoder['probabilities'] = multiclass_probabilities\n",
    "        multiclass_s1_autoencoder['classes'] = best_multiclass_model.classes_\n",
    "        \n",
    "        print(f\"      - ì˜ˆì¸¡ ì™„ë£Œ: {len(multiclass_predictions)}ê°œ ìƒ˜í”Œ\")\n",
    "        print(f\"      - ì˜ˆì¸¡ í´ë˜ìŠ¤ ë¶„í¬: {dict(pd.Series(multiclass_predictions).value_counts().sort_index())}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ í•™ìŠµ ì‹¤íŒ¨: {str(e)}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e2fa0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š ì´ì§„ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ (ì˜¤í† ì¸ì½”ë” íŠ¹ì„± ì‚¬ìš©)\n",
      "============================================================\n",
      "\n",
      "ğŸ¯ Q1 ëª¨ë¸ë¡œ Q1 íƒ€ê²Ÿ ì˜ˆì¸¡ (ì˜¤í† ì¸ì½”ë” íŠ¹ì„± ì‚¬ìš©)\n",
      "--------------------------------------------------\n",
      "   - ì‚¬ìš© íŠ¹ì„± ìˆ˜: 30ê°œ (ì˜¤í† ì¸ì½”ë” ê²°í•©)\n",
      "   - í›ˆë ¨ ìƒ˜í”Œ ìˆ˜: 450ê°œ\n",
      "   - íƒ€ê²Ÿ: Q1\n",
      "   - í´ë˜ìŠ¤ ìˆ˜: 2ê°œ\n",
      "   - ì›ë³¸ í´ë˜ìŠ¤ ë¶„í¬: {0: 227, 1: 223}\n",
      "   - SMOTE k_neighbors: 5\n",
      "   - SMOTE í›„ í´ë˜ìŠ¤ ë¶„í¬: {0: 227, 1: 227}\n",
      "   - ìƒ˜í”Œ ìˆ˜ ë³€í™”: 450 â†’ 454\n",
      "   - ëª¨ë¸ ìµœì í™” ì¤‘...\n",
      "   âœ… í•™ìŠµ ì™„ë£Œ!\n",
      "      - ìµœì  F1-Macro ì ìˆ˜: 0.627922\n",
      "      - ì˜ˆì¸¡ ì™„ë£Œ: 250ê°œ ìƒ˜í”Œ\n",
      "      - ì˜ˆì¸¡ í´ë˜ìŠ¤ ë¶„í¬: {0: 119, 1: 131}\n",
      "\n",
      "ğŸ¯ Q2 ëª¨ë¸ë¡œ Q2 íƒ€ê²Ÿ ì˜ˆì¸¡ (ì˜¤í† ì¸ì½”ë” íŠ¹ì„± ì‚¬ìš©)\n",
      "--------------------------------------------------\n",
      "   - ì‚¬ìš© íŠ¹ì„± ìˆ˜: 30ê°œ (ì˜¤í† ì¸ì½”ë” ê²°í•©)\n",
      "   - í›ˆë ¨ ìƒ˜í”Œ ìˆ˜: 450ê°œ\n",
      "   - íƒ€ê²Ÿ: Q2\n",
      "   - í´ë˜ìŠ¤ ìˆ˜: 2ê°œ\n",
      "   - ì›ë³¸ í´ë˜ìŠ¤ ë¶„í¬: {0: 197, 1: 253}\n",
      "   - SMOTE k_neighbors: 5\n",
      "   - SMOTE í›„ í´ë˜ìŠ¤ ë¶„í¬: {0: 253, 1: 253}\n",
      "   - ìƒ˜í”Œ ìˆ˜ ë³€í™”: 450 â†’ 506\n",
      "   - ëª¨ë¸ ìµœì í™” ì¤‘...\n",
      "   âœ… í•™ìŠµ ì™„ë£Œ!\n",
      "      - ìµœì  F1-Macro ì ìˆ˜: 0.593284\n",
      "      - ì˜ˆì¸¡ ì™„ë£Œ: 250ê°œ ìƒ˜í”Œ\n",
      "      - ì˜ˆì¸¡ í´ë˜ìŠ¤ ë¶„í¬: {0: 99, 1: 151}\n",
      "\n",
      "ğŸ¯ Q3 ëª¨ë¸ë¡œ Q3 íƒ€ê²Ÿ ì˜ˆì¸¡ (ì˜¤í† ì¸ì½”ë” íŠ¹ì„± ì‚¬ìš©)\n",
      "--------------------------------------------------\n",
      "   - ì‚¬ìš© íŠ¹ì„± ìˆ˜: 30ê°œ (ì˜¤í† ì¸ì½”ë” ê²°í•©)\n",
      "   - í›ˆë ¨ ìƒ˜í”Œ ìˆ˜: 450ê°œ\n",
      "   - íƒ€ê²Ÿ: Q3\n",
      "   - í´ë˜ìŠ¤ ìˆ˜: 2ê°œ\n",
      "   - ì›ë³¸ í´ë˜ìŠ¤ ë¶„í¬: {0: 180, 1: 270}\n",
      "   - SMOTE k_neighbors: 5\n",
      "   - SMOTE í›„ í´ë˜ìŠ¤ ë¶„í¬: {0: 270, 1: 270}\n",
      "   - ìƒ˜í”Œ ìˆ˜ ë³€í™”: 450 â†’ 540\n",
      "   - ëª¨ë¸ ìµœì í™” ì¤‘...\n",
      "   âœ… í•™ìŠµ ì™„ë£Œ!\n",
      "      - ìµœì  F1-Macro ì ìˆ˜: 0.658349\n",
      "      - ì˜ˆì¸¡ ì™„ë£Œ: 250ê°œ ìƒ˜í”Œ\n",
      "      - ì˜ˆì¸¡ í´ë˜ìŠ¤ ë¶„í¬: {0: 84, 1: 166}\n",
      "\n",
      "ğŸ¯ S2 ëª¨ë¸ë¡œ S2 íƒ€ê²Ÿ ì˜ˆì¸¡ (ì˜¤í† ì¸ì½”ë” íŠ¹ì„± ì‚¬ìš©)\n",
      "--------------------------------------------------\n",
      "   - ì‚¬ìš© íŠ¹ì„± ìˆ˜: 30ê°œ (ì˜¤í† ì¸ì½”ë” ê²°í•©)\n",
      "   - í›ˆë ¨ ìƒ˜í”Œ ìˆ˜: 450ê°œ\n",
      "   - íƒ€ê²Ÿ: S2\n",
      "   - í´ë˜ìŠ¤ ìˆ˜: 2ê°œ\n",
      "   - ì›ë³¸ í´ë˜ìŠ¤ ë¶„í¬: {0: 157, 1: 293}\n",
      "   - SMOTE k_neighbors: 5\n",
      "   - SMOTE í›„ í´ë˜ìŠ¤ ë¶„í¬: {0: 293, 1: 293}\n",
      "   - ìƒ˜í”Œ ìˆ˜ ë³€í™”: 450 â†’ 586\n",
      "   - ëª¨ë¸ ìµœì í™” ì¤‘...\n",
      "   âœ… í•™ìŠµ ì™„ë£Œ!\n",
      "      - ìµœì  F1-Macro ì ìˆ˜: 0.681043\n",
      "      - ì˜ˆì¸¡ ì™„ë£Œ: 250ê°œ ìƒ˜í”Œ\n",
      "      - ì˜ˆì¸¡ í´ë˜ìŠ¤ ë¶„í¬: {0: 81, 1: 169}\n",
      "\n",
      "ğŸ¯ S3 ëª¨ë¸ë¡œ S3 íƒ€ê²Ÿ ì˜ˆì¸¡ (ì˜¤í† ì¸ì½”ë” íŠ¹ì„± ì‚¬ìš©)\n",
      "--------------------------------------------------\n",
      "   - ì‚¬ìš© íŠ¹ì„± ìˆ˜: 30ê°œ (ì˜¤í† ì¸ì½”ë” ê²°í•©)\n",
      "   - í›ˆë ¨ ìƒ˜í”Œ ìˆ˜: 450ê°œ\n",
      "   - íƒ€ê²Ÿ: S3\n",
      "   - í´ë˜ìŠ¤ ìˆ˜: 2ê°œ\n",
      "   - ì›ë³¸ í´ë˜ìŠ¤ ë¶„í¬: {0: 152, 1: 298}\n",
      "   - SMOTE k_neighbors: 5\n",
      "   - SMOTE í›„ í´ë˜ìŠ¤ ë¶„í¬: {0: 298, 1: 298}\n",
      "   - ìƒ˜í”Œ ìˆ˜ ë³€í™”: 450 â†’ 596\n",
      "   - ëª¨ë¸ ìµœì í™” ì¤‘...\n",
      "   âœ… í•™ìŠµ ì™„ë£Œ!\n",
      "      - ìµœì  F1-Macro ì ìˆ˜: 0.653835\n",
      "      - ì˜ˆì¸¡ ì™„ë£Œ: 250ê°œ ìƒ˜í”Œ\n",
      "      - ì˜ˆì¸¡ í´ë˜ìŠ¤ ë¶„í¬: {0: 66, 1: 184}\n",
      "\n",
      "ğŸ“Š ë‹¤ì¤‘ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ (ì˜¤í† ì¸ì½”ë” íŠ¹ì„± ì‚¬ìš©)\n",
      "============================================================\n",
      "\n",
      "ğŸ¯ S1 ëª¨ë¸ë¡œ S1 ë‹¤ì¤‘ë¶„ë¥˜ íƒ€ê²Ÿ ì˜ˆì¸¡ (ì˜¤í† ì¸ì½”ë” íŠ¹ì„± ì‚¬ìš©)\n",
      "--------------------------------------------------\n",
      "   - ì‚¬ìš© íŠ¹ì„± ìˆ˜: 30ê°œ (ì˜¤í† ì¸ì½”ë” ê²°í•©)\n",
      "   - í›ˆë ¨ ìƒ˜í”Œ ìˆ˜: 450ê°œ\n",
      "   - íƒ€ê²Ÿ: S1 (ë‹¤ì¤‘ë¶„ë¥˜)\n",
      "   - í´ë˜ìŠ¤ ìˆ˜: 3ê°œ\n",
      "   - ì›ë³¸ í´ë˜ìŠ¤ ë¶„í¬: {0: 143, 1: 224, 2: 83}\n",
      "   - SMOTE k_neighbors: 5\n",
      "   - SMOTE í›„ í´ë˜ìŠ¤ ë¶„í¬: {0: 224, 1: 224, 2: 224}\n",
      "   - ìƒ˜í”Œ ìˆ˜ ë³€í™”: 450 â†’ 672\n",
      "   - S1 ë‹¤ì¤‘ë¶„ë¥˜ ëª¨ë¸ ìµœì í™” ì¤‘...\n",
      "   âœ… í•™ìŠµ ì™„ë£Œ!\n",
      "      - ìµœì  F1-Macro ì ìˆ˜: 0.707160\n",
      "      - ì˜ˆì¸¡ ì™„ë£Œ: 250ê°œ ìƒ˜í”Œ\n",
      "      - ì˜ˆì¸¡ í´ë˜ìŠ¤ ë¶„í¬: {0: 69, 1: 142, 2: 39}\n",
      "\n",
      "============================================================\n",
      "ğŸ‰ ì˜¤í† ì¸ì½”ë” íŠ¹ì„± ê¸°ë°˜ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š í•™ìŠµ ê²°ê³¼ ìš”ì•½:\n",
      "   ğŸŸ¢ ì„±ê³µí•œ ì´ì§„ë¶„ë¥˜ ëª¨ë¸: 5ê°œ\n",
      "      - Q1\n",
      "      - Q2\n",
      "      - Q3\n",
      "      - S2\n",
      "      - S3\n",
      "   ğŸŸ¢ S1 ë‹¤ì¤‘ë¶„ë¥˜ ëª¨ë¸: ì„±ê³µ\n"
     ]
    }
   ],
   "source": [
    "# =========================== ëª¨ë¸ í•™ìŠµ ì‹¤í–‰ ===========================\n",
    "\n",
    "# ì´ì§„ë¶„ë¥˜ ëª¨ë¸ë“¤ í•™ìŠµ\n",
    "print(\"\\nğŸ“Š ì´ì§„ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ (ì˜¤í† ì¸ì½”ë” íŠ¹ì„± ì‚¬ìš©)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "successful_models = []\n",
    "failed_models = []\n",
    "\n",
    "for model_name in binary_model_names:\n",
    "    target_col = model_name  # Q1â†’Q1, Q2â†’Q2, Q3â†’Q3, S2â†’S2, S3â†’S3\n",
    "    success = train_binary_model_with_autoencoder(model_name, target_col)\n",
    "    \n",
    "    if success:\n",
    "        successful_models.append(model_name)\n",
    "    else:\n",
    "        failed_models.append(model_name)\n",
    "\n",
    "# S1 ë‹¤ì¤‘ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ\n",
    "print(\"\\nğŸ“Š ë‹¤ì¤‘ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ (ì˜¤í† ì¸ì½”ë” íŠ¹ì„± ì‚¬ìš©)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "s1_success = train_multiclass_s1_with_autoencoder()\n",
    "\n",
    "# =========================== ê²°ê³¼ ìš”ì•½ ===========================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ‰ ì˜¤í† ì¸ì½”ë” íŠ¹ì„± ê¸°ë°˜ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nğŸ“Š í•™ìŠµ ê²°ê³¼ ìš”ì•½:\")\n",
    "print(f\"   ğŸŸ¢ ì„±ê³µí•œ ì´ì§„ë¶„ë¥˜ ëª¨ë¸: {len(successful_models)}ê°œ\")\n",
    "for model in successful_models:\n",
    "    print(f\"      - {model}\")\n",
    "\n",
    "if failed_models:\n",
    "    print(f\"   ğŸ”´ ì‹¤íŒ¨í•œ ì´ì§„ë¶„ë¥˜ ëª¨ë¸: {len(failed_models)}ê°œ\")\n",
    "    for model in failed_models:\n",
    "        print(f\"      - {model}\")\n",
    "\n",
    "print(f\"   ğŸŸ¢ S1 ë‹¤ì¤‘ë¶„ë¥˜ ëª¨ë¸: {'ì„±ê³µ' if s1_success else 'ì‹¤íŒ¨'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba1730f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ“‹ ì˜¤í† ì¸ì½”ë” ê¸°ë°˜ ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„±\n",
      "============================================================\n",
      "ğŸ“Š ì œì¶œ ë°ì´í„° ê¸°ë³¸ ì •ë³´:\n",
      "   - ì œì¶œ ìƒ˜í”Œ ìˆ˜: 250ê°œ\n",
      "   - ê³ ìœ  í”¼í—˜ì ìˆ˜: 10ëª…\n",
      "\n",
      "ğŸ”„ S1 ë‹¤ì¤‘ë¶„ë¥˜ ì˜ˆì¸¡ ê²°ê³¼ í• ë‹¹ ì¤‘...\n",
      "   âœ… S1 (ë‹¤ì¤‘ë¶„ë¥˜) í• ë‹¹ ì™„ë£Œ:\n",
      "      í´ë˜ìŠ¤ 0: 69ê°œ (27.6%)\n",
      "      í´ë˜ìŠ¤ 1: 142ê°œ (56.8%)\n",
      "      í´ë˜ìŠ¤ 2: 39ê°œ (15.6%)\n",
      "\n",
      "ğŸ”„ ì´ì§„ë¶„ë¥˜ ì˜ˆì¸¡ ê²°ê³¼ í• ë‹¹ ì¤‘...\n",
      "   âœ… Q1 (ì´ì§„ë¶„ë¥˜) í• ë‹¹ ì™„ë£Œ:\n",
      "      í´ë˜ìŠ¤ 0: 119ê°œ (47.6%)\n",
      "      í´ë˜ìŠ¤ 1: 131ê°œ (52.4%)\n",
      "   âœ… Q2 (ì´ì§„ë¶„ë¥˜) í• ë‹¹ ì™„ë£Œ:\n",
      "      í´ë˜ìŠ¤ 0: 99ê°œ (39.6%)\n",
      "      í´ë˜ìŠ¤ 1: 151ê°œ (60.4%)\n",
      "   âœ… Q3 (ì´ì§„ë¶„ë¥˜) í• ë‹¹ ì™„ë£Œ:\n",
      "      í´ë˜ìŠ¤ 0: 84ê°œ (33.6%)\n",
      "      í´ë˜ìŠ¤ 1: 166ê°œ (66.4%)\n",
      "   âœ… S2 (ì´ì§„ë¶„ë¥˜) í• ë‹¹ ì™„ë£Œ:\n",
      "      í´ë˜ìŠ¤ 0: 81ê°œ (32.4%)\n",
      "      í´ë˜ìŠ¤ 1: 169ê°œ (67.6%)\n",
      "   âœ… S3 (ì´ì§„ë¶„ë¥˜) í• ë‹¹ ì™„ë£Œ:\n",
      "      í´ë˜ìŠ¤ 0: 66ê°œ (26.4%)\n",
      "      í´ë˜ìŠ¤ 1: 184ê°œ (73.6%)\n",
      "\n",
      "ğŸ“Š ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½ (ì˜¤í† ì¸ì½”ë” ê¸°ë°˜):\n",
      "==================================================\n",
      "ğŸ”¹ Q1:\n",
      "   í´ë˜ìŠ¤ 0: 119ê°œ (47.6%)\n",
      "   í´ë˜ìŠ¤ 1: 131ê°œ (52.4%)\n",
      "ğŸ”¹ Q2:\n",
      "   í´ë˜ìŠ¤ 0: 99ê°œ (39.6%)\n",
      "   í´ë˜ìŠ¤ 1: 151ê°œ (60.4%)\n",
      "ğŸ”¹ Q3:\n",
      "   í´ë˜ìŠ¤ 0: 84ê°œ (33.6%)\n",
      "   í´ë˜ìŠ¤ 1: 166ê°œ (66.4%)\n",
      "ğŸ”¹ S1:\n",
      "   í´ë˜ìŠ¤ 0: 69ê°œ (27.6%)\n",
      "   í´ë˜ìŠ¤ 1: 142ê°œ (56.8%)\n",
      "   í´ë˜ìŠ¤ 2: 39ê°œ (15.6%)\n",
      "ğŸ”¹ S2:\n",
      "   í´ë˜ìŠ¤ 0: 81ê°œ (32.4%)\n",
      "   í´ë˜ìŠ¤ 1: 169ê°œ (67.6%)\n",
      "ğŸ”¹ S3:\n",
      "   í´ë˜ìŠ¤ 0: 66ê°œ (26.4%)\n",
      "   í´ë˜ìŠ¤ 1: 184ê°œ (73.6%)\n",
      "\n",
      "ğŸ” ë°ì´í„° í’ˆì§ˆ ê²€ì¦:\n",
      "   - ì´ í–‰ ìˆ˜: 250\n",
      "   - ê²°ì¸¡ê°’ í™•ì¸:\n",
      "      Q1: ê²°ì¸¡ê°’ ì—†ìŒ âœ…\n",
      "      Q2: ê²°ì¸¡ê°’ ì—†ìŒ âœ…\n",
      "      Q3: ê²°ì¸¡ê°’ ì—†ìŒ âœ…\n",
      "      S1: ê²°ì¸¡ê°’ ì—†ìŒ âœ…\n",
      "      S2: ê²°ì¸¡ê°’ ì—†ìŒ âœ…\n",
      "      S3: ê²°ì¸¡ê°’ ì—†ìŒ âœ…\n",
      "\n",
      "âœ… ì˜¤í† ì¸ì½”ë” ê¸°ë°˜ ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ!\n",
      "   - íŒŒì¼ëª…: submission_autoencoder_final_mod4_1_6_0616_1129.csv\n",
      "   - ì €ì¥ ê²½ë¡œ: /users/KTL/Desktop/dacon/submission/submission_autoencoder_final_mod4_1_6_0616_1129.csv\n",
      "   - íŒŒì¼ í¬ê¸°: 250 í–‰ x 9 ì—´\n",
      "\n",
      "============================================================\n",
      "ğŸ‰ ì˜¤í† ì¸ì½”ë” ê¸°ë°˜ ëª¨ë¸ë§ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!\n",
      "============================================================\n",
      "\n",
      "ğŸ“‹ ì™„ë£Œëœ ì‘ì—… ìš”ì•½:\n",
      "   âœ… ê° ëª¨ë¸ë³„ ìƒìœ„ í”¼ì²˜ ì„ íƒ (20ê°œì”©)\n",
      "   âœ… ëª¨ë¸ë³„ ì˜¤í† ì¸ì½”ë” í•™ìŠµ ë° íŠ¹ì„± ì¶”ì¶œ\n",
      "   âœ… ì›ë³¸ + ì¸ì½”ë”© íŠ¹ì„± ê²°í•©\n",
      "   âœ… SMOTE + LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\n",
      "   âœ… ì´ì§„ë¶„ë¥˜ ëª¨ë¸ 5ê°œ í•™ìŠµ\n",
      "   âœ… S1 ë‹¤ì¤‘ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ\n",
      "   âœ… ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„±\n",
      "\n",
      "ğŸ” ì„±ëŠ¥ ê°œì„  í¬ì¸íŠ¸:\n",
      "   - ê° ëª¨ë¸ë³„ ê°œë³„ ì˜¤í† ì¸ì½”ë”ë¡œ íŠ¹ì„± ìµœì í™”\n",
      "   - StandardScalerë¡œ ì •ê·œí™” ì ìš©\n",
      "   - SMOTEë¡œ í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°\n",
      "   - í™•ì¥ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n",
      "   - 5-fold êµì°¨ê²€ì¦ìœ¼ë¡œ ê²¬ê³ í•œ í‰ê°€\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =========================== ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„± ===========================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“‹ ì˜¤í† ì¸ì½”ë” ê¸°ë°˜ ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„±\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# sample ê¸°ë°˜ ì œì¶œ í¬ë§· ê°€ì ¸ì˜¤ê¸°\n",
    "submission_autoencoder = sample_submission[['subject_id', 'sleep_date', 'lifelog_date']].copy()\n",
    "\n",
    "# lifelog_date ê¸°ì¤€ìœ¼ë¡œ string â†’ date í˜•ì‹ í†µì¼\n",
    "submission_autoencoder['lifelog_date'] = pd.to_datetime(submission_autoencoder['lifelog_date']).dt.date\n",
    "\n",
    "print(f\"ğŸ“Š ì œì¶œ ë°ì´í„° ê¸°ë³¸ ì •ë³´:\")\n",
    "print(f\"   - ì œì¶œ ìƒ˜í”Œ ìˆ˜: {len(submission_autoencoder):,}ê°œ\")\n",
    "print(f\"   - ê³ ìœ  í”¼í—˜ì ìˆ˜: {submission_autoencoder['subject_id'].nunique()}ëª…\")\n",
    "\n",
    "# ì˜ˆì¸¡ ê²°ê³¼ ê²€ì¦ ë° í• ë‹¹\n",
    "def assign_predictions_safely(df, target_col, predictions, model_type=\"ì´ì§„ë¶„ë¥˜\"):\n",
    "    \"\"\"ì•ˆì „í•˜ê²Œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í• ë‹¹\"\"\"\n",
    "    if len(df) != len(predictions):\n",
    "        print(f\"   âš ï¸  {target_col} ê¸¸ì´ ë¶ˆì¼ì¹˜: submission={len(df)}, predictions={len(predictions)}\")\n",
    "        # ê¸¸ì´ê°€ ë‹¤ë¥´ë©´ ìµœì†Œ ê¸¸ì´ë¡œ ë§ì¶¤\n",
    "        min_length = min(len(df), len(predictions))\n",
    "        df = df.iloc[:min_length].copy()\n",
    "        predictions = predictions[:min_length]\n",
    "        print(f\"   â†’ ê¸¸ì´ë¥¼ {min_length}ë¡œ ì¡°ì •\")\n",
    "    \n",
    "    df[target_col] = predictions.astype(int)\n",
    "    \n",
    "    # ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½\n",
    "    value_counts = pd.Series(predictions).value_counts().sort_index()\n",
    "    total = len(predictions)\n",
    "    \n",
    "    print(f\"   âœ… {target_col} ({model_type}) í• ë‹¹ ì™„ë£Œ:\")\n",
    "    for value, count in value_counts.items():\n",
    "        percentage = (count / total) * 100\n",
    "        print(f\"      í´ë˜ìŠ¤ {value}: {count:,}ê°œ ({percentage:.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# S1 ë‹¤ì¤‘ë¶„ë¥˜ ì˜ˆì¸¡ ê²°ê³¼ í• ë‹¹\n",
    "if s1_success and 'predictions' in multiclass_s1_autoencoder:\n",
    "    print(f\"\\nğŸ”„ S1 ë‹¤ì¤‘ë¶„ë¥˜ ì˜ˆì¸¡ ê²°ê³¼ í• ë‹¹ ì¤‘...\")\n",
    "    s1_predictions = multiclass_s1_autoencoder['predictions']\n",
    "    submission_autoencoder = assign_predictions_safely(\n",
    "        submission_autoencoder, 'S1', s1_predictions, \"ë‹¤ì¤‘ë¶„ë¥˜\"\n",
    "    )\n",
    "else:\n",
    "    print(f\"\\nâŒ S1 ë‹¤ì¤‘ë¶„ë¥˜ ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’(0)ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.\")\n",
    "    submission_autoencoder['S1'] = 0\n",
    "\n",
    "# ì´ì§„ë¶„ë¥˜ ì˜ˆì¸¡ ê²°ê³¼ í• ë‹¹\n",
    "print(f\"\\nğŸ”„ ì´ì§„ë¶„ë¥˜ ì˜ˆì¸¡ ê²°ê³¼ í• ë‹¹ ì¤‘...\")\n",
    "for col in ['Q1', 'Q2', 'Q3', 'S2', 'S3']:\n",
    "    if col in binary_preds_autoencoder:\n",
    "        individual_predictions = binary_preds_autoencoder[col]\n",
    "        submission_autoencoder = assign_predictions_safely(\n",
    "            submission_autoencoder, col, individual_predictions, \"ì´ì§„ë¶„ë¥˜\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"   âš ï¸  {col} ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’(0)ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.\")\n",
    "        submission_autoencoder[col] = 0\n",
    "\n",
    "# ìµœì¢… ì œì¶œ í˜•ì‹ ì •ë ¬ ë° ê²€ì¦\n",
    "submission_autoencoder = submission_autoencoder[['subject_id', 'sleep_date', 'lifelog_date', 'Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']]\n",
    "\n",
    "print(f\"\\nğŸ“Š ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½ (ì˜¤í† ì¸ì½”ë” ê¸°ë°˜):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for col in ['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']:\n",
    "    value_counts = submission_autoencoder[col].value_counts().sort_index()\n",
    "    total = len(submission_autoencoder)\n",
    "    print(f\"ğŸ”¹ {col}:\")\n",
    "    for value, count in value_counts.items():\n",
    "        percentage = (count / total) * 100\n",
    "        print(f\"   í´ë˜ìŠ¤ {value}: {count:,}ê°œ ({percentage:.1f}%)\")\n",
    "\n",
    "# ë°ì´í„° í’ˆì§ˆ ê²€ì¦\n",
    "print(f\"\\nğŸ” ë°ì´í„° í’ˆì§ˆ ê²€ì¦:\")\n",
    "print(f\"   - ì´ í–‰ ìˆ˜: {len(submission_autoencoder):,}\")\n",
    "print(f\"   - ê²°ì¸¡ê°’ í™•ì¸:\")\n",
    "for col in ['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']:\n",
    "    null_count = submission_autoencoder[col].isnull().sum()\n",
    "    if null_count > 0:\n",
    "        print(f\"      {col}: {null_count}ê°œ ê²°ì¸¡ê°’ âš ï¸\")\n",
    "    else:\n",
    "        print(f\"      {col}: ê²°ì¸¡ê°’ ì—†ìŒ âœ…\")\n",
    "\n",
    "# íŒŒì¼ ì €ì¥\n",
    "submission_path = submission_folder + submission_file\n",
    "submission_autoencoder.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"\\nâœ… ì˜¤í† ì¸ì½”ë” ê¸°ë°˜ ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ!\")\n",
    "print(f\"   - íŒŒì¼ëª…: {submission_file}\")\n",
    "print(f\"   - ì €ì¥ ê²½ë¡œ: {submission_path}\")\n",
    "print(f\"   - íŒŒì¼ í¬ê¸°: {len(submission_autoencoder):,} í–‰ x {len(submission_autoencoder.columns)} ì—´\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ‰ ì˜¤í† ì¸ì½”ë” ê¸°ë°˜ ëª¨ë¸ë§ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nğŸ“‹ ì™„ë£Œëœ ì‘ì—… ìš”ì•½:\")\n",
    "print(f\"   âœ… ê° ëª¨ë¸ë³„ ìƒìœ„ í”¼ì²˜ ì„ íƒ (20ê°œì”©)\")\n",
    "print(f\"   âœ… ëª¨ë¸ë³„ ì˜¤í† ì¸ì½”ë” í•™ìŠµ ë° íŠ¹ì„± ì¶”ì¶œ\")\n",
    "print(f\"   âœ… ì›ë³¸ + ì¸ì½”ë”© íŠ¹ì„± ê²°í•©\")\n",
    "print(f\"   âœ… SMOTE + LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\")\n",
    "print(f\"   âœ… ì´ì§„ë¶„ë¥˜ ëª¨ë¸ {len(successful_models)}ê°œ í•™ìŠµ\")\n",
    "print(f\"   âœ… S1 ë‹¤ì¤‘ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ\")\n",
    "print(f\"   âœ… ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„±\")\n",
    "\n",
    "print(f\"\\nğŸ” ì„±ëŠ¥ ê°œì„  í¬ì¸íŠ¸:\")\n",
    "print(f\"   - ê° ëª¨ë¸ë³„ ê°œë³„ ì˜¤í† ì¸ì½”ë”ë¡œ íŠ¹ì„± ìµœì í™”\")\n",
    "print(f\"   - StandardScalerë¡œ ì •ê·œí™” ì ìš©\")\n",
    "print(f\"   - SMOTEë¡œ í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°\")\n",
    "print(f\"   - í™•ì¥ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\")\n",
    "print(f\"   - 5-fold êµì°¨ê²€ì¦ìœ¼ë¡œ ê²¬ê³ í•œ í‰ê°€\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
