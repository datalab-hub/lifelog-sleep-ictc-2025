{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1f085c0",
   "metadata": {},
   "source": [
    "### feature 대상으로 LGBM 학습  \n",
    "(Developed from dacon_etri_base_mod1.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0593c4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# 현재 날짜 및 시간 가져오기\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"%m%d_%H%M\")  # 예: 0517_1530\n",
    "\n",
    "\n",
    "submission_folder = '/users/user/Desktop/dacon/submission/'\n",
    "submission_file = f'submission_final_mod4_4_1_{timestamp}.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bead988",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import glob \n",
    "import random \n",
    "import os \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import ast \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903c2113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed 고정 \n",
    "SD = 42 \n",
    "random.seed(SD) \n",
    "np.random.seed(SD) \n",
    "os.environ['PYTHONHASHSEED'] = str(SD)\n",
    "tf.random.set_seed(SD)  # TensorFlow 시드 설정\n",
    "\n",
    "# 파일 경로 설정 - VSCode 상대경로로 변경\n",
    "# 실제 경로에 맞게 수정 필요\n",
    "base_folder =  '/users/user/Desktop/ETRI_lifelog_dataset'\n",
    "folder = '/ch2025_data_items'\n",
    "\n",
    "data_dir = base_folder + folder \n",
    "\n",
    "\n",
    "# Parquet 파일 전체 경로 리스트 \n",
    "parquet_files = glob.glob(os.path.join(data_dir, 'ch2025_*.parquet')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9028dc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 이름을 키로, DataFrame을 값으로 저장할 딕셔너리 \n",
    "lifelog_data = {} \n",
    "\n",
    "# 파일별로 읽기 \n",
    "for file_path in parquet_files: \n",
    "    name = os.path.basename(file_path).replace('.parquet', '').replace('ch2025_', '') \n",
    "    lifelog_data[name] = pd.read_parquet(file_path) \n",
    "    print(f\"✅ Loaded: {name}, shape = {lifelog_data[name].shape}\") \n",
    "\n",
    "# 딕셔너리에 있는 모든 항목을 독립적인 변수로 할당 \n",
    "for key, df in lifelog_data.items(): \n",
    "    globals()[f\"{key}_df\"] = df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbd9424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메트릭스 파일 읽기\n",
    "metrics_train = pd.read_csv(base_folder + '/ch2025_metrics_train.csv')\n",
    "sample_submission = pd.read_csv(base_folder+'/ch2025_submission_sample.csv')\n",
    "\n",
    "# ✅ 기준 쌍 (subject_id, lifelog_date) \n",
    "sample_submission['lifelog_date'] = pd.to_datetime(sample_submission['lifelog_date']) \n",
    "test_keys = set(zip(sample_submission['subject_id'], sample_submission['lifelog_date'].dt.date)) \n",
    "\n",
    "# ✅ DataFrame 별 timestamp 컬럼 수동 지정 \n",
    "dataframes = { \n",
    "    'mACStatus': (mACStatus_df, 'timestamp'), \n",
    "    'mActivity': (mActivity_df, 'timestamp'), \n",
    "    'mAmbience': (mAmbience_df, 'timestamp'), \n",
    "    'mBle': (mBle_df, 'timestamp'), \n",
    "    'mGps': (mGps_df, 'timestamp'), \n",
    "    'mLight': (mLight_df, 'timestamp'), \n",
    "    'mScreenStatus': (mScreenStatus_df, 'timestamp'), \n",
    "    'mUsageStats': (mUsageStats_df, 'timestamp'), \n",
    "    'mWifi': (mWifi_df, 'timestamp'), \n",
    "    'wHr': (wHr_df, 'timestamp'), \n",
    "    'wLight': (wLight_df, 'timestamp'), \n",
    "    'wPedo': (wPedo_df, 'timestamp'), \n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77b6fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 분리 함수 \n",
    "def split_test_train(df, subject_col='subject_id', timestamp_col='timestamp'): \n",
    "    df[timestamp_col] = pd.to_datetime(df[timestamp_col], errors='coerce') \n",
    "    df = df.dropna(subset=[timestamp_col]) \n",
    "    df['date_only'] = df[timestamp_col].dt.date \n",
    "    df['key'] = list(zip(df[subject_col], df['date_only'])) \n",
    "    test_df = df[df['key'].isin(test_keys)].drop(columns=['date_only', 'key']) \n",
    "    train_df = df[~df['key'].isin(test_keys)].drop(columns=['date_only', 'key']) \n",
    "    return test_df, train_df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3416230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 결과 저장 \n",
    "for name, (df, ts_col) in dataframes.items(): \n",
    "    print(f\"⏳ {name} 분리 중...\") \n",
    "    test_df, train_df = split_test_train(df.copy(), subject_col='subject_id', timestamp_col=ts_col) \n",
    "    globals()[f\"{name}_test\"] = test_df \n",
    "    globals()[f\"{name}_train\"] = train_df \n",
    "    print(f\"✅ {name}_test → {test_df.shape}, {name}_train → {train_df.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f34d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mACStatus(df): \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    df = df.sort_values(['subject_id', 'timestamp']) \n",
    "    results = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        status = group['m_charging'].values # 0/1 상태 \n",
    "        times = group['timestamp'].values # 충전 상태 비율 \n",
    "        ratio_charging = status.mean() \n",
    "        # 상태 전이 횟수 \n",
    "        transitions = (status[1:] != status[:-1]).sum() \n",
    "        # 연속된 1 상태 길이들 \n",
    "        lengths = [] \n",
    "        current_len = 0 \n",
    "        for val in status: \n",
    "            if val == 1: \n",
    "                current_len += 1 \n",
    "            elif current_len > 0: \n",
    "                lengths.append(current_len) \n",
    "                current_len = 0 \n",
    "        if current_len > 0: \n",
    "            lengths.append(current_len) \n",
    "        avg_charging_duration = np.mean(lengths) if lengths else 0 \n",
    "        max_charging_duration = np.max(lengths) if lengths else 0 \n",
    "        results.append({ \n",
    "            'subject_id': subj, \n",
    "            'date': date, \n",
    "            'charging_ratio': ratio_charging, \n",
    "            'charging_transitions': transitions, \n",
    "            'avg_charging_duration': avg_charging_duration, \n",
    "            'max_charging_duration': max_charging_duration, \n",
    "        }) \n",
    "    return pd.DataFrame(results) \n",
    "\n",
    "mACStatus_df2 = process_mACStatus(mACStatus_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c3c0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mActivity(df): \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    summary = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        counts = group['m_activity'].value_counts(normalize=True) # 비율 \n",
    "        row = {'subject_id': subj, 'date': date} \n",
    "        # 0~8 비율 저장 \n",
    "        for i in range(9): \n",
    "            row[f'activity_{i}_ratio'] = counts.get(i, 0) \n",
    "        # 주요 활동 정보 \n",
    "        row['dominant_activity'] = group['m_activity'].mode()[0] \n",
    "        row['num_unique_activities'] = group['m_activity'].nunique() \n",
    "        summary.append(row) \n",
    "    return pd.DataFrame(summary) \n",
    "\n",
    "mActivity_df2 = process_mActivity(mActivity_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa6659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_process_mActivity(df):\n",
    "    \"\"\"\n",
    "    모바일 활동 데이터를 일별로 종합 분석하는 함수\n",
    "    기존 비율 기반 분석 + MET 값 + 시간대별 패턴 + 활동 강도 분석\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    # 기본 전처리\n",
    "    df = df.copy()\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['date'] = df['timestamp'].dt.date\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df['minute'] = df['timestamp'].dt.minute\n",
    "    \n",
    "    # MET 값 매핑 (에너지 소비량)\n",
    "    met_mapping = {\n",
    "        0: 1.3,  # 가벼운 좌식 활동\n",
    "        1: 8.0,  # 격렬한 활동\n",
    "        2: 2.0,  # 추정값 (없었던 활동)\n",
    "        3: 1.2,  # 매우 가벼운 활동\n",
    "        4: 3.0,  # 중간 강도 활동\n",
    "        5: 4.0,  # 추정값\n",
    "        6: 5.0,  # 추정값\n",
    "        7: 3.5,  # 중간 강도 활동\n",
    "        8: 10.0  # 매우 격렬한 활동\n",
    "    }\n",
    "    df['met_value'] = df['m_activity'].map(met_mapping).fillna(2.0)\n",
    "    \n",
    "    # 활동 강도 분류\n",
    "    def classify_intensity(activity):\n",
    "        if activity in [0, 3]:\n",
    "            return 'low'\n",
    "        elif activity in [2, 4, 7]:\n",
    "            return 'moderate'\n",
    "        elif activity in [1, 5, 6, 8]:\n",
    "            return 'high'\n",
    "        else:\n",
    "            return 'unknown'\n",
    "    \n",
    "    df['intensity'] = df['m_activity'].apply(classify_intensity)\n",
    "    \n",
    "    summary = []\n",
    "    \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']):\n",
    "        # 기존 비율 계산\n",
    "        counts = group['m_activity'].value_counts(normalize=True)\n",
    "        row = {'subject_id': subj, 'date': date}\n",
    "        \n",
    "        # 1. 기본 활동 비율 (0~8)\n",
    "        for i in range(9):\n",
    "            row[f'activity_{i}_ratio'] = counts.get(i, 0)\n",
    "        \n",
    "        # 2. 기본 통계\n",
    "        row['dominant_activity'] = group['m_activity'].mode()[0] if not group['m_activity'].mode().empty else 0\n",
    "        row['num_unique_activities'] = group['m_activity'].nunique()\n",
    "        \n",
    "        # 3. MET 기반 에너지 분석\n",
    "        row['daily_met_total'] = group['met_value'].sum()\n",
    "        row['daily_met_mean'] = group['met_value'].mean()\n",
    "        row['daily_met_max'] = group['met_value'].max()\n",
    "        row['daily_met_std'] = group['met_value'].std()\n",
    "        \n",
    "        # 4. 활동 강도 분석\n",
    "        intensity_counts = group['intensity'].value_counts(normalize=True)\n",
    "        row['low_intensity_ratio'] = intensity_counts.get('low', 0)\n",
    "        row['moderate_intensity_ratio'] = intensity_counts.get('moderate', 0)\n",
    "        row['high_intensity_ratio'] = intensity_counts.get('high', 0)\n",
    "        \n",
    "        # 5. 시간대별 패턴 분석\n",
    "        # 아침(6-12), 오후(12-18), 저녁(18-24), 밤(0-6)\n",
    "        morning = group[(group['hour'] >= 6) & (group['hour'] < 12)]\n",
    "        afternoon = group[(group['hour'] >= 12) & (group['hour'] < 18)]\n",
    "        evening = group[(group['hour'] >= 18) & (group['hour'] < 24)]\n",
    "        night = group[group['hour'] < 6]\n",
    "        \n",
    "        # 시간대별 주요 활동\n",
    "        row['morning_dominant'] = morning['m_activity'].mode()[0] if len(morning) > 0 and not morning['m_activity'].mode().empty else -1\n",
    "        row['afternoon_dominant'] = afternoon['m_activity'].mode()[0] if len(afternoon) > 0 and not afternoon['m_activity'].mode().empty else -1\n",
    "        row['evening_dominant'] = evening['m_activity'].mode()[0] if len(evening) > 0 and not evening['m_activity'].mode().empty else -1\n",
    "        row['night_dominant'] = night['m_activity'].mode()[0] if len(night) > 0 and not night['m_activity'].mode().empty else -1\n",
    "        \n",
    "        # 시간대별 평균 MET\n",
    "        row['morning_met_avg'] = morning['met_value'].mean() if len(morning) > 0 else 0\n",
    "        row['afternoon_met_avg'] = afternoon['met_value'].mean() if len(afternoon) > 0 else 0\n",
    "        row['evening_met_avg'] = evening['met_value'].mean() if len(evening) > 0 else 0\n",
    "        row['night_met_avg'] = night['met_value'].mean() if len(night) > 0 else 0\n",
    "        \n",
    "        # 6. 활동 변화 패턴\n",
    "        # 연속된 같은 활동의 길이 (세션 길이)\n",
    "        activity_changes = (group['m_activity'] != group['m_activity'].shift()).sum()\n",
    "        row['activity_transitions'] = activity_changes\n",
    "        row['avg_session_length'] = len(group) / max(activity_changes, 1)\n",
    "        \n",
    "        # 7. 고강도 활동 패턴\n",
    "        high_intensity_activities = group[group['intensity'] == 'high']\n",
    "        row['high_intensity_sessions'] = len(high_intensity_activities)\n",
    "        row['max_continuous_high_intensity'] = 0\n",
    "        \n",
    "        if len(high_intensity_activities) > 0:\n",
    "            # 연속된 고강도 활동의 최대 길이\n",
    "            high_intensity_mask = group['intensity'] == 'high'\n",
    "            continuous_lengths = []\n",
    "            current_length = 0\n",
    "            \n",
    "            for is_high in high_intensity_mask:\n",
    "                if is_high:\n",
    "                    current_length += 1\n",
    "                else:\n",
    "                    if current_length > 0:\n",
    "                        continuous_lengths.append(current_length)\n",
    "                    current_length = 0\n",
    "            \n",
    "            if current_length > 0:\n",
    "                continuous_lengths.append(current_length)\n",
    "            \n",
    "            row['max_continuous_high_intensity'] = max(continuous_lengths) if continuous_lengths else 0\n",
    "        \n",
    "        # 8. 활동 다양성 지수 (엔트로피)\n",
    "        if len(counts) > 1:\n",
    "            entropy = -sum(counts * np.log2(counts))\n",
    "            row['activity_entropy'] = entropy\n",
    "        else:\n",
    "            row['activity_entropy'] = 0\n",
    "        \n",
    "        # 9. 시간대별 활동 비율\n",
    "        total_records = len(group)\n",
    "        row['morning_activity_ratio'] = len(morning) / total_records if total_records > 0 else 0\n",
    "        row['afternoon_activity_ratio'] = len(afternoon) / total_records if total_records > 0 else 0\n",
    "        row['evening_activity_ratio'] = len(evening) / total_records if total_records > 0 else 0\n",
    "        row['night_activity_ratio'] = len(night) / total_records if total_records > 0 else 0\n",
    "        \n",
    "        # 10. 활동 강도 변화\n",
    "        if len(group) > 1:\n",
    "            met_diff = group['met_value'].diff().abs().mean()\n",
    "            row['met_variability'] = met_diff\n",
    "        else:\n",
    "            row['met_variability'] = 0\n",
    "            \n",
    "        summary.append(row)\n",
    "    \n",
    "    return pd.DataFrame(summary)\n",
    "\n",
    "# 사용 예시\n",
    "enhanced_mActivity_df2 = enhanced_process_mActivity(mActivity_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddb753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지정된 10개 라벨 \n",
    "top_10_labels = [ \n",
    "    \"Inside, small room\", \"Speech\", \"Silence\", \"Music\", \"Narration, monologue\", \n",
    "    \"Child speech, kid speaking\", \"Conversation\", \"Speech synthesizer\", \"Shout\", \"Babbling\" \n",
    "] \n",
    "\n",
    "def process_mAmbience_top10(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    # 초기화 \n",
    "    for label in top_10_labels + ['others']: \n",
    "        df[label] = 0.0 \n",
    "    for idx, row in df.iterrows(): \n",
    "        parsed = ast.literal_eval(row['m_ambience']) if isinstance(row['m_ambience'], str) else row['m_ambience'] \n",
    "        others_prob = 0.0 \n",
    "        for label, prob in parsed: \n",
    "            prob = float(prob) \n",
    "            if label in top_10_labels: \n",
    "                df.at[idx, label] = prob \n",
    "            else: \n",
    "                others_prob += prob \n",
    "        df.at[idx, 'others'] = others_prob \n",
    "    return df.drop(columns=['m_ambience']) \n",
    "\n",
    "mAmbience_df2= process_mAmbience_top10(mAmbience_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cc98b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_mAmbience_daily(df): \n",
    "    prob_cols = [col for col in df.columns if col not in ['subject_id', 'timestamp', 'date']] \n",
    "    # 하루 단위로 평균값 요약 \n",
    "    daily_summary = df.groupby(['subject_id', 'date'])[prob_cols].mean().reset_index() \n",
    "    return daily_summary \n",
    "\n",
    "mAmbience_df2 = summarize_mAmbience_daily(mAmbience_df2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763e541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mBle(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    features = [] \n",
    "    for idx, row in df.iterrows(): \n",
    "        entry = ast.literal_eval(row['m_ble']) if isinstance(row['m_ble'], str) else row['m_ble'] \n",
    "        rssi_list = [] \n",
    "        class_0_cnt = 0 \n",
    "        class_other_cnt = 0 \n",
    "        for device in entry: \n",
    "            try: \n",
    "                rssi = int(device['rssi']) \n",
    "                rssi_list.append(rssi) \n",
    "                if str(device['device_class']) == '0': \n",
    "                    class_0_cnt += 1 \n",
    "                else: \n",
    "                    class_other_cnt += 1 \n",
    "            except: \n",
    "                continue # malformed record \n",
    "        feature = { \n",
    "            'subject_id': row['subject_id'], \n",
    "            'date': row['date'], \n",
    "            'device_class_0_cnt': class_0_cnt, \n",
    "            'device_class_others_cnt': class_other_cnt, \n",
    "            'device_count': len(rssi_list), \n",
    "            'rssi_mean': np.mean(rssi_list) if rssi_list else np.nan, \n",
    "            'rssi_min': np.min(rssi_list) if rssi_list else np.nan, \n",
    "            'rssi_max': np.max(rssi_list) if rssi_list else np.nan, \n",
    "        } \n",
    "        features.append(feature) \n",
    "    return pd.DataFrame(features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38ac08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_mBle_daily(df): \n",
    "    # row 단위 BLE feature 추출 \n",
    "    df = process_mBle(df) \n",
    "    # 하루 단위로 cnt 합치기 \n",
    "    grouped = df.groupby(['subject_id', 'date']).agg({ \n",
    "        'device_class_0_cnt': 'sum', \n",
    "        'device_class_others_cnt': 'sum', \n",
    "        'rssi_mean': 'mean', \n",
    "        'rssi_min': 'min', \n",
    "        'rssi_max': 'max', \n",
    "    }).reset_index() \n",
    "    # 총합 구해서 비율 계산 \n",
    "    total_cnt = grouped['device_class_0_cnt'] + grouped['device_class_others_cnt'] \n",
    "    grouped['device_class_0_ratio'] = grouped['device_class_0_cnt'] / total_cnt.replace(0, np.nan) \n",
    "    grouped['device_class_others_ratio'] = grouped['device_class_others_cnt'] / total_cnt.replace(0, np.nan) \n",
    "    # 필요 없는 원래 cnt 컬럼 제거 \n",
    "    grouped.drop(columns=['device_class_0_cnt', 'device_class_others_cnt'], inplace=True) \n",
    "    return grouped \n",
    "\n",
    "mBle_df2 = summarize_mBle_daily(mBle_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618d366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mGps(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    features = [] \n",
    "    for idx, row in df.iterrows(): \n",
    "        gps_list = ast.literal_eval(row['m_gps']) if isinstance(row['m_gps'], str) else row['m_gps'] \n",
    "        altitudes = [] \n",
    "        latitudes = [] \n",
    "        longitudes = [] \n",
    "        speeds = [] \n",
    "        for entry in gps_list: \n",
    "            try: \n",
    "                altitudes.append(float(entry['altitude'])) \n",
    "                latitudes.append(float(entry['latitude'])) \n",
    "                longitudes.append(float(entry['longitude'])) \n",
    "                speeds.append(float(entry['speed'])) \n",
    "            except: \n",
    "                continue \n",
    "        features.append({ \n",
    "            'subject_id': row['subject_id'], \n",
    "            'date': row['date'], \n",
    "            'altitude_mean': np.mean(altitudes) if altitudes else np.nan, \n",
    "            'latitude_std': np.std(latitudes) if latitudes else np.nan, \n",
    "            'longitude_std': np.std(longitudes) if longitudes else np.nan, \n",
    "            'speed_mean': np.mean(speeds) if speeds else np.nan, \n",
    "            'speed_max': np.max(speeds) if speeds else np.nan, \n",
    "            'speed_std': np.std(speeds) if speeds else np.nan, \n",
    "        }) \n",
    "    return pd.DataFrame(features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a0be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_Gps_df2 = process_mGps(mGps_df) \n",
    "m_Gps_df2 = m_Gps_df2.groupby(['subject_id', 'date']).agg({ \n",
    "    'altitude_mean': 'mean', \n",
    "    'latitude_std': 'mean', \n",
    "    'longitude_std': 'mean', \n",
    "    'speed_mean': 'mean', \n",
    "    'speed_max': 'max', \n",
    "    'speed_std': 'mean' \n",
    "}).reset_index() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8389d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mLight(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    df['hour'] = df['timestamp'].dt.hour \n",
    "    # 밤(22~05시), 낮(06~21시) 구분 \n",
    "    df['is_night'] = df['hour'].apply(lambda h: h >= 22 or h < 6) \n",
    "    # 하루 단위 요약 \n",
    "    daily = df.groupby(['subject_id', 'date']).agg( \n",
    "        light_mean=('m_light', 'mean'), \n",
    "        light_std=('m_light', 'std'), \n",
    "        light_max=('m_light', 'max'), \n",
    "        light_min=('m_light', 'min'), \n",
    "        light_night_mean=('m_light', lambda x: x[df.loc[x.index, 'is_night']].mean()), \n",
    "        light_day_mean=('m_light', lambda x: x[~df.loc[x.index, 'is_night']].mean()), \n",
    "        light_night_ratio=('is_night', 'mean') # 밤 시간 측정 비율 \n",
    "    ).reset_index() \n",
    "    return daily \n",
    "\n",
    "mLight_df2 = process_mLight(mLight_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ad526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mScreenStatus(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    features = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        status = group['m_screen_use'].values \n",
    "        ratio_on = status.mean() \n",
    "        transitions = (status[1:] != status[:-1]).sum() \n",
    "        # 연속된 1 상태 길이들 \n",
    "        durations = [] \n",
    "        current = 0 \n",
    "        for val in status: \n",
    "            if val == 1: \n",
    "                current += 1 \n",
    "            elif current > 0: \n",
    "                durations.append(current) \n",
    "                current = 0 \n",
    "        if current > 0: \n",
    "            durations.append(current) \n",
    "        features.append({ \n",
    "            'subject_id': subj, \n",
    "            'date': date, \n",
    "            'screen_on_ratio': ratio_on, \n",
    "            'screen_on_transitions': transitions, \n",
    "            'screen_on_duration_avg': np.mean(durations) if durations else 0, \n",
    "            'screen_on_duration_max': np.max(durations) if durations else 0, \n",
    "        }) \n",
    "    return pd.DataFrame(features) \n",
    "\n",
    "mScreenStatus_df2 = process_mScreenStatus(mScreenStatus_df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04edbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_apps = [ \n",
    "    'One UI 홈', '카카오톡', '시스템 UI', 'NAVER', '캐시워크', \n",
    "    '성경일독Q', 'YouTube', '통화', '메시지', '타임스프레드', 'Instagram'\n",
    "] \n",
    "\n",
    "def process_mUsageStats(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    features = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        app_time = {app: 0 for app in top_apps} \n",
    "        others_time = 0 \n",
    "        for row in group['m_usage_stats']: \n",
    "            parsed = ast.literal_eval(row) if isinstance(row, str) else row \n",
    "            for entry in parsed: \n",
    "                app = entry.get('app_name') \n",
    "                time = entry.get('total_time', 0) \n",
    "                if app in top_apps: \n",
    "                    app_time[app] += int(time) \n",
    "                else: \n",
    "                    others_time += int(time) \n",
    "        feature = { \n",
    "            'subject_id': subj, \n",
    "            'date': date, \n",
    "            'others_time': others_time \n",
    "        } \n",
    "        # 각 앱별 컬럼 추가 \n",
    "        feature.update({f'{app}_time': app_time[app] for app in top_apps}) \n",
    "        features.append(feature) \n",
    "    return pd.DataFrame(features) \n",
    "\n",
    "mUsageStats_df2 = process_mUsageStats(mUsageStats_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d68686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앱 사용 시간 (합계)\n",
    "mUsageStats_df3 = mUsageStats_df.copy() \n",
    "# 방법 1: 기존 DataFrame에 새로운 컬럼 추가\n",
    "def calculate_total_usage_time(usage_stats):\n",
    "    \"\"\"m_usage_stats에서 모든 앱의 total_time 합계 계산\"\"\"\n",
    "    total_time = 0\n",
    "    for app in usage_stats:\n",
    "        total_time += app['total_time']\n",
    "    return total_time\n",
    "\n",
    "# m_usage_time 컬럼 추가\n",
    "mUsageStats_df3['m_usage_time'] = mUsageStats_df3['m_usage_stats'].apply(calculate_total_usage_time)\n",
    "\n",
    "# # 결과 확인\n",
    "# print(\"새로운 컬럼이 추가된 DataFrame:\")\n",
    "# print(mUsageStats_df3[['subject_id', 'timestamp', 'm_usage_time']].head())\n",
    "\n",
    "\n",
    "# # 전체 구조 확인\n",
    "# print(f\"\\nDataFrame shape: {mUsageStats_df3.shape}\")\n",
    "# print(f\"Columns: {mUsageStats_df3.columns.tolist()}\")\n",
    "\n",
    "mUsageStats_df3 = mUsageStats_df3[['subject_id', 'timestamp', 'm_usage_time']]\n",
    "\n",
    "# 간단한 방법: 날짜별 사용시간 합계\n",
    "mUsageStats_df3['date'] = pd.to_datetime(mUsageStats_df3['timestamp']).dt.date\n",
    "\n",
    "# 날짜별 합계 계산\n",
    "mUsageStats_df3 = mUsageStats_df3.groupby(['subject_id', 'date'])['m_usage_time'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bcd6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_mWifi(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    results = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        rssi_all = [] \n",
    "        for row in group['m_wifi']: \n",
    "            parsed = ast.literal_eval(row) if isinstance(row, str) else row \n",
    "            for ap in parsed: \n",
    "                try: \n",
    "                    rssi = int(ap['rssi']) \n",
    "                    rssi_all.append(rssi) \n",
    "                except: \n",
    "                    continue \n",
    "        results.append({ \n",
    "            'subject_id': subj, \n",
    "            'date': date, \n",
    "            'wifi_rssi_mean': np.mean(rssi_all) if rssi_all else np.nan, \n",
    "            'wifi_rssi_min': np.min(rssi_all) if rssi_all else np.nan, \n",
    "            'wifi_rssi_max': np.max(rssi_all) if rssi_all else np.nan, \n",
    "            'wifi_detected_cnt': len(rssi_all) \n",
    "        }) \n",
    "    return pd.DataFrame(results) \n",
    "\n",
    "mWifi_df2 = process_mWifi(mWifi_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83782374",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_time_block(hour): \n",
    "    if 0 <= hour < 6: \n",
    "        return 'early_morning' \n",
    "    elif 6 <= hour < 12: \n",
    "        return 'morning' \n",
    "    elif 12 <= hour < 18: \n",
    "        return 'afternoon' \n",
    "    else: \n",
    "        return 'evening' \n",
    "\n",
    "def process_wHr_by_timeblock(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    df['block'] = df['timestamp'].dt.hour.map(get_time_block) \n",
    "    results = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        block_stats = {'subject_id': subj, 'date': date} \n",
    "        for block, block_group in group.groupby('block'): \n",
    "            hr_all = [] \n",
    "            for row in block_group['heart_rate']: \n",
    "                parsed = ast.literal_eval(row) if isinstance(row, str) else row \n",
    "                hr_all.extend([int(h) for h in parsed if h is not None]) \n",
    "            if not hr_all: \n",
    "                continue \n",
    "            above_100 = [hr for hr in hr_all if hr > 100] \n",
    "            block_stats[f'hr_{block}_mean'] = np.mean(hr_all) \n",
    "            block_stats[f'hr_{block}_std'] = np.std(hr_all) \n",
    "            block_stats[f'hr_{block}_max'] = np.max(hr_all) \n",
    "            block_stats[f'hr_{block}_min'] = np.min(hr_all) \n",
    "            block_stats[f'hr_{block}_above_100_ratio'] = len(above_100) / len(hr_all) \n",
    "        results.append(block_stats) \n",
    "    return pd.DataFrame(results) \n",
    "\n",
    "wHr_df2 = process_wHr_by_timeblock(wHr_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39b5000",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_wLight_by_timeblock(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    df['block'] = df['timestamp'].dt.hour.map(get_time_block) \n",
    "    results = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        block_stats = {'subject_id': subj, 'date': date} \n",
    "        for block, block_group in group.groupby('block'): \n",
    "            lux = block_group['w_light'].dropna().values \n",
    "            if len(lux) == 0: \n",
    "                continue \n",
    "            block_stats[f'wlight_{block}_mean'] = np.mean(lux) \n",
    "            block_stats[f'wlight_{block}_std'] = np.std(lux) \n",
    "            block_stats[f'wlight_{block}_max'] = np.max(lux) \n",
    "            block_stats[f'wlight_{block}_min'] = np.min(lux) \n",
    "        results.append(block_stats) \n",
    "    return pd.DataFrame(results) \n",
    "\n",
    "wLight_df2 = process_wLight_by_timeblock(wLight_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55ce49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_wPedo(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    summary = df.groupby(['subject_id', 'date']).agg({ \n",
    "        'step': 'sum', \n",
    "        'step_frequency': 'mean', \n",
    "        'distance': 'sum', \n",
    "        'speed': ['mean', 'max'], \n",
    "        'burned_calories': 'sum' \n",
    "    }).reset_index() \n",
    "    # 컬럼 이름 정리 \n",
    "    summary.columns = ['subject_id', 'date', 'step_sum', 'step_frequency_mean', 'distance_sum', 'speed_mean', 'speed_max', 'burned_calories_sum'] \n",
    "    return summary \n",
    "\n",
    "wPedo_df2 = process_wPedo(wPedo_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c9a3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from functools import reduce \n",
    "df_list = [ \n",
    "    mACStatus_df2, \n",
    "    mActivity_df2,\n",
    "    enhanced_mActivity_df2,  \n",
    "    mAmbience_df2, \n",
    "    mBle_df2, \n",
    "    m_Gps_df2, \n",
    "    mLight_df2, \n",
    "    mScreenStatus_df2, \n",
    "    mUsageStats_df2,\n",
    "    mUsageStats_df3,\n",
    "    mWifi_df2, \n",
    "    wHr_df2, \n",
    "    wLight_df2, \n",
    "    wPedo_df2 \n",
    "] \n",
    "\n",
    "merged_df = reduce(lambda left, right: pd.merge(left, right, on=['subject_id', 'date'], how='outer'), df_list) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5335404",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# metrics_train의 lifelog_date → datetime.date 형으로 변환 \n",
    "metrics_train['lifelog_date'] = pd.to_datetime(metrics_train['lifelog_date']).dt.date \n",
    "\n",
    "# merged_df의 date도 변환 \n",
    "merged_df['date'] = pd.to_datetime(merged_df['date']).dt.date \n",
    "\n",
    "# 1. date 기준 정렬을 위해 metrics_train의 lifelog_date -> date로 맞추기 \n",
    "metrics_train_renamed = metrics_train.rename(columns={'lifelog_date': 'date'}) \n",
    "\n",
    "# 2. train_df: metrics_train과 일치하는 (subject_id, date) → 라벨 포함 \n",
    "train_df = pd.merge(metrics_train_renamed, merged_df, on=['subject_id', 'date'], how='inner') \n",
    "\n",
    "# 3. test_df: metrics_train에 없는 (subject_id, date) \n",
    "merged_keys = merged_df[['subject_id', 'date']] \n",
    "train_keys = metrics_train_renamed[['subject_id', 'date']] \n",
    "test_keys = pd.merge(merged_keys, train_keys, on=['subject_id', 'date'], how='left', indicator=True) \n",
    "test_keys = test_keys[test_keys['_merge'] == 'left_only'].drop(columns=['_merge']) \n",
    "test_df = pd.merge(test_keys, merged_df, on=['subject_id', 'date'], how='left') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c98b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ✅ 타겟 리스트 \n",
    "targets_binary = ['Q1', 'Q2', 'Q3', 'S2', 'S3'] \n",
    "target_multiclass = 'S1' \n",
    "\n",
    "# ✅ feature 준비 \n",
    "X = train_df.drop(columns=['subject_id', 'sleep_date', 'date', 'Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']) \n",
    "X.fillna(0, inplace=True) # 결측값 처리 \n",
    "\n",
    "test_X = test_df.drop(columns=['subject_id', 'date']) \n",
    "test_X.fillna(0, inplace=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d497e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 컬럼 이름에서 특수 문자 제거/변환 \n",
    "def sanitize_column_names(df): \n",
    "    df.columns = ( \n",
    "        df.columns \n",
    "        .str.replace(r\"[^\\w]\", \"_\", regex=True) # 특수문자 → _ \n",
    "        .str.replace(r\"__+\", \"_\", regex=True) # 연속된 _ 제거 \n",
    "        .str.strip(\"_\") # 앞뒤 _ 제거 \n",
    "    ) \n",
    "    return df \n",
    "\n",
    "# 모든 입력에 적용 \n",
    "X = sanitize_column_names(X) \n",
    "test_X = sanitize_column_names(test_X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3ca789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 결과 저장 \n",
    "# # 그룹별 중요 특성 선택\n",
    "# important_features = [\n",
    "#     # 스크린 관련 특성\n",
    "#     'screen_on_ratio', 'screen_on_duration_avg', 'screen_on_duration_max',\n",
    "    \n",
    "#     # 와이파이 신호 관련 특성\n",
    "#     'wifi_rssi_max', 'wifi_rssi_mean', 'wifi_detected_cnt',\n",
    "    \n",
    "#     # 활동 관련 특성\n",
    "#     'activity_3_ratio', 'activity_4_ratio',\n",
    "    \n",
    "#     # 충전 관련 특성\n",
    "#     'charging_ratio', 'max_charging_duration', 'avg_charging_duration',\n",
    "    \n",
    "#     # 신호 관련 특성\n",
    "#     'rssi_mean',\n",
    "    \n",
    "#     # 빛 관련 특성\n",
    "#     'light_night_mean', 'light_max', 'light_std',\n",
    "    \n",
    "#     # 위치/움직임 관련 특성\n",
    "#     'altitude_mean', 'speed_max_x',\n",
    "    \n",
    "#     # 앱 사용 관련 특성\n",
    "#     '메시지_time', 'others_time', 'Narration_monologue'\n",
    "# ]\n",
    "\n",
    "# # 선택된 특성만으로 데이터셋 구성\n",
    "# X_selected = X[important_features]\n",
    "# test_X_selected = test_X[important_features]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d1d732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM 라이브러리 임포트\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping \n",
    "\n",
    "# GridSearch를 위한 교차 검증 설정\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# LightGBM 기본 파라미터 설정\n",
    "lgbm_params = {\n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate': 0.03,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "# 이진 분류 모델 성능 향상을 위한 그리드 서치 파라미터\n",
    "binary_param_grid = {\n",
    "    'learning_rate': [0.01, 0.03],\n",
    "    'n_estimators': [500, 1000],\n",
    "    'num_leaves': [50, 100],\n",
    "    'max_depth': [-1, 5],\n",
    "    'min_child_samples': [10, 30],\n",
    "    # 'reg_alpha': [0, 0.01, 0.1],\n",
    "    # 'reg_lambda': [0, 0.01, 0.1],\n",
    "}\n",
    "\n",
    "# 다중 분류 모델 성능 향상을 위한 그리드 서치 파라미터\n",
    "multi_param_grid = {\n",
    "    'learning_rate': [0.01, 0.03],\n",
    "    'n_estimators': [500, 1000],\n",
    "    'num_leaves': [50, 100],\n",
    "    'max_depth': [-1, 5],\n",
    "    'min_child_samples': [10, 30],\n",
    "    # 'reg_alpha': [0, 0.01, 0.1],\n",
    "    # 'reg_lambda': [0, 0.01, 0.1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e1d502",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost\n",
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34f548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ------------------ 앙상블 모델 설정 ------------------\n",
    "def create_ensemble_models(lgbm_params, n_classes=None):\n",
    "    \"\"\"앙상블을 위한 기본 모델들 생성\"\"\"\n",
    "    \n",
    "    # LightGBM\n",
    "    if n_classes is None:  # 이진 분류\n",
    "        lgbm_model = LGBMClassifier(objective='binary', **lgbm_params)\n",
    "    else:  # 다중 분류\n",
    "        lgbm_model = LGBMClassifier(objective='multiclass', num_class=n_classes, **lgbm_params)\n",
    "    \n",
    "    # XGBoost\n",
    "    if n_classes is None:  # 이진 분류\n",
    "        xgb_model = XGBClassifier(\n",
    "            objective='binary:logistic',\n",
    "            eval_metric='logloss',\n",
    "            random_state=42,\n",
    "            n_estimators=100,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            verbosity=0\n",
    "        )\n",
    "    else:  # 다중 분류\n",
    "        xgb_model = XGBClassifier(\n",
    "            objective='multi:softprob',\n",
    "            eval_metric='mlogloss',\n",
    "            num_class=n_classes,\n",
    "            random_state=42,\n",
    "            n_estimators=100,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            verbosity=0\n",
    "        )\n",
    "    \n",
    "    # CatBoost\n",
    "    if n_classes is None:  # 이진 분류\n",
    "        cat_model = CatBoostClassifier(\n",
    "            objective='Logloss',\n",
    "            random_state=42,\n",
    "            iterations=100,\n",
    "            depth=6,\n",
    "            learning_rate=0.1,\n",
    "            verbose=False\n",
    "        )\n",
    "    else:  # 다중 분류\n",
    "        cat_model = CatBoostClassifier(\n",
    "            objective='MultiClass',\n",
    "            random_state=42,\n",
    "            iterations=100,\n",
    "            depth=6,\n",
    "            learning_rate=0.1,\n",
    "            verbose=False\n",
    "        )\n",
    "    \n",
    "    return lgbm_model, xgb_model, cat_model\n",
    "\n",
    "def create_voting_ensemble(lgbm_model, xgb_model, cat_model):\n",
    "    \"\"\"Voting 앙상블 생성\"\"\"\n",
    "    voting_ensemble = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('lgbm', lgbm_model),\n",
    "            ('xgb', xgb_model),\n",
    "            ('cat', cat_model)\n",
    "        ],\n",
    "        voting='soft'  # 확률 기반 투표\n",
    "    )\n",
    "    return voting_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502dcc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 다중 분류 학습 (S1)\n",
    "y_multi = train_df['S1']\n",
    "\n",
    "# S1의 클래스 개수 확인\n",
    "n_classes = len(y_multi.unique())\n",
    "print(f\"S1 클래스 개수: {n_classes}\")\n",
    "\n",
    "\n",
    "# 앙상블 모델 생성\n",
    "lgbm_model, xgb_model, cat_model = create_ensemble_models(lgbm_params, n_classes=n_classes)\n",
    "voting_ensemble = create_voting_ensemble(lgbm_model, xgb_model, cat_model)\n",
    "\n",
    "\n",
    "# 다중 분류 학습 (S1)\n",
    "model_s1 = voting_ensemble\n",
    "model_s1.fit(X, y_multi)\n",
    "multiclass_pred = model_s1.predict(test_X)\n",
    "\n",
    "# # 특성 중요도 추출 및 데이터프레임 생성\n",
    "# feature_importance = pd.DataFrame({\n",
    "#     'feature': X.columns,  # important_features 대신 X.columns 사용\n",
    "#     'importance': model_s1.feature_importances_\n",
    "# }).sort_values('importance', ascending=False)\n",
    "\n",
    "# # 결과 요약 - 특성 중요도 상위 20개 상세 표시\n",
    "# print(\"\\n===== 주요 특성 중요도 (상위 20개) =====\")\n",
    "# top_20_features = feature_importance.head(20)\n",
    "\n",
    "# # 인덱스와 함께 특성명과 중요도 값 출력\n",
    "# for idx, (_, row) in enumerate(top_20_features.iterrows(), 1):\n",
    "#     print(f\"{idx:2d}. {row['feature']:<30} : {row['importance']:.6f}\")\n",
    "\n",
    "# # 전체 특성 중요도 통계\n",
    "# print(f\"\\n===== 특성 중요도 통계 =====\")\n",
    "# print(f\"전체 특성 개수: {len(feature_importance)}\")\n",
    "# print(f\"최대 중요도: {feature_importance['importance'].max():.6f}\")\n",
    "# print(f\"최소 중요도: {feature_importance['importance'].min():.6f}\")\n",
    "# print(f\"평균 중요도: {feature_importance['importance'].mean():.6f}\")\n",
    "# print(f\"상위 20개 중요도 합계: {top_20_features['importance'].sum():.6f}\")\n",
    "# print(f\"전체 중요도 대비 상위 20개 비율: {(top_20_features['importance'].sum() / feature_importance['importance'].sum() * 100):.2f}%\")\n",
    "\n",
    "# # 시각화 - 상위 20개만\n",
    "# plt.figure(figsize=(12, 10))\n",
    "# sns.barplot(data=top_20_features, x='importance', y='feature', palette='viridis')\n",
    "# plt.title('상위 20개 특성 중요도', fontsize=16, fontweight='bold')\n",
    "# plt.xlabel('중요도', fontsize=12)\n",
    "# plt.ylabel('특성명', fontsize=12)\n",
    "\n",
    "# # 각 막대에 중요도 값 표시\n",
    "# for i, (_, row) in enumerate(top_20_features.iterrows()):\n",
    "#     plt.text(row['importance'] + 0.001, i, f'{row[\"importance\"]:.4f}', \n",
    "#              va='center', fontsize=9)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('feature_importance_top20.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# # 상위 20개 특성명만 별도로 리스트 생성\n",
    "# top_20_feature_names = top_20_features['feature'].tolist()\n",
    "# print(f\"\\n===== 상위 20개 특성명 리스트 =====\")\n",
    "# print(top_20_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e382154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 개별 모델의 feature importance 추출 =====\n",
    "# 각 모델의 feature importance 추출\n",
    "lgbm_importance = model_s1.named_estimators_['lgbm'].feature_importances_\n",
    "xgb_importance = model_s1.named_estimators_['xgb'].feature_importances_\n",
    "cat_importance = model_s1.named_estimators_['cat'].feature_importances_\n",
    "\n",
    "# 평균 feature importance 계산\n",
    "avg_importance = (lgbm_importance + xgb_importance + cat_importance) / 3\n",
    "\n",
    "# 특성 중요도 데이터프레임 생성\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': avg_importance,\n",
    "    'lgbm_importance': lgbm_importance,\n",
    "    'xgb_importance': xgb_importance,\n",
    "    'cat_importance': cat_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# 결과 요약 - 특성 중요도 상위 20개 상세 표시\n",
    "print(\"\\n===== 주요 특성 중요도 (상위 20개) - 앙상블 평균 =====\")\n",
    "top_20_features = feature_importance.head(20)\n",
    "\n",
    "for idx, (_, row) in enumerate(top_20_features.iterrows(), 1):\n",
    "    print(f\"{idx:2d}. {row['feature']:<30} : {row['importance']:.6f}\")\n",
    "\n",
    "# 개별 모델별 중요도도 표시\n",
    "print(\"\\n===== 개별 모델별 상위 10개 특성 중요도 =====\")\n",
    "print(\"\\n--- LightGBM ---\")\n",
    "lgbm_top10 = feature_importance.nlargest(10, 'lgbm_importance')\n",
    "for idx, (_, row) in enumerate(lgbm_top10.iterrows(), 1):\n",
    "    print(f\"{idx:2d}. {row['feature']:<30} : {row['lgbm_importance']:.6f}\")\n",
    "\n",
    "print(\"\\n--- XGBoost ---\")\n",
    "xgb_top10 = feature_importance.nlargest(10, 'xgb_importance')\n",
    "for idx, (_, row) in enumerate(xgb_top10.iterrows(), 1):\n",
    "    print(f\"{idx:2d}. {row['feature']:<30} : {row['xgb_importance']:.6f}\")\n",
    "\n",
    "print(\"\\n--- CatBoost ---\")\n",
    "cat_top10 = feature_importance.nlargest(10, 'cat_importance')\n",
    "for idx, (_, row) in enumerate(cat_top10.iterrows(), 1):\n",
    "    print(f\"{idx:2d}. {row['feature']:<30} : {row['cat_importance']:.6f}\")\n",
    "\n",
    "# 전체 특성 중요도 통계\n",
    "print(f\"\\n===== 특성 중요도 통계 (앙상블 평균) =====\")\n",
    "print(f\"전체 특성 개수: {len(feature_importance)}\")\n",
    "print(f\"최대 중요도: {feature_importance['importance'].max():.6f}\")\n",
    "print(f\"최소 중요도: {feature_importance['importance'].min():.6f}\")\n",
    "print(f\"평균 중요도: {feature_importance['importance'].mean():.6f}\")\n",
    "print(f\"상위 20개 중요도 합계: {top_20_features['importance'].sum():.6f}\")\n",
    "print(f\"전체 중요도 대비 상위 20개 비율: {(top_20_features['importance'].sum() / feature_importance['importance'].sum() * 100):.2f}%\")\n",
    "\n",
    "# 시각화 - 앙상블 평균 중요도\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 서브플롯 생성\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
    "\n",
    "# 1. 앙상블 평균 중요도 (상위 20개)\n",
    "sns.barplot(data=top_20_features, x='importance', y='feature', palette='viridis', ax=axes[0,0])\n",
    "axes[0,0].set_title('앙상블 평균 특성 중요도 (상위 20개)', fontsize=14, fontweight='bold')\n",
    "axes[0,0].set_xlabel('중요도', fontsize=12)\n",
    "axes[0,0].set_ylabel('특성명', fontsize=12)\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ensemble_feature_importance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 상위 20개 특성명 리스트\n",
    "top_20_feature_names = top_20_features['feature'].tolist()\n",
    "print(f\"\\n===== 상위 20개 특성명 리스트 (앙상블 평균 기준) =====\")\n",
    "print(top_20_feature_names)\n",
    "\n",
    "# 모델 간 중요도 상관관계 분석\n",
    "print(f\"\\n===== 모델 간 특성 중요도 상관관계 =====\")\n",
    "corr_lgbm_xgb = np.corrcoef(lgbm_importance, xgb_importance)[0,1]\n",
    "corr_lgbm_cat = np.corrcoef(lgbm_importance, cat_importance)[0,1]\n",
    "corr_xgb_cat = np.corrcoef(xgb_importance, cat_importance)[0,1]\n",
    "\n",
    "print(f\"LightGBM vs XGBoost: {corr_lgbm_xgb:.4f}\")\n",
    "print(f\"LightGBM vs CatBoost: {corr_lgbm_cat:.4f}\")\n",
    "print(f\"XGBoost vs CatBoost: {corr_xgb_cat:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c973cf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선택된 특성만으로 데이터셋 구성\n",
    "X_selected = X[top_20_feature_names]\n",
    "test_X_selected = test_X[top_20_feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcb2c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n===== 오토인코더를 사용한 고급 특성 추출 =====\")\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "encoding_dim = min(64, X_selected.shape[1])  # 인코딩 차원 (원본 특성 수보다 작게 설정)\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "\n",
    "# 입력 차원\n",
    "input_dim = X_selected.shape[1]\n",
    "\n",
    "# 오토인코더 모델 구축\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "\n",
    "# 인코더 부분\n",
    "encoded = Dense(128, activation='relu')(input_layer)\n",
    "encoded = BatchNormalization()(encoded)\n",
    "encoded = Dropout(0.3)(encoded)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "encoded = BatchNormalization()(encoded)\n",
    "encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "\n",
    "# 디코더 부분\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = BatchNormalization()(decoded)\n",
    "decoded = Dropout(0.3)(decoded)\n",
    "decoded = Dense(128, activation='relu')(decoded)\n",
    "decoded = BatchNormalization()(decoded)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "# 전체 오토인코더 모델\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# 인코더 부분만 추출 (특성 추출용)\n",
    "encoder = Model(input_layer, encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea730fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련/검증 세트 분리\n",
    "X_train_auto, X_val_auto = train_test_split(X_selected, test_size=0.2, random_state=42)\n",
    "\n",
    "# 조기 종료 설정\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# 오토인코더 모델 학습\n",
    "print(\"오토인코더 학습 중...\")\n",
    "autoencoder.fit(\n",
    "    X_train_auto, X_train_auto,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_val_auto, X_val_auto),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 인코딩된 특성 추출\n",
    "print(\"인코딩된 특성 추출 중...\")\n",
    "encoded_features_train = encoder.predict(X_selected)\n",
    "encoded_features_test = encoder.predict(test_X_selected)\n",
    "\n",
    "# 원본 특성과 인코딩된 특성 결합\n",
    "X_combined = np.hstack([X_selected, encoded_features_train])\n",
    "test_X_combined = np.hstack([test_X_selected, encoded_features_test])\n",
    "\n",
    "print(f\"원본 특성 수: {X_selected.shape[1]}\")\n",
    "print(f\"인코딩된 특성 수: {encoded_features_train.shape[1]}\")\n",
    "print(f\"결합된 특성 수: {X_combined.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3171ae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ 결합된 특성으로 이진 분류 모델 학습 (앙상블 적용) ------------------\n",
    "binary_preds_combined = {}\n",
    "binary_preds_voting = {}\n",
    "binary_preds_stacking = {}\n",
    "\n",
    "for col in targets_binary:\n",
    "    print(f\"\\n===== {col} 이진 분류 모델 학습 =====\")\n",
    "    y = train_df[col]\n",
    "    \n",
    "    # 개별 모델들 생성\n",
    "    lgbm_model, xgb_model, cat_model = create_ensemble_models(lgbm_params)\n",
    "    \n",
    "    # 1. 기존 LightGBM 최적화\n",
    "    binary_model = LGBMClassifier(objective='binary', **lgbm_params)\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=binary_model,\n",
    "        param_grid=binary_param_grid,\n",
    "        scoring='f1',\n",
    "        cv=cv,\n",
    "        verbose=1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print(f\"{col} LightGBM 최적화 중...\")\n",
    "    grid_search.fit(X_combined, y)\n",
    "    print(f\"LightGBM 최적 파라미터: {grid_search.best_params_}\")\n",
    "    print(f\"LightGBM 최고 점수: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    best_lgbm = grid_search.best_estimator_\n",
    "    binary_preds_combined[col] = best_lgbm.predict(test_X_combined)\n",
    "    \n",
    "    # 2. Voting 앙상블\n",
    "    print(f\"{col} Voting 앙상블 학습 중...\")\n",
    "    voting_ensemble = create_voting_ensemble(best_lgbm, xgb_model, cat_model)\n",
    "    voting_ensemble.fit(X_combined, y)\n",
    "    binary_preds_voting[col] = voting_ensemble.predict(test_X_combined)\n",
    "    \n",
    "    # 검증 성능 비교\n",
    "    X_train_comb, X_val_comb, y_train_bin, y_val_bin = train_test_split(\n",
    "        X_combined, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # 각 모델 성능 평가\n",
    "    models = {\n",
    "        'LightGBM': best_lgbm,\n",
    "        'Voting': voting_ensemble,\n",
    " \n",
    "    }\n",
    "    \n",
    "    print(f\"\\n--- {col} 모델 성능 비교 ---\")\n",
    "    for name, model in models.items():\n",
    "        if name == 'LightGBM':\n",
    "            model_eval = LGBMClassifier(**grid_search.best_params_)\n",
    "        else:\n",
    "            model_eval = model\n",
    "        \n",
    "        model_eval.fit(X_train_comb, y_train_bin)\n",
    "        y_pred = model_eval.predict(X_val_comb)\n",
    "        f1 = f1_score(y_val_bin, y_pred)\n",
    "        print(f\"{name} F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b7e19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ 결합된 특성으로 다중 분류 모델 학습 (앙상블 적용) ------------------\n",
    "print(\"\\n===== S1 다중 분류 모델 학습 =====\")\n",
    "y_multi = train_df['S1']\n",
    "n_classes = len(y_multi.unique())\n",
    "\n",
    "# 개별 모델들 생성\n",
    "lgbm_multi, xgb_multi, cat_multi = create_ensemble_models(lgbm_params, n_classes)\n",
    "\n",
    "# 1. 기존 LightGBM 최적화\n",
    "multiclass_model = LGBMClassifier(objective='multiclass', num_class=n_classes, **lgbm_params)\n",
    "grid_search_multi = GridSearchCV(\n",
    "    estimator=multiclass_model,\n",
    "    param_grid=multi_param_grid,\n",
    "    scoring='f1_macro',\n",
    "    cv=cv,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"LightGBM 다중 분류 최적화 중...\")\n",
    "grid_search_multi.fit(X_combined, y_multi)\n",
    "print(f\"LightGBM 최적 파라미터: {grid_search_multi.best_params_}\")\n",
    "print(f\"LightGBM 최고 점수: {grid_search_multi.best_score_:.4f}\")\n",
    "\n",
    "best_lgbm_multi = grid_search_multi.best_estimator_\n",
    "multiclass_pred_combined = best_lgbm_multi.predict(test_X_combined)\n",
    "\n",
    "# 2. Voting 앙상블 (다중 분류)\n",
    "print(\"Voting 앙상블 다중 분류 학습 중...\")\n",
    "voting_multi = create_voting_ensemble(best_lgbm_multi, xgb_multi, cat_multi)\n",
    "voting_multi.fit(X_combined, y_multi)\n",
    "multiclass_pred_voting = voting_multi.predict(test_X_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011c03b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ 성능 평가 및 비교 ------------------\n",
    "X_train_comb, X_val_comb, y_train_multi, y_val_multi = train_test_split(\n",
    "    X_combined, y_multi, test_size=0.2, random_state=42, stratify=y_multi\n",
    ")\n",
    "\n",
    "# 다중 분류 모델들 성능 비교\n",
    "multi_models = {\n",
    "    'LightGBM': best_lgbm_multi,\n",
    "    'Voting': voting_multi,\n",
    "\n",
    "}\n",
    "\n",
    "print(\"\\n===== 다중 분류 모델 성능 비교 =====\")\n",
    "for name, model in multi_models.items():\n",
    "    if name == 'LightGBM':\n",
    "        model_eval = LGBMClassifier(**grid_search_multi.best_params_)\n",
    "    else:\n",
    "        model_eval = model\n",
    "    \n",
    "    model_eval.fit(X_train_comb, y_train_multi)\n",
    "    y_pred_multi = model_eval.predict(X_val_comb)\n",
    "    \n",
    "    print(f\"\\n--- {name} 성능 ---\")\n",
    "    print(classification_report(y_val_multi, y_pred_multi))\n",
    "    print(\"혼동 행렬:\")\n",
    "    print(confusion_matrix(y_val_multi, y_pred_multi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ce44b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ 성능 평가 및 비교 ------------------\n",
    "X_train_comb, X_val_comb, y_train_multi, y_val_multi = train_test_split(\n",
    "    X_combined, y_multi, test_size=0.2, random_state=42, stratify=y_multi\n",
    ")\n",
    "\n",
    "# 다중 분류 모델들 성능 비교\n",
    "multi_models = {\n",
    "    'LightGBM': best_lgbm_multi,\n",
    "    'Voting': voting_multi,\n",
    "\n",
    "}\n",
    "\n",
    "print(\"\\n===== 다중 분류 모델 성능 비교 =====\")\n",
    "for name, model in multi_models.items():\n",
    "    if name == 'LightGBM':\n",
    "        model_eval = LGBMClassifier(**grid_search_multi.best_params_)\n",
    "    else:\n",
    "        model_eval = model\n",
    "    \n",
    "    model_eval.fit(X_train_comb, y_train_multi)\n",
    "    y_pred_multi = model_eval.predict(X_val_comb)\n",
    "    \n",
    "    print(f\"\\n--- {name} 성능 ---\")\n",
    "    print(classification_report(y_val_multi, y_pred_multi))\n",
    "    print(\"혼동 행렬:\")\n",
    "    print(confusion_matrix(y_val_multi, y_pred_multi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95750df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ------------------ 성능 비교 분석 ------------------\n",
    "\n",
    "# # 원본 특성만 사용한 모델과 결합 특성 모델 비교\n",
    "# print(\"\\n===== 원본 특성 vs 결합 특성 모델 성능 비교 =====\")\n",
    "\n",
    "# # 원본 특성 모델 재학습 (비교용)\n",
    "# X_train_orig, X_val_orig, y_train, y_val = train_test_split(\n",
    "#     X_selected, y_multi, test_size=0.2, random_state=42, stratify=y_multi\n",
    "# )\n",
    "\n",
    "# eval_model_orig = LGBMClassifier(**grid_search_multi.best_params_)\n",
    "# eval_model_orig.fit(X_train_orig, y_train)\n",
    "# y_pred_orig = eval_model_orig.predict(X_val_orig)\n",
    "\n",
    "# from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "# # 정확도 비교\n",
    "# acc_orig = accuracy_score(y_val, y_pred_orig)\n",
    "# acc_comb = accuracy_score(y_val, y_pred_comb)\n",
    "# print(f\"정확도 - 원본 특성: {acc_orig:.4f}, 결합 특성: {acc_comb:.4f}, 향상: {(acc_comb-acc_orig)*100:.2f}%\")\n",
    "\n",
    "# # F1 점수 비교 (다중 분류는 macro 평균)\n",
    "# f1_orig = f1_score(y_val, y_pred_orig, average='macro')\n",
    "# f1_comb = f1_score(y_val, y_pred_comb, average='macro')\n",
    "# print(f\"F1 점수 - 원본 특성: {f1_orig:.4f}, 결합 특성: {f1_comb:.4f}, 향상: {(f1_comb-f1_orig)*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa052704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 검증 데이터에 대한 성능 평가\n",
    "# X_train_sel, X_val_sel, y_train, y_val = train_test_split(\n",
    "#     X_selected, y_multi, test_size=0.2, random_state=42, stratify=y_multi\n",
    "# )\n",
    "\n",
    "# # 최적 파라미터를 사용하여 검증용 모델 학습\n",
    "# eval_model_sel = LGBMClassifier(**grid_search_multi.best_params_)\n",
    "# eval_model_sel.fit(X_train_sel, y_train)\n",
    "\n",
    "# # 예측 및 평가\n",
    "# y_pred_sel = eval_model_sel.predict(X_val_sel)\n",
    "\n",
    "# print(\"\\n===== LightGBM 최적화 모델 평가 (S1) =====\")\n",
    "# print(classification_report(y_val, y_pred_sel))\n",
    "# print(\"\\n혼동 행렬:\")\n",
    "# print(confusion_matrix(y_val, y_pred_sel))\n",
    "\n",
    "# # 성능 평가 (원본 모델과 비교)\n",
    "# # 검증 데이터 분할\n",
    "# X_train_sel, X_val_sel, y_train, y_val = train_test_split(\n",
    "#     X_selected, y_multi, test_size=0.2, random_state=42, stratify=y_multi\n",
    "# )\n",
    "\n",
    "# # 평가용 모델 학습\n",
    "# eval_model_sel = LGBMClassifier(**multiclass_params)\n",
    "# eval_model_sel.fit(X_train_sel, y_train)\n",
    "\n",
    "# # 예측 및 평가\n",
    "# y_pred_sel = eval_model_sel.predict(X_val_sel)\n",
    "\n",
    "# print(\"\\n===== LightGBM 그룹 기반 특성 선택 모델 평가 (S1) =====\")\n",
    "# print(classification_report(y_val, y_pred_sel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be857a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 기반 제출 포맷 가져오기\n",
    "submission_final = sample_submission[['subject_id', 'sleep_date', 'lifelog_date']].copy()\n",
    "\n",
    "# lifelog_date 기준으로 string → date 형식 통일\n",
    "submission_final['lifelog_date'] = pd.to_datetime(submission_final['lifelog_date']).dt.date\n",
    "\n",
    "# ID 만들기 (submission에서 예측한 결과와 연결하기 위해)\n",
    "submission_final['ID'] = submission_final['subject_id'] + '_' + submission_final['lifelog_date'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f142a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 예측 결과 연결할 수 있도록 동일한 순서로 정렬\n",
    "# 보통 예측 결과는 test_df 기준이므로 정렬 보장되어야 함\n",
    "assert len(submission_final) == len(multiclass_pred_voting) # shape 체크\n",
    "\n",
    "# 다중 분류 예측 붙이기\n",
    "submission_final['S1'] = multiclass_pred_voting\n",
    "\n",
    "# 이진 분류 결과 붙이기\n",
    "for col in ['Q1', 'Q2', 'Q3', 'S2', 'S3']:\n",
    "    submission_final[col] = binary_preds_voting[col].astype(int) # 확률 아닌 class 예측\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ac68e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 최종 제출 형식 정렬\n",
    "submission_final = submission_final[['subject_id', 'sleep_date', 'lifelog_date', 'Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']]\n",
    "\n",
    "# 저장\n",
    "submission_final.to_csv(submission_folder + submission_file, index=False)\n",
    "\n",
    "# VSCode에서는 files.download()가 작동하지 않으므로 대체\n",
    "\n",
    "print(f\"✅ 제출 파일이 저장되었습니다: {os.path.abspath(submission_file)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba047669",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
