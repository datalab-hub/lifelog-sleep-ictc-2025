#!/usr/bin/env python
# coding: utf-8

# In[4]:


import numpy as np 
import pandas as pd 
import glob 
import random 
import os 
import matplotlib.pyplot as plt 
import seaborn as sns 
import ast 
import warnings
warnings.filterwarnings('ignore') 
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import f1_score
import torch
from pytorch_tabnet.tab_model import TabNetClassifier
import tensorflow as tf


# In[25]:


# seed ê³ ì • 
SD = 42 
random.seed(SD) 
np.random.seed(SD) 
os.environ['PYTHONHASHSEED'] = str(SD)
tf.random.set_seed(SD)  # TensorFlow ì‹œë“œ ì„¤ì •

# íŒŒì¼ ê²½ë¡œ ì„¤ì • - VSCode ìƒëŒ€ê²½ë¡œë¡œ ë³€ê²½
# ì‹¤ì œ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì • í•„ìš”
base_folder =  '/home/user/torch_ubuntu/src/data/ETRI_lifelog_dataset'
folder = '/ch2025_data_items'

data_dir = base_folder + folder 


# Parquet íŒŒì¼ ì „ì²´ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ 
parquet_files = glob.glob(os.path.join(data_dir, 'ch2025_*.parquet')) 


# In[26]:


# íŒŒì¼ ì´ë¦„ì„ í‚¤ë¡œ, DataFrameì„ ê°’ìœ¼ë¡œ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ 
lifelog_data = {} 

# íŒŒì¼ë³„ë¡œ ì½ê¸° 
for file_path in parquet_files: 
    name = os.path.basename(file_path).replace('.parquet', '').replace('ch2025_', '') 
    lifelog_data[name] = pd.read_parquet(file_path) 
    print(f"âœ… Loaded: {name}, shape = {lifelog_data[name].shape}") 

# ë”•ì…”ë„ˆë¦¬ì— ìˆëŠ” ëª¨ë“  í•­ëª©ì„ ë…ë¦½ì ì¸ ë³€ìˆ˜ë¡œ í• ë‹¹ 
for key, df in lifelog_data.items(): 
    globals()[f"{key}_df"] = df 


# In[27]:


# ë©”íŠ¸ë¦­ìŠ¤ íŒŒì¼ ì½ê¸°
metrics_train = pd.read_csv(base_folder + '/ch2025_metrics_train.csv')
sample_submission = pd.read_csv(base_folder+'/ch2025_submission_sample.csv')

# âœ… ê¸°ì¤€ ìŒ (subject_id, lifelog_date) 
sample_submission['lifelog_date'] = pd.to_datetime(sample_submission['lifelog_date']) 
test_keys = set(zip(sample_submission['subject_id'], sample_submission['lifelog_date'].dt.date)) 

# âœ… DataFrame ë³„ timestamp ì»¬ëŸ¼ ìˆ˜ë™ ì§€ì • 
dataframes = { 
    'mACStatus': (mACStatus_df, 'timestamp'), 
    'mActivity': (mActivity_df, 'timestamp'), 
    'mAmbience': (mAmbience_df, 'timestamp'), 
    'mBle': (mBle_df, 'timestamp'), 
    'mGps': (mGps_df, 'timestamp'), 
    'mLight': (mLight_df, 'timestamp'), 
    'mScreenStatus': (mScreenStatus_df, 'timestamp'), 
    'mUsageStats': (mUsageStats_df, 'timestamp'), 
    'mWifi': (mWifi_df, 'timestamp'), 
    'wHr': (wHr_df, 'timestamp'), 
    'wLight': (wLight_df, 'timestamp'), 
    'wPedo': (wPedo_df, 'timestamp'), 
} 


# In[6]:


# âœ… ë¶„ë¦¬ í•¨ìˆ˜ 
def split_test_train(df, subject_col='subject_id', timestamp_col='timestamp'): 
    df[timestamp_col] = pd.to_datetime(df[timestamp_col], errors='coerce') 
    df = df.dropna(subset=[timestamp_col]) 
    df['date_only'] = df[timestamp_col].dt.date 
    df['key'] = list(zip(df[subject_col], df['date_only'])) 
    test_df = df[df['key'].isin(test_keys)].drop(columns=['date_only', 'key']) 
    train_df = df[~df['key'].isin(test_keys)].drop(columns=['date_only', 'key']) 
    return test_df, train_df 


# In[7]:


# âœ… ê²°ê³¼ ì €ì¥ 
for name, (df, ts_col) in dataframes.items(): 
    print(f"â³ {name} ë¶„ë¦¬ ì¤‘...") 
    test_df, train_df = split_test_train(df.copy(), subject_col='subject_id', timestamp_col=ts_col) 
    globals()[f"{name}_test"] = test_df 
    globals()[f"{name}_train"] = train_df 
    print(f"âœ… {name}_test â†’ {test_df.shape}, {name}_train â†’ {train_df.shape}") 


# In[28]:


def process_mACStatus(df): 
    df['timestamp'] = pd.to_datetime(df['timestamp']) 
    df['date'] = df['timestamp'].dt.date 
    df = df.sort_values(['subject_id', 'timestamp']) 
    results = [] 
    for (subj, date), group in df.groupby(['subject_id', 'date']): 
        status = group['m_charging'].values # 0/1 ìƒíƒœ 
        times = group['timestamp'].values # ì¶©ì „ ìƒíƒœ ë¹„ìœ¨ 
        ratio_charging = status.mean() 
        # ìƒíƒœ ì „ì´ íšŸìˆ˜ 
        transitions = (status[1:] != status[:-1]).sum() 
        # ì—°ì†ëœ 1 ìƒíƒœ ê¸¸ì´ë“¤ 
        lengths = [] 
        current_len = 0 
        for val in status: 
            if val == 1: 
                current_len += 1 
            elif current_len > 0: 
                lengths.append(current_len) 
                current_len = 0 
        if current_len > 0: 
            lengths.append(current_len) 
        avg_charging_duration = np.mean(lengths) if lengths else 0 
        max_charging_duration = np.max(lengths) if lengths else 0 
        results.append({ 
            'subject_id': subj, 
            'date': date, 
            'charging_ratio': ratio_charging, 
            'charging_transitions': transitions, 
            'avg_charging_duration': avg_charging_duration, 
            'max_charging_duration': max_charging_duration, 
        }) 
    return pd.DataFrame(results) 

mACStatus_df2 = process_mACStatus(mACStatus_df) 


# In[29]:


def process_mActivity(df): 
    df['timestamp'] = pd.to_datetime(df['timestamp']) 
    df['date'] = df['timestamp'].dt.date 
    summary = [] 
    for (subj, date), group in df.groupby(['subject_id', 'date']): 
        counts = group['m_activity'].value_counts(normalize=True) # ë¹„ìœ¨ 
        row = {'subject_id': subj, 'date': date} 
        # 0~8 ë¹„ìœ¨ ì €ì¥ 
        for i in range(9): 
            row[f'activity_{i}_ratio'] = counts.get(i, 0) 
        # ì£¼ìš” í™œë™ ì •ë³´ 
        row['dominant_activity'] = group['m_activity'].mode()[0] 
        row['num_unique_activities'] = group['m_activity'].nunique() 
        summary.append(row) 
    return pd.DataFrame(summary) 

mActivity_df2 = process_mActivity(mActivity_df) 


# In[30]:


# ì§€ì •ëœ 10ê°œ ë¼ë²¨ 
top_10_labels = [ 
    "Inside, small room", "Speech", "Silence", "Music", "Narration, monologue", 
    "Child speech, kid speaking", "Conversation", "Speech synthesizer", "Shout", "Babbling" 
] 

def process_mAmbience_top10(df): 
    df = df.copy() 
    df['timestamp'] = pd.to_datetime(df['timestamp']) 
    df['date'] = df['timestamp'].dt.date 
    # ì´ˆê¸°í™” 
    for label in top_10_labels + ['others']: 
        df[label] = 0.0 
    for idx, row in df.iterrows(): 
        parsed = ast.literal_eval(row['m_ambience']) if isinstance(row['m_ambience'], str) else row['m_ambience'] 
        others_prob = 0.0 
        for label, prob in parsed: 
            prob = float(prob) 
            if label in top_10_labels: 
                df.at[idx, label] = prob 
            else: 
                others_prob += prob 
        df.at[idx, 'others'] = others_prob 
    return df.drop(columns=['m_ambience']) 

mAmbience_df2= process_mAmbience_top10(mAmbience_df) 


# In[31]:


def summarize_mAmbience_daily(df): 
    prob_cols = [col for col in df.columns if col not in ['subject_id', 'timestamp', 'date']] 
    # í•˜ë£¨ ë‹¨ìœ„ë¡œ í‰ê· ê°’ ìš”ì•½ 
    daily_summary = df.groupby(['subject_id', 'date'])[prob_cols].mean().reset_index() 
    return daily_summary 

mAmbience_df2 = summarize_mAmbience_daily(mAmbience_df2) 


# In[32]:


def process_mBle(df): 
    df = df.copy() 
    df['timestamp'] = pd.to_datetime(df['timestamp']) 
    df['date'] = df['timestamp'].dt.date 
    features = [] 
    for idx, row in df.iterrows(): 
        entry = ast.literal_eval(row['m_ble']) if isinstance(row['m_ble'], str) else row['m_ble'] 
        rssi_list = [] 
        class_0_cnt = 0 
        class_other_cnt = 0 
        for device in entry: 
            try: 
                rssi = int(device['rssi']) 
                rssi_list.append(rssi) 
                if str(device['device_class']) == '0': 
                    class_0_cnt += 1 
                else: 
                    class_other_cnt += 1 
            except: 
                continue # malformed record 
        feature = { 
            'subject_id': row['subject_id'], 
            'date': row['date'], 
            'device_class_0_cnt': class_0_cnt, 
            'device_class_others_cnt': class_other_cnt, 
            'device_count': len(rssi_list), 
            'rssi_mean': np.mean(rssi_list) if rssi_list else np.nan, 
            'rssi_min': np.min(rssi_list) if rssi_list else np.nan, 
            'rssi_max': np.max(rssi_list) if rssi_list else np.nan, 
        } 
        features.append(feature) 
    return pd.DataFrame(features) 


# In[33]:


def summarize_mBle_daily(df): 
    # row ë‹¨ìœ„ BLE feature ì¶”ì¶œ 
    df = process_mBle(df) 
    # í•˜ë£¨ ë‹¨ìœ„ë¡œ cnt í•©ì¹˜ê¸° 
    grouped = df.groupby(['subject_id', 'date']).agg({ 
        'device_class_0_cnt': 'sum', 
        'device_class_others_cnt': 'sum', 
        'rssi_mean': 'mean', 
        'rssi_min': 'min', 
        'rssi_max': 'max', 
    }).reset_index() 
    # ì´í•© êµ¬í•´ì„œ ë¹„ìœ¨ ê³„ì‚° 
    total_cnt = grouped['device_class_0_cnt'] + grouped['device_class_others_cnt'] 
    grouped['device_class_0_ratio'] = grouped['device_class_0_cnt'] / total_cnt.replace(0, np.nan) 
    grouped['device_class_others_ratio'] = grouped['device_class_others_cnt'] / total_cnt.replace(0, np.nan) 
    # í•„ìš” ì—†ëŠ” ì›ë˜ cnt ì»¬ëŸ¼ ì œê±° 
    grouped.drop(columns=['device_class_0_cnt', 'device_class_others_cnt'], inplace=True) 
    return grouped 

mBle_df2 = summarize_mBle_daily(mBle_df) 


# In[34]:


def process_mGps(df): 
    df = df.copy() 
    df['timestamp'] = pd.to_datetime(df['timestamp']) 
    df['date'] = df['timestamp'].dt.date 
    features = [] 
    for idx, row in df.iterrows(): 
        gps_list = ast.literal_eval(row['m_gps']) if isinstance(row['m_gps'], str) else row['m_gps'] 
        altitudes = [] 
        latitudes = [] 
        longitudes = [] 
        speeds = [] 
        for entry in gps_list: 
            try: 
                altitudes.append(float(entry['altitude'])) 
                latitudes.append(float(entry['latitude'])) 
                longitudes.append(float(entry['longitude'])) 
                speeds.append(float(entry['speed'])) 
            except: 
                continue 
        features.append({ 
            'subject_id': row['subject_id'], 
            'date': row['date'], 
            'altitude_mean': np.mean(altitudes) if altitudes else np.nan, 
            'latitude_std': np.std(latitudes) if latitudes else np.nan, 
            'longitude_std': np.std(longitudes) if longitudes else np.nan, 
            'speed_mean': np.mean(speeds) if speeds else np.nan, 
            'speed_max': np.max(speeds) if speeds else np.nan, 
            'speed_std': np.std(speeds) if speeds else np.nan, 
        }) 
    return pd.DataFrame(features) 


# In[35]:


m_Gps_df2 = process_mGps(mGps_df) 
m_Gps_df2 = m_Gps_df2.groupby(['subject_id', 'date']).agg({ 
    'altitude_mean': 'mean', 
    'latitude_std': 'mean', 
    'longitude_std': 'mean', 
    'speed_mean': 'mean', 
    'speed_max': 'max', 
    'speed_std': 'mean' 
}).reset_index() 


# In[36]:


def process_mLight(df): 
    df = df.copy() 
    df['timestamp'] = pd.to_datetime(df['timestamp']) 
    df['date'] = df['timestamp'].dt.date 
    df['hour'] = df['timestamp'].dt.hour 
    # ë°¤(22~05ì‹œ), ë‚®(06~21ì‹œ) êµ¬ë¶„ 
    df['is_night'] = df['hour'].apply(lambda h: h >= 22 or h < 6) 
    # í•˜ë£¨ ë‹¨ìœ„ ìš”ì•½ 
    daily = df.groupby(['subject_id', 'date']).agg( 
        light_mean=('m_light', 'mean'), 
        light_std=('m_light', 'std'), 
        light_max=('m_light', 'max'), 
        light_min=('m_light', 'min'), 
        light_night_mean=('m_light', lambda x: x[df.loc[x.index, 'is_night']].mean()), 
        light_day_mean=('m_light', lambda x: x[~df.loc[x.index, 'is_night']].mean()), 
        light_night_ratio=('is_night', 'mean') # ë°¤ ì‹œê°„ ì¸¡ì • ë¹„ìœ¨ 
    ).reset_index() 
    return daily 

mLight_df2 = process_mLight(mLight_df) 


# In[37]:


def process_mScreenStatus(df): 
    df = df.copy() 
    df['timestamp'] = pd.to_datetime(df['timestamp']) 
    df['date'] = df['timestamp'].dt.date 
    features = [] 
    for (subj, date), group in df.groupby(['subject_id', 'date']): 
        status = group['m_screen_use'].values 
        ratio_on = status.mean() 
        transitions = (status[1:] != status[:-1]).sum() 
        # ì—°ì†ëœ 1 ìƒíƒœ ê¸¸ì´ë“¤ 
        durations = [] 
        current = 0 
        for val in status: 
            if val == 1: 
                current += 1 
            elif current > 0: 
                durations.append(current) 
                current = 0 
        if current > 0: 
            durations.append(current) 
        features.append({ 
            'subject_id': subj, 
            'date': date, 
            'screen_on_ratio': ratio_on, 
            'screen_on_transitions': transitions, 
            'screen_on_duration_avg': np.mean(durations) if durations else 0, 
            'screen_on_duration_max': np.max(durations) if durations else 0, 
        }) 
    return pd.DataFrame(features) 

mScreenStatus_df2 = process_mScreenStatus(mScreenStatus_df) 


# In[38]:


top_apps = [ 
    'One UI í™ˆ', 'ì¹´ì¹´ì˜¤í†¡', 'ì‹œìŠ¤í…œ UI', 'NAVER', 'ìºì‹œì›Œí¬', 
    'ì„±ê²½ì¼ë…Q', 'YouTube', 'í†µí™”', 'ë©”ì‹œì§€', 'íƒ€ì„ìŠ¤í”„ë ˆë“œ', 'Instagram'
] 

def process_mUsageStats(df): 
    df = df.copy() 
    df['timestamp'] = pd.to_datetime(df['timestamp']) 
    df['date'] = df['timestamp'].dt.date 
    features = [] 
    for (subj, date), group in df.groupby(['subject_id', 'date']): 
        app_time = {app: 0 for app in top_apps} 
        others_time = 0 
        for row in group['m_usage_stats']: 
            parsed = ast.literal_eval(row) if isinstance(row, str) else row 
            for entry in parsed: 
                app = entry.get('app_name') 
                time = entry.get('total_time', 0) 
                if app in top_apps: 
                    app_time[app] += int(time) 
                else: 
                    others_time += int(time) 
        feature = { 
            'subject_id': subj, 
            'date': date, 
            'others_time': others_time 
        } 
        # ê° ì•±ë³„ ì»¬ëŸ¼ ì¶”ê°€ 
        feature.update({f'{app}_time': app_time[app] for app in top_apps}) 
        features.append(feature) 
    return pd.DataFrame(features) 

mUsageStats_df2 = process_mUsageStats(mUsageStats_df) 


# In[39]:


def process_mWifi(df): 
    df = df.copy() 
    df['timestamp'] = pd.to_datetime(df['timestamp']) 
    df['date'] = df['timestamp'].dt.date 
    results = [] 
    for (subj, date), group in df.groupby(['subject_id', 'date']): 
        rssi_all = [] 
        for row in group['m_wifi']: 
            parsed = ast.literal_eval(row) if isinstance(row, str) else row 
            for ap in parsed: 
                try: 
                    rssi = int(ap['rssi']) 
                    rssi_all.append(rssi) 
                except: 
                    continue 
        results.append({ 
            'subject_id': subj, 
            'date': date, 
            'wifi_rssi_mean': np.mean(rssi_all) if rssi_all else np.nan, 
            'wifi_rssi_min': np.min(rssi_all) if rssi_all else np.nan, 
            'wifi_rssi_max': np.max(rssi_all) if rssi_all else np.nan, 
            'wifi_detected_cnt': len(rssi_all) 
        }) 
    return pd.DataFrame(results) 

mWifi_df2 = process_mWifi(mWifi_df) 


# In[40]:


def get_time_block(hour): 
    if 0 <= hour < 6: 
        return 'early_morning' 
    elif 6 <= hour < 12: 
        return 'morning' 
    elif 12 <= hour < 18: 
        return 'afternoon' 
    else: 
        return 'evening' 

def process_wHr_by_timeblock(df): 
    df = df.copy() 
    df['timestamp'] = pd.to_datetime(df['timestamp']) 
    df['date'] = df['timestamp'].dt.date 
    df['block'] = df['timestamp'].dt.hour.map(get_time_block) 
    results = [] 
    for (subj, date), group in df.groupby(['subject_id', 'date']): 
        block_stats = {'subject_id': subj, 'date': date} 
        for block, block_group in group.groupby('block'): 
            hr_all = [] 
            for row in block_group['heart_rate']: 
                parsed = ast.literal_eval(row) if isinstance(row, str) else row 
                hr_all.extend([int(h) for h in parsed if h is not None]) 
            if not hr_all: 
                continue 
            above_100 = [hr for hr in hr_all if hr > 100] 
            block_stats[f'hr_{block}_mean'] = np.mean(hr_all) 
            block_stats[f'hr_{block}_std'] = np.std(hr_all) 
            block_stats[f'hr_{block}_max'] = np.max(hr_all) 
            block_stats[f'hr_{block}_min'] = np.min(hr_all) 
            block_stats[f'hr_{block}_above_100_ratio'] = len(above_100) / len(hr_all) 
        results.append(block_stats) 
    return pd.DataFrame(results) 

wHr_df2 = process_wHr_by_timeblock(wHr_df) 


# In[41]:


def process_wLight_by_timeblock(df): 
    df = df.copy() 
    df['timestamp'] = pd.to_datetime(df['timestamp']) 
    df['date'] = df['timestamp'].dt.date 
    df['block'] = df['timestamp'].dt.hour.map(get_time_block) 
    results = [] 
    for (subj, date), group in df.groupby(['subject_id', 'date']): 
        block_stats = {'subject_id': subj, 'date': date} 
        for block, block_group in group.groupby('block'): 
            lux = block_group['w_light'].dropna().values 
            if len(lux) == 0: 
                continue 
            block_stats[f'wlight_{block}_mean'] = np.mean(lux) 
            block_stats[f'wlight_{block}_std'] = np.std(lux) 
            block_stats[f'wlight_{block}_max'] = np.max(lux) 
            block_stats[f'wlight_{block}_min'] = np.min(lux) 
        results.append(block_stats) 
    return pd.DataFrame(results) 

wLight_df2 = process_wLight_by_timeblock(wLight_df)


# In[42]:


def process_wPedo(df): 
    df = df.copy() 
    df['timestamp'] = pd.to_datetime(df['timestamp']) 
    df['date'] = df['timestamp'].dt.date 
    summary = df.groupby(['subject_id', 'date']).agg({ 
        'step': 'sum', 
        'step_frequency': 'mean', 
        'distance': 'sum', 
        'speed': ['mean', 'max'], 
        'burned_calories': 'sum' 
    }).reset_index() 
    # ì»¬ëŸ¼ ì´ë¦„ ì •ë¦¬ 
    summary.columns = ['subject_id', 'date', 'step_sum', 'step_frequency_mean', 'distance_sum', 'speed_mean', 'speed_max', 'burned_calories_sum'] 
    return summary 

wPedo_df2 = process_wPedo(wPedo_df) 


# In[43]:


from functools import reduce 
df_list = [ 
    mACStatus_df2, 
    mActivity_df2, 
    mAmbience_df2, 
    mBle_df2, 
    m_Gps_df2, 
    mLight_df2, 
    mScreenStatus_df2, 
    mUsageStats_df2, 
    mWifi_df2, 
    wHr_df2, 
    wLight_df2, 
    wPedo_df2 
] 

merged_df = reduce(lambda left, right: pd.merge(left, right, on=['subject_id', 'date'], how='outer'), df_list) 


# In[44]:


# metrics_trainì˜ lifelog_date â†’ datetime.date í˜•ìœ¼ë¡œ ë³€í™˜ 
metrics_train['lifelog_date'] = pd.to_datetime(metrics_train['lifelog_date']).dt.date 

# merged_dfì˜ dateë„ ë³€í™˜ 
merged_df['date'] = pd.to_datetime(merged_df['date']).dt.date 

# 1. date ê¸°ì¤€ ì •ë ¬ì„ ìœ„í•´ metrics_trainì˜ lifelog_date -> dateë¡œ ë§ì¶”ê¸° 
metrics_train_renamed = metrics_train.rename(columns={'lifelog_date': 'date'}) 

# 2. train_df: metrics_trainê³¼ ì¼ì¹˜í•˜ëŠ” (subject_id, date) â†’ ë¼ë²¨ í¬í•¨ 
train_df = pd.merge(metrics_train_renamed, merged_df, on=['subject_id', 'date'], how='inner') 

# 3. test_df: metrics_trainì— ì—†ëŠ” (subject_id, date) 
merged_keys = merged_df[['subject_id', 'date']] 
train_keys = metrics_train_renamed[['subject_id', 'date']] 
test_keys = pd.merge(merged_keys, train_keys, on=['subject_id', 'date'], how='left', indicator=True) 
test_keys = test_keys[test_keys['_merge'] == 'left_only'].drop(columns=['_merge']) 
test_df = pd.merge(test_keys, merged_df, on=['subject_id', 'date'], how='left') 


# In[45]:


# âœ… íƒ€ê²Ÿ ë¦¬ìŠ¤íŠ¸ 
targets_binary = ['Q1', 'Q2', 'Q3', 'S2', 'S3'] 
target_multiclass = 'S1' 

# âœ… feature ì¤€ë¹„ 
X = train_df.drop(columns=['subject_id', 'sleep_date', 'date', 'Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']) 
X.fillna(0, inplace=True) # ê²°ì¸¡ê°’ ì²˜ë¦¬ 

test_X = test_df.drop(columns=['subject_id', 'date']) 
test_X.fillna(0, inplace=True) 


# In[46]:


# ì»¬ëŸ¼ ì´ë¦„ì—ì„œ íŠ¹ìˆ˜ ë¬¸ì ì œê±°/ë³€í™˜ 
def sanitize_column_names(df): 
    df.columns = ( 
        df.columns 
        .str.replace(r"[^\w]", "_", regex=True) # íŠ¹ìˆ˜ë¬¸ì â†’ _ 
        .str.replace(r"__+", "_", regex=True) # ì—°ì†ëœ _ ì œê±° 
        .str.strip("_") # ì•ë’¤ _ ì œê±° 
    ) 
    return df 

# ëª¨ë“  ì…ë ¥ì— ì ìš© 
X = sanitize_column_names(X) 
test_X = sanitize_column_names(test_X) 


# In[56]:


# ê²°ê³¼ ì €ì¥ 
# ê·¸ë£¹ë³„ ì¤‘ìš” íŠ¹ì„± ì„ íƒ
important_features = [
    # ìŠ¤í¬ë¦° ê´€ë ¨ íŠ¹ì„±
    'screen_on_ratio', 'screen_on_duration_avg', 'screen_on_duration_max',

    # ì™€ì´íŒŒì´ ì‹ í˜¸ ê´€ë ¨ íŠ¹ì„±
    'wifi_rssi_max', 'wifi_rssi_mean', 'wifi_detected_cnt',

    # í™œë™ ê´€ë ¨ íŠ¹ì„±
    'activity_3_ratio', 'activity_4_ratio',

    # ì¶©ì „ ê´€ë ¨ íŠ¹ì„±
    'charging_ratio', 'max_charging_duration', 'avg_charging_duration',

    # ì‹ í˜¸ ê´€ë ¨ íŠ¹ì„±
    'rssi_mean',

    # ë¹› ê´€ë ¨ íŠ¹ì„±
    'light_night_mean', 'light_max', 'light_std',

    # ìœ„ì¹˜/ì›€ì§ì„ ê´€ë ¨ íŠ¹ì„±
    'altitude_mean', 'speed_max_x',

    # ì•± ì‚¬ìš© ê´€ë ¨ íŠ¹ì„±
    'ë©”ì‹œì§€_time', 'others_time', 'Narration_monologue'
]

# ì„ íƒëœ íŠ¹ì„±ë§Œìœ¼ë¡œ ë°ì´í„°ì…‹ êµ¬ì„±
X_selected = X[important_features]
test_X_selected = test_X[important_features]


# TabNet ê³µí†µ íŒŒë¼ë¯¸í„° ì„¤ì •
tabnet_params = {
    'optimizer_fn': torch.optim.Adam,
    'optimizer_params': dict(lr=0.02),
    'scheduler_params': {"step_size":10, "gamma":0.9},
    'scheduler_fn': torch.optim.lr_scheduler.StepLR,
    'mask_type': 'entmax',
    'n_d': 64,  # ê²°ì • ë‹¨ê³„ì˜ ì°¨ì›
    'n_a': 64,  # ì£¼ì˜ ë‹¨ê³„ì˜ ì°¨ì›
    'n_steps': 5,  # ê²°ì • ë‹¨ê³„ì˜ ìˆ˜
    'gamma': 1.5,  # íŠ¹ì„± ì„ íƒ ì •ê·œí™” ë§¤ê°œë³€ìˆ˜
    'lambda_sparse': 1e-3,  # í¬ì†Œì„± ì •ê·œí™”
    # 'max_epochs': 100, 
    # 'patience': 10,
    # 'batch_size': 256,
    # 'virtual_batch_size': 128
}




# In[59]:


# TabNet ëª¨ë¸ import
from pytorch_tabnet.tab_model import TabNetClassifier
import torch



# ì„ íƒëœ íŠ¹ì„±ìœ¼ë¡œ ëª¨ë¸ í›ˆë ¨
# ì´ì§„ ë¶„ë¥˜
binary_preds_selected = {}
for col in targets_binary:
    y = train_df[col]

    # TabNetClassifier ì´ˆê¸°í™”
    model = TabNetClassifier(**tabnet_params)

    # ëª¨ë¸ í•™ìŠµ
    model.fit(
        X_selected.values,  # ì²« ë²ˆì§¸ ì¸ì: X_train
        y,                  # ë‘ ë²ˆì§¸ ì¸ì: y_train
        eval_set=[(X_selected.values, y)], # ê²€ì¦ ì„¸íŠ¸ë¥¼ ë³„ë„ë¡œ ë¶„ë¦¬í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤
        eval_metric=['auc'],
        max_epochs=100 , # ì—¬ê¸°ì— max_epochs ì¶”ê°€ ,
        patience=10,     # ì¡°ê¸° ì¢…ë£Œ ì¸ë‚´
        batch_size=256,  # ë°°ì¹˜ í¬ê¸°
        virtual_batch_size=128  # ê°€ìƒ ë°°ì¹˜ í¬ê¸°
    )

    # ì˜ˆì¸¡ (predict_probaë¡œ í™•ë¥  ë°˜í™˜, class 1ì˜ í™•ë¥ ë§Œ ì‚¬ìš©)
    probs = model.predict_proba(test_X_selected.values)
    binary_preds_selected[col] = (probs[:, 1] > 0.5).astype(int)  # 0.5 ì„ê³„ê°’ ì ìš©

    # ë˜ëŠ” ë°”ë¡œ í´ë˜ìŠ¤ ì˜ˆì¸¡
    # binary_preds_selected[col] = model.predict(test_X_selected)

# ë‹¤ì¤‘ ë¶„ë¥˜ (S1)
y_multi = train_df['S1'].values  # DataFrame ì—´ì„ NumPy ë°°ì—´ë¡œ ë³€í™˜

# TabNetClassifier ì´ˆê¸°í™”
model_s1_selected = TabNetClassifier(**tabnet_params)

# ëª¨ë¸ í•™ìŠµ
model_s1_selected.fit(
    X_selected.values,  # X_train: NumPy ë°°ì—´
    y_multi,            # y_train: NumPy ë°°ì—´
    eval_set=[(X_selected.values, y_multi)],  # ëª¨ë“  ë°ì´í„°ë¥¼ NumPy ë°°ì—´ë¡œ
    eval_metric=['accuracy'],
    max_epochs=100,
    patience=10,
    batch_size=256,
    virtual_batch_size=128
)

# ì˜ˆì¸¡ (predict ë©”ì„œë“œì—ë„ NumPy ë°°ì—´ ì „ë‹¬)
multiclass_pred_selected = model_s1_selected.predict(test_X_selected.values)


# In[ ]:


# # ì„ íƒëœ íŠ¹ì„±ìœ¼ë¡œ ëª¨ë¸ í›ˆë ¨
# # ì´ì§„ ë¶„ë¥˜
# binary_preds_selected = {}
# for col in targets_binary:
#     y = train_df[col]
#     model = RandomForestClassifier(**common_params)
#     model.fit(X_selected, y)
#     binary_preds_selected[col] = model.predict(test_X_selected)

# # ë‹¤ì¤‘ ë¶„ë¥˜ (S1)
# y_multi = train_df['S1']
# model_s1_selected = RandomForestClassifier(**common_params)
# model_s1_selected.fit(X_selected, y_multi)
# multiclass_pred_selected = model_s1_selected.predict(test_X_selected)


# In[61]:


# ì„±ëŠ¥ í‰ê°€ (ì›ë³¸ ëª¨ë¸ê³¼ ë¹„êµ)
# ê²€ì¦ ë°ì´í„° ë¶„í• 

# ëª¨ë¸ í‰ê°€ (ê²€ì¦ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

# X_selectedì™€ y_multiê°€ DataFrameì¸ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ NumPy ë°°ì—´ë¡œ ë³€í™˜
X_selected_numpy = X_selected.values if hasattr(X_selected, 'values') else X_selected
y_multi_numpy = y_multi.values if hasattr(y_multi, 'values') else y_multi

X_train_sel, X_val_sel, y_train, y_val = train_test_split(
    X_selected_numpy, y_multi_numpy, test_size=0.2, random_state=42, stratify=y_multi_numpy
)

# í‰ê°€ìš© ëª¨ë¸ í•™ìŠµ
eval_model_sel = TabNetClassifier(
    optimizer_fn=torch.optim.Adam,
    optimizer_params=dict(lr=0.02),
    scheduler_params={"step_size":10, "gamma":0.9},
    scheduler_fn=torch.optim.lr_scheduler.StepLR,
    mask_type='entmax'
)

# ì¶”ê°€ í›ˆë ¨ ë§¤ê°œë³€ìˆ˜ë“¤ì„ fit ë©”ì„œë“œì— ì§ì ‘ ì „ë‹¬
eval_model_sel.fit(
    X_train_sel,  # ì´ë¯¸ NumPy ë°°ì—´
    y_train,      # ì´ë¯¸ NumPy ë°°ì—´
    eval_set=[(X_train_sel, y_train), (X_val_sel, y_val)],  # ê²€ì¦ ì„¸íŠ¸ ì¶”ê°€
    eval_metric=['accuracy'],
    max_epochs=100,
    patience=10,
    batch_size=256,
    virtual_batch_size=128
)

# ì˜ˆì¸¡ ë° í‰ê°€
y_pred_sel = eval_model_sel.predict(X_val_sel)  # ì´ë¯¸ NumPy ë°°ì—´

print("\n===== ê·¸ë£¹ ê¸°ë°˜ íŠ¹ì„± ì„ íƒ ëª¨ë¸ í‰ê°€ (S1) =====")
print(classification_report(y_val, y_pred_sel))


# In[62]:


# ì´ì§„ ë¶„ë¥˜ í•™ìŠµ 
for col in targets_binary: 
    y = train_df[col] 
    model = RandomForestClassifier(**common_params) 
    model.fit(X, y) 
    binary_preds[col] = model.predict(test_X) # ğŸ”¥ í™•ë¥ X, í´ë˜ìŠ¤ ì§ì ‘ ì˜ˆì¸¡ 

# ë‹¤ì¤‘ ë¶„ë¥˜ í•™ìŠµ (S1) 
y_multi = train_df['S1'] 
model_s1 = RandomForestClassifier(**common_params) 
model_s1.fit(X, y_multi) 
multiclass_pred = model_s1.predict(test_X) # ğŸ”¥ í´ë˜ìŠ¤ ì§ì ‘ ì˜ˆì¸¡ 

# importance ì¶œë ¥
feature_importance = pd.DataFrame({ 
    'feature': X.columns, 
    'importance': model_s1.feature_importances_ 
}).sort_values('importance', ascending=False) 

# ì‹œê°í™” 
plt.figure(figsize=(10, 10)) 
sns.barplot(x='importance', y='feature', data=feature_importance) 
plt.title('Feature Importance')
plt.tight_layout()
plt.savefig('feature_importance.png')
plt.show()


# In[ ]:


# sample ê¸°ë°˜ ì œì¶œ í¬ë§· ê°€ì ¸ì˜¤ê¸°
submission_final = sample_submission[['subject_id', 'sleep_date', 'lifelog_date']].copy()

# lifelog_date ê¸°ì¤€ìœ¼ë¡œ string â†’ date í˜•ì‹ í†µì¼
submission_final['lifelog_date'] = pd.to_datetime(submission_final['lifelog_date']).dt.date

# ID ë§Œë“¤ê¸° (submissionì—ì„œ ì˜ˆì¸¡í•œ ê²°ê³¼ì™€ ì—°ê²°í•˜ê¸° ìœ„í•´)
submission_final['ID'] = submission_final['subject_id'] + '_' + submission_final['lifelog_date'].astype(str)


# In[ ]:


# ì˜ˆì¸¡ ê²°ê³¼ ì—°ê²°í•  ìˆ˜ ìˆë„ë¡ ë™ì¼í•œ ìˆœì„œë¡œ ì •ë ¬
# ë³´í†µ ì˜ˆì¸¡ ê²°ê³¼ëŠ” test_df ê¸°ì¤€ì´ë¯€ë¡œ ì •ë ¬ ë³´ì¥ë˜ì–´ì•¼ í•¨
assert len(submission_final) == len(multiclass_pred_selected) # shape ì²´í¬

# ë‹¤ì¤‘ ë¶„ë¥˜ ì˜ˆì¸¡ ë¶™ì´ê¸°
submission_final['S1'] = multiclass_pred_selected

# ì´ì§„ ë¶„ë¥˜ ê²°ê³¼ ë¶™ì´ê¸°
for col in ['Q1', 'Q2', 'Q3', 'S2', 'S3']:
    submission_final[col] = binary_preds_selected[col].astype(int) # í™•ë¥  ì•„ë‹Œ class ì˜ˆì¸¡


# In[ ]:


# ìµœì¢… ì œì¶œ í˜•ì‹ ì •ë ¬
submission_final = submission_final[['subject_id', 'sleep_date', 'lifelog_date', 'Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']]

# ì €ì¥
submission_final.to_csv("submission_final_mod2.csv", index=False)

# VSCodeì—ì„œëŠ” files.download()ê°€ ì‘ë™í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ëŒ€ì²´
print(f"âœ… ì œì¶œ íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {os.path.abspath('submission_final_mod2.csv')}")


# In[ ]:


# # ëª¨ë¸ ì €ì¥ (ì¶”ê°€ ê¸°ëŠ¥)
# import joblib

# # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
# os.makedirs('models', exist_ok=True)

# # ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ ì €ì¥
# for col in targets_binary:
#     joblib.dump(binary_preds[col], f'models/binary_model_{col}.pkl')
#     print(f"âœ… ì €ì¥ ì™„ë£Œ: binary_model_{col}.pkl")

# # ë‹¤ì¤‘ ë¶„ë¥˜ ëª¨ë¸ ì €ì¥
# joblib.dump(model_s1, 'models/multiclass_model_S1.pkl')
# print(f"âœ… ì €ì¥ ì™„ë£Œ: multiclass_model_S1.pkl")


# In[ ]:


# # ê²°ê³¼ ìš”ì•½ - íŠ¹ì„± ì¤‘ìš”ë„ ìƒìœ„ 20ê°œ í‘œì‹œ
# print("\n===== ì£¼ìš” íŠ¹ì„± ì¤‘ìš”ë„ (ìƒìœ„ 20ê°œ) =====")
# print(feature_importance.head(20))

# # ëª¨ë¸ í‰ê°€ (ê²€ì¦ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import classification_report, confusion_matrix

# # ê²€ì¦ìš© ë°ì´í„° ë¶„ë¦¬
# X_train, X_val, y_train, y_val = train_test_split(
#     X, y_multi, test_size=0.2, random_state=42, stratify=y_multi
# )


# In[ ]:


# # í‰ê°€ìš© ëª¨ë¸ í•™ìŠµ
# eval_model = RandomForestClassifier(**common_params)
# eval_model.fit(X_train, y_train)

# # ì˜ˆì¸¡ ë° í‰ê°€
# y_pred = eval_model.predict(X_val)
# print("\n===== ë‹¤ì¤‘ ë¶„ë¥˜ ëª¨ë¸ í‰ê°€ (S1) =====")
# print(classification_report(y_val, y_pred))


# In[ ]:


# # ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ í‰ê°€
# for col in targets_binary:
#     y_binary = train_df[col]
#     X_train, X_val, y_train, y_val = train_test_split(
#         X, y_binary, test_size=0.2, random_state=42, stratify=y_binary
#     )

#     eval_model = RandomForestClassifier(**common_params)
#     eval_model.fit(X_train, y_train)

#     y_pred = eval_model.predict(X_val)
#     print(f"\n===== ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ í‰ê°€ ({col}) =====")
#     print(classification_report(y_val, y_pred))

# print("\nâœ… ëª¨ë“  ê³¼ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")


# In[ ]:




