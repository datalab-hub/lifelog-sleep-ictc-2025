{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1f085c0",
   "metadata": {},
   "source": [
    "### feature ëŒ€ìƒìœ¼ë¡œ LGBM í•™ìŠµ (GPU ver.)\n",
    "(Developed from dacon_etri_base_mod1.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0593c4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# í˜„ì¬ ë‚ ì§œ ë° ì‹œê°„ ê°€ì ¸ì˜¤ê¸°\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"%m%d_%H%M\")  # ì˜ˆ: 0517_1530\n",
    "\n",
    "# í´ë” ê²½ë¡œ\n",
    "submission_folder = '/home/user/torch_ubuntu/lifelog-sleep-ictc-2025/minjeong/submission/'\n",
    "\n",
    "# íŒŒì¼ëª…ì— íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€\n",
    "submission_file = f'submission_final_mod2_1_{timestamp}.csv'\n",
    "\n",
    "\n",
    "# ë°ì´í„° ê²½ë¡œ \n",
    "base_folder =  '/home/user/torch_ubuntu/src/data/ETRI_lifelog_dataset'\n",
    "folder = '/ch2025_data_items'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bead988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-24 13:07:14.066590: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-24 13:07:14.366059: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748059634.477618     793 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748059634.510086     793 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748059634.752842     793 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748059634.752907     793 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748059634.752908     793 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748059634.752909     793 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-24 13:07:14.776873: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import glob \n",
    "import random \n",
    "import os \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import ast \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "903c2113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed ê³ ì • \n",
    "SD = 42 \n",
    "random.seed(SD) \n",
    "np.random.seed(SD) \n",
    "os.environ['PYTHONHASHSEED'] = str(SD)\n",
    "tf.random.set_seed(SD)  # TensorFlow ì‹œë“œ ì„¤ì •\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ \n",
    "\n",
    "data_dir = base_folder + folder \n",
    "\n",
    "\n",
    "# Parquet íŒŒì¼ ì „ì²´ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ \n",
    "parquet_files = glob.glob(os.path.join(data_dir, 'ch2025_*.parquet')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9028dc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded: mBle, shape = (21830, 3)\n",
      "âœ… Loaded: mAmbience, shape = (476577, 3)\n",
      "âœ… Loaded: mACStatus, shape = (939896, 3)\n",
      "âœ… Loaded: wHr, shape = (382918, 3)\n",
      "âœ… Loaded: mWifi, shape = (76336, 3)\n",
      "âœ… Loaded: mUsageStats, shape = (45197, 3)\n",
      "âœ… Loaded: mActivity, shape = (961062, 3)\n",
      "âœ… Loaded: wLight, shape = (633741, 3)\n",
      "âœ… Loaded: mLight, shape = (96258, 3)\n",
      "âœ… Loaded: mScreenStatus, shape = (939653, 3)\n",
      "âœ… Loaded: mGps, shape = (800611, 3)\n",
      "âœ… Loaded: wPedo, shape = (748100, 9)\n"
     ]
    }
   ],
   "source": [
    "# íŒŒì¼ ì´ë¦„ì„ í‚¤ë¡œ, DataFrameì„ ê°’ìœ¼ë¡œ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ \n",
    "lifelog_data = {} \n",
    "\n",
    "# íŒŒì¼ë³„ë¡œ ì½ê¸° \n",
    "for file_path in parquet_files: \n",
    "    name = os.path.basename(file_path).replace('.parquet', '').replace('ch2025_', '') \n",
    "    lifelog_data[name] = pd.read_parquet(file_path) \n",
    "    print(f\"âœ… Loaded: {name}, shape = {lifelog_data[name].shape}\") \n",
    "\n",
    "# ë”•ì…”ë„ˆë¦¬ì— ìˆëŠ” ëª¨ë“  í•­ëª©ì„ ë…ë¦½ì ì¸ ë³€ìˆ˜ë¡œ í• ë‹¹ \n",
    "for key, df in lifelog_data.items(): \n",
    "    globals()[f\"{key}_df\"] = df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cbd9424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”íŠ¸ë¦­ìŠ¤ íŒŒì¼ ì½ê¸°\n",
    "metrics_train = pd.read_csv(base_folder + '/ch2025_metrics_train.csv')\n",
    "sample_submission = pd.read_csv(base_folder+'/ch2025_submission_sample.csv')\n",
    "\n",
    "# âœ… ê¸°ì¤€ ìŒ (subject_id, lifelog_date) \n",
    "sample_submission['lifelog_date'] = pd.to_datetime(sample_submission['lifelog_date']) \n",
    "test_keys = set(zip(sample_submission['subject_id'], sample_submission['lifelog_date'].dt.date)) \n",
    "\n",
    "# âœ… DataFrame ë³„ timestamp ì»¬ëŸ¼ ìˆ˜ë™ ì§€ì • \n",
    "dataframes = { \n",
    "    'mACStatus': (mACStatus_df, 'timestamp'), \n",
    "    'mActivity': (mActivity_df, 'timestamp'), \n",
    "    'mAmbience': (mAmbience_df, 'timestamp'), \n",
    "    'mBle': (mBle_df, 'timestamp'), \n",
    "    'mGps': (mGps_df, 'timestamp'), \n",
    "    'mLight': (mLight_df, 'timestamp'), \n",
    "    'mScreenStatus': (mScreenStatus_df, 'timestamp'), \n",
    "    'mUsageStats': (mUsageStats_df, 'timestamp'), \n",
    "    'mWifi': (mWifi_df, 'timestamp'), \n",
    "    'wHr': (wHr_df, 'timestamp'), \n",
    "    'wLight': (wLight_df, 'timestamp'), \n",
    "    'wPedo': (wPedo_df, 'timestamp'), \n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b77b6fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… ë¶„ë¦¬ í•¨ìˆ˜ \n",
    "def split_test_train(df, subject_col='subject_id', timestamp_col='timestamp'): \n",
    "    df[timestamp_col] = pd.to_datetime(df[timestamp_col], errors='coerce') \n",
    "    df = df.dropna(subset=[timestamp_col]) \n",
    "    df['date_only'] = df[timestamp_col].dt.date \n",
    "    df['key'] = list(zip(df[subject_col], df['date_only'])) \n",
    "    test_df = df[df['key'].isin(test_keys)].drop(columns=['date_only', 'key']) \n",
    "    train_df = df[~df['key'].isin(test_keys)].drop(columns=['date_only', 'key']) \n",
    "    return test_df, train_df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3416230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ mACStatus ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mACStatus_test â†’ (335849, 3), mACStatus_train â†’ (604047, 3)\n",
      "â³ mActivity ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mActivity_test â†’ (343579, 3), mActivity_train â†’ (617483, 3)\n",
      "â³ mAmbience ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mAmbience_test â†’ (170453, 3), mAmbience_train â†’ (306124, 3)\n",
      "â³ mBle ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mBle_test â†’ (8140, 3), mBle_train â†’ (13690, 3)\n",
      "â³ mGps ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mGps_test â†’ (287386, 3), mGps_train â†’ (513225, 3)\n",
      "â³ mLight ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mLight_test â†’ (34439, 3), mLight_train â†’ (61819, 3)\n",
      "â³ mScreenStatus ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mScreenStatus_test â†’ (336160, 3), mScreenStatus_train â†’ (603493, 3)\n",
      "â³ mUsageStats ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mUsageStats_test â†’ (16499, 3), mUsageStats_train â†’ (28698, 3)\n",
      "â³ mWifi ë¶„ë¦¬ ì¤‘...\n",
      "âœ… mWifi_test â†’ (27467, 3), mWifi_train â†’ (48869, 3)\n",
      "â³ wHr ë¶„ë¦¬ ì¤‘...\n",
      "âœ… wHr_test â†’ (143311, 3), wHr_train â†’ (239607, 3)\n",
      "â³ wLight ë¶„ë¦¬ ì¤‘...\n",
      "âœ… wLight_test â†’ (233809, 3), wLight_train â†’ (399932, 3)\n",
      "â³ wPedo ë¶„ë¦¬ ì¤‘...\n",
      "âœ… wPedo_test â†’ (288832, 9), wPedo_train â†’ (459268, 9)\n"
     ]
    }
   ],
   "source": [
    "# âœ… ê²°ê³¼ ì €ì¥ \n",
    "for name, (df, ts_col) in dataframes.items(): \n",
    "    print(f\"â³ {name} ë¶„ë¦¬ ì¤‘...\") \n",
    "    test_df, train_df = split_test_train(df.copy(), subject_col='subject_id', timestamp_col=ts_col) \n",
    "    globals()[f\"{name}_test\"] = test_df \n",
    "    globals()[f\"{name}_train\"] = train_df \n",
    "    print(f\"âœ… {name}_test â†’ {test_df.shape}, {name}_train â†’ {train_df.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7f34d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mACStatus(df): \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    df = df.sort_values(['subject_id', 'timestamp']) \n",
    "    results = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        status = group['m_charging'].values # 0/1 ìƒíƒœ \n",
    "        times = group['timestamp'].values # ì¶©ì „ ìƒíƒœ ë¹„ìœ¨ \n",
    "        ratio_charging = status.mean() \n",
    "        # ìƒíƒœ ì „ì´ íšŸìˆ˜ \n",
    "        transitions = (status[1:] != status[:-1]).sum() \n",
    "        # ì—°ì†ëœ 1 ìƒíƒœ ê¸¸ì´ë“¤ \n",
    "        lengths = [] \n",
    "        current_len = 0 \n",
    "        for val in status: \n",
    "            if val == 1: \n",
    "                current_len += 1 \n",
    "            elif current_len > 0: \n",
    "                lengths.append(current_len) \n",
    "                current_len = 0 \n",
    "        if current_len > 0: \n",
    "            lengths.append(current_len) \n",
    "        avg_charging_duration = np.mean(lengths) if lengths else 0 \n",
    "        max_charging_duration = np.max(lengths) if lengths else 0 \n",
    "        results.append({ \n",
    "            'subject_id': subj, \n",
    "            'date': date, \n",
    "            'charging_ratio': ratio_charging, \n",
    "            'charging_transitions': transitions, \n",
    "            'avg_charging_duration': avg_charging_duration, \n",
    "            'max_charging_duration': max_charging_duration, \n",
    "        }) \n",
    "    return pd.DataFrame(results) \n",
    "\n",
    "mACStatus_df2 = process_mACStatus(mACStatus_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7c3c0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mActivity(df): \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    summary = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        counts = group['m_activity'].value_counts(normalize=True) # ë¹„ìœ¨ \n",
    "        row = {'subject_id': subj, 'date': date} \n",
    "        # 0~8 ë¹„ìœ¨ ì €ì¥ \n",
    "        for i in range(9): \n",
    "            row[f'activity_{i}_ratio'] = counts.get(i, 0) \n",
    "        # ì£¼ìš” í™œë™ ì •ë³´ \n",
    "        row['dominant_activity'] = group['m_activity'].mode()[0] \n",
    "        row['num_unique_activities'] = group['m_activity'].nunique() \n",
    "        summary.append(row) \n",
    "    return pd.DataFrame(summary) \n",
    "\n",
    "mActivity_df2 = process_mActivity(mActivity_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ddb753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì§€ì •ëœ 10ê°œ ë¼ë²¨ \n",
    "top_10_labels = [ \n",
    "    \"Inside, small room\", \"Speech\", \"Silence\", \"Music\", \"Narration, monologue\", \n",
    "    \"Child speech, kid speaking\", \"Conversation\", \"Speech synthesizer\", \"Shout\", \"Babbling\" \n",
    "] \n",
    "\n",
    "def process_mAmbience_top10(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    # ì´ˆê¸°í™” \n",
    "    for label in top_10_labels + ['others']: \n",
    "        df[label] = 0.0 \n",
    "    for idx, row in df.iterrows(): \n",
    "        parsed = ast.literal_eval(row['m_ambience']) if isinstance(row['m_ambience'], str) else row['m_ambience'] \n",
    "        others_prob = 0.0 \n",
    "        for label, prob in parsed: \n",
    "            prob = float(prob) \n",
    "            if label in top_10_labels: \n",
    "                df.at[idx, label] = prob \n",
    "            else: \n",
    "                others_prob += prob \n",
    "        df.at[idx, 'others'] = others_prob \n",
    "    return df.drop(columns=['m_ambience']) \n",
    "\n",
    "mAmbience_df2= process_mAmbience_top10(mAmbience_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7cc98b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_mAmbience_daily(df): \n",
    "    prob_cols = [col for col in df.columns if col not in ['subject_id', 'timestamp', 'date']] \n",
    "    # í•˜ë£¨ ë‹¨ìœ„ë¡œ í‰ê· ê°’ ìš”ì•½ \n",
    "    daily_summary = df.groupby(['subject_id', 'date'])[prob_cols].mean().reset_index() \n",
    "    return daily_summary \n",
    "\n",
    "mAmbience_df2 = summarize_mAmbience_daily(mAmbience_df2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "763e541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mBle(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    features = [] \n",
    "    for idx, row in df.iterrows(): \n",
    "        entry = ast.literal_eval(row['m_ble']) if isinstance(row['m_ble'], str) else row['m_ble'] \n",
    "        rssi_list = [] \n",
    "        class_0_cnt = 0 \n",
    "        class_other_cnt = 0 \n",
    "        for device in entry: \n",
    "            try: \n",
    "                rssi = int(device['rssi']) \n",
    "                rssi_list.append(rssi) \n",
    "                if str(device['device_class']) == '0': \n",
    "                    class_0_cnt += 1 \n",
    "                else: \n",
    "                    class_other_cnt += 1 \n",
    "            except: \n",
    "                continue # malformed record \n",
    "        feature = { \n",
    "            'subject_id': row['subject_id'], \n",
    "            'date': row['date'], \n",
    "            'device_class_0_cnt': class_0_cnt, \n",
    "            'device_class_others_cnt': class_other_cnt, \n",
    "            'device_count': len(rssi_list), \n",
    "            'rssi_mean': np.mean(rssi_list) if rssi_list else np.nan, \n",
    "            'rssi_min': np.min(rssi_list) if rssi_list else np.nan, \n",
    "            'rssi_max': np.max(rssi_list) if rssi_list else np.nan, \n",
    "        } \n",
    "        features.append(feature) \n",
    "    return pd.DataFrame(features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e38ac08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_mBle_daily(df): \n",
    "    # row ë‹¨ìœ„ BLE feature ì¶”ì¶œ \n",
    "    df = process_mBle(df) \n",
    "    # í•˜ë£¨ ë‹¨ìœ„ë¡œ cnt í•©ì¹˜ê¸° \n",
    "    grouped = df.groupby(['subject_id', 'date']).agg({ \n",
    "        'device_class_0_cnt': 'sum', \n",
    "        'device_class_others_cnt': 'sum', \n",
    "        'rssi_mean': 'mean', \n",
    "        'rssi_min': 'min', \n",
    "        'rssi_max': 'max', \n",
    "    }).reset_index() \n",
    "    # ì´í•© êµ¬í•´ì„œ ë¹„ìœ¨ ê³„ì‚° \n",
    "    total_cnt = grouped['device_class_0_cnt'] + grouped['device_class_others_cnt'] \n",
    "    grouped['device_class_0_ratio'] = grouped['device_class_0_cnt'] / total_cnt.replace(0, np.nan) \n",
    "    grouped['device_class_others_ratio'] = grouped['device_class_others_cnt'] / total_cnt.replace(0, np.nan) \n",
    "    # í•„ìš” ì—†ëŠ” ì›ë˜ cnt ì»¬ëŸ¼ ì œê±° \n",
    "    grouped.drop(columns=['device_class_0_cnt', 'device_class_others_cnt'], inplace=True) \n",
    "    return grouped \n",
    "\n",
    "mBle_df2 = summarize_mBle_daily(mBle_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "618d366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mGps(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    features = [] \n",
    "    for idx, row in df.iterrows(): \n",
    "        gps_list = ast.literal_eval(row['m_gps']) if isinstance(row['m_gps'], str) else row['m_gps'] \n",
    "        altitudes = [] \n",
    "        latitudes = [] \n",
    "        longitudes = [] \n",
    "        speeds = [] \n",
    "        for entry in gps_list: \n",
    "            try: \n",
    "                altitudes.append(float(entry['altitude'])) \n",
    "                latitudes.append(float(entry['latitude'])) \n",
    "                longitudes.append(float(entry['longitude'])) \n",
    "                speeds.append(float(entry['speed'])) \n",
    "            except: \n",
    "                continue \n",
    "        features.append({ \n",
    "            'subject_id': row['subject_id'], \n",
    "            'date': row['date'], \n",
    "            'altitude_mean': np.mean(altitudes) if altitudes else np.nan, \n",
    "            'latitude_std': np.std(latitudes) if latitudes else np.nan, \n",
    "            'longitude_std': np.std(longitudes) if longitudes else np.nan, \n",
    "            'speed_mean': np.mean(speeds) if speeds else np.nan, \n",
    "            'speed_max': np.max(speeds) if speeds else np.nan, \n",
    "            'speed_std': np.std(speeds) if speeds else np.nan, \n",
    "        }) \n",
    "    return pd.DataFrame(features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14a0be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_Gps_df2 = process_mGps(mGps_df) \n",
    "m_Gps_df2 = m_Gps_df2.groupby(['subject_id', 'date']).agg({ \n",
    "    'altitude_mean': 'mean', \n",
    "    'latitude_std': 'mean', \n",
    "    'longitude_std': 'mean', \n",
    "    'speed_mean': 'mean', \n",
    "    'speed_max': 'max', \n",
    "    'speed_std': 'mean' \n",
    "}).reset_index() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8389d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mLight(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    df['hour'] = df['timestamp'].dt.hour \n",
    "    # ë°¤(22~05ì‹œ), ë‚®(06~21ì‹œ) êµ¬ë¶„ \n",
    "    df['is_night'] = df['hour'].apply(lambda h: h >= 22 or h < 6) \n",
    "    # í•˜ë£¨ ë‹¨ìœ„ ìš”ì•½ \n",
    "    daily = df.groupby(['subject_id', 'date']).agg( \n",
    "        light_mean=('m_light', 'mean'), \n",
    "        light_std=('m_light', 'std'), \n",
    "        light_max=('m_light', 'max'), \n",
    "        light_min=('m_light', 'min'), \n",
    "        light_night_mean=('m_light', lambda x: x[df.loc[x.index, 'is_night']].mean()), \n",
    "        light_day_mean=('m_light', lambda x: x[~df.loc[x.index, 'is_night']].mean()), \n",
    "        light_night_ratio=('is_night', 'mean') # ë°¤ ì‹œê°„ ì¸¡ì • ë¹„ìœ¨ \n",
    "    ).reset_index() \n",
    "    return daily \n",
    "\n",
    "mLight_df2 = process_mLight(mLight_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16ad526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mScreenStatus(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    features = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        status = group['m_screen_use'].values \n",
    "        ratio_on = status.mean() \n",
    "        transitions = (status[1:] != status[:-1]).sum() \n",
    "        # ì—°ì†ëœ 1 ìƒíƒœ ê¸¸ì´ë“¤ \n",
    "        durations = [] \n",
    "        current = 0 \n",
    "        for val in status: \n",
    "            if val == 1: \n",
    "                current += 1 \n",
    "            elif current > 0: \n",
    "                durations.append(current) \n",
    "                current = 0 \n",
    "        if current > 0: \n",
    "            durations.append(current) \n",
    "        features.append({ \n",
    "            'subject_id': subj, \n",
    "            'date': date, \n",
    "            'screen_on_ratio': ratio_on, \n",
    "            'screen_on_transitions': transitions, \n",
    "            'screen_on_duration_avg': np.mean(durations) if durations else 0, \n",
    "            'screen_on_duration_max': np.max(durations) if durations else 0, \n",
    "        }) \n",
    "    return pd.DataFrame(features) \n",
    "\n",
    "mScreenStatus_df2 = process_mScreenStatus(mScreenStatus_df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f04edbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_apps = [ \n",
    "    'One UI í™ˆ', 'ì¹´ì¹´ì˜¤í†¡', 'ì‹œìŠ¤í…œ UI', 'NAVER', 'ìºì‹œì›Œí¬', \n",
    "    'ì„±ê²½ì¼ë…Q', 'YouTube', 'í†µí™”', 'ë©”ì‹œì§€', 'íƒ€ì„ìŠ¤í”„ë ˆë“œ', 'Instagram'\n",
    "] \n",
    "\n",
    "def process_mUsageStats(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    features = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        app_time = {app: 0 for app in top_apps} \n",
    "        others_time = 0 \n",
    "        for row in group['m_usage_stats']: \n",
    "            parsed = ast.literal_eval(row) if isinstance(row, str) else row \n",
    "            for entry in parsed: \n",
    "                app = entry.get('app_name') \n",
    "                time = entry.get('total_time', 0) \n",
    "                if app in top_apps: \n",
    "                    app_time[app] += int(time) \n",
    "                else: \n",
    "                    others_time += int(time) \n",
    "        feature = { \n",
    "            'subject_id': subj, \n",
    "            'date': date, \n",
    "            'others_time': others_time \n",
    "        } \n",
    "        # ê° ì•±ë³„ ì»¬ëŸ¼ ì¶”ê°€ \n",
    "        feature.update({f'{app}_time': app_time[app] for app in top_apps}) \n",
    "        features.append(feature) \n",
    "    return pd.DataFrame(features) \n",
    "\n",
    "mUsageStats_df2 = process_mUsageStats(mUsageStats_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73bcd6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_mWifi(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    results = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        rssi_all = [] \n",
    "        for row in group['m_wifi']: \n",
    "            parsed = ast.literal_eval(row) if isinstance(row, str) else row \n",
    "            for ap in parsed: \n",
    "                try: \n",
    "                    rssi = int(ap['rssi']) \n",
    "                    rssi_all.append(rssi) \n",
    "                except: \n",
    "                    continue \n",
    "        results.append({ \n",
    "            'subject_id': subj, \n",
    "            'date': date, \n",
    "            'wifi_rssi_mean': np.mean(rssi_all) if rssi_all else np.nan, \n",
    "            'wifi_rssi_min': np.min(rssi_all) if rssi_all else np.nan, \n",
    "            'wifi_rssi_max': np.max(rssi_all) if rssi_all else np.nan, \n",
    "            'wifi_detected_cnt': len(rssi_all) \n",
    "        }) \n",
    "    return pd.DataFrame(results) \n",
    "\n",
    "mWifi_df2 = process_mWifi(mWifi_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83782374",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_time_block(hour): \n",
    "    if 0 <= hour < 6: \n",
    "        return 'early_morning' \n",
    "    elif 6 <= hour < 12: \n",
    "        return 'morning' \n",
    "    elif 12 <= hour < 18: \n",
    "        return 'afternoon' \n",
    "    else: \n",
    "        return 'evening' \n",
    "\n",
    "def process_wHr_by_timeblock(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    df['block'] = df['timestamp'].dt.hour.map(get_time_block) \n",
    "    results = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        block_stats = {'subject_id': subj, 'date': date} \n",
    "        for block, block_group in group.groupby('block'): \n",
    "            hr_all = [] \n",
    "            for row in block_group['heart_rate']: \n",
    "                parsed = ast.literal_eval(row) if isinstance(row, str) else row \n",
    "                hr_all.extend([int(h) for h in parsed if h is not None]) \n",
    "            if not hr_all: \n",
    "                continue \n",
    "            above_100 = [hr for hr in hr_all if hr > 100] \n",
    "            block_stats[f'hr_{block}_mean'] = np.mean(hr_all) \n",
    "            block_stats[f'hr_{block}_std'] = np.std(hr_all) \n",
    "            block_stats[f'hr_{block}_max'] = np.max(hr_all) \n",
    "            block_stats[f'hr_{block}_min'] = np.min(hr_all) \n",
    "            block_stats[f'hr_{block}_above_100_ratio'] = len(above_100) / len(hr_all) \n",
    "        results.append(block_stats) \n",
    "    return pd.DataFrame(results) \n",
    "\n",
    "wHr_df2 = process_wHr_by_timeblock(wHr_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f39b5000",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_wLight_by_timeblock(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    df['block'] = df['timestamp'].dt.hour.map(get_time_block) \n",
    "    results = [] \n",
    "    for (subj, date), group in df.groupby(['subject_id', 'date']): \n",
    "        block_stats = {'subject_id': subj, 'date': date} \n",
    "        for block, block_group in group.groupby('block'): \n",
    "            lux = block_group['w_light'].dropna().values \n",
    "            if len(lux) == 0: \n",
    "                continue \n",
    "            block_stats[f'wlight_{block}_mean'] = np.mean(lux) \n",
    "            block_stats[f'wlight_{block}_std'] = np.std(lux) \n",
    "            block_stats[f'wlight_{block}_max'] = np.max(lux) \n",
    "            block_stats[f'wlight_{block}_min'] = np.min(lux) \n",
    "        results.append(block_stats) \n",
    "    return pd.DataFrame(results) \n",
    "\n",
    "wLight_df2 = process_wLight_by_timeblock(wLight_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a55ce49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_wPedo(df): \n",
    "    df = df.copy() \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "    df['date'] = df['timestamp'].dt.date \n",
    "    summary = df.groupby(['subject_id', 'date']).agg({ \n",
    "        'step': 'sum', \n",
    "        'step_frequency': 'mean', \n",
    "        'distance': 'sum', \n",
    "        'speed': ['mean', 'max'], \n",
    "        'burned_calories': 'sum' \n",
    "    }).reset_index() \n",
    "    # ì»¬ëŸ¼ ì´ë¦„ ì •ë¦¬ \n",
    "    summary.columns = ['subject_id', 'date', 'step_sum', 'step_frequency_mean', 'distance_sum', 'speed_mean', 'speed_max', 'burned_calories_sum'] \n",
    "    return summary \n",
    "\n",
    "wPedo_df2 = process_wPedo(wPedo_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50c9a3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from functools import reduce \n",
    "df_list = [ \n",
    "    mACStatus_df2, \n",
    "    mActivity_df2, \n",
    "    mAmbience_df2, \n",
    "    mBle_df2, \n",
    "    m_Gps_df2, \n",
    "    mLight_df2, \n",
    "    mScreenStatus_df2, \n",
    "    mUsageStats_df2, \n",
    "    mWifi_df2, \n",
    "    wHr_df2, \n",
    "    wLight_df2, \n",
    "    wPedo_df2 \n",
    "] \n",
    "\n",
    "merged_df = reduce(lambda left, right: pd.merge(left, right, on=['subject_id', 'date'], how='outer'), df_list) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5335404",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# metrics_trainì˜ lifelog_date â†’ datetime.date í˜•ìœ¼ë¡œ ë³€í™˜ \n",
    "metrics_train['lifelog_date'] = pd.to_datetime(metrics_train['lifelog_date']).dt.date \n",
    "\n",
    "# merged_dfì˜ dateë„ ë³€í™˜ \n",
    "merged_df['date'] = pd.to_datetime(merged_df['date']).dt.date \n",
    "\n",
    "# 1. date ê¸°ì¤€ ì •ë ¬ì„ ìœ„í•´ metrics_trainì˜ lifelog_date -> dateë¡œ ë§ì¶”ê¸° \n",
    "metrics_train_renamed = metrics_train.rename(columns={'lifelog_date': 'date'}) \n",
    "\n",
    "# 2. train_df: metrics_trainê³¼ ì¼ì¹˜í•˜ëŠ” (subject_id, date) â†’ ë¼ë²¨ í¬í•¨ \n",
    "train_df = pd.merge(metrics_train_renamed, merged_df, on=['subject_id', 'date'], how='inner') \n",
    "\n",
    "# 3. test_df: metrics_trainì— ì—†ëŠ” (subject_id, date) \n",
    "merged_keys = merged_df[['subject_id', 'date']] \n",
    "train_keys = metrics_train_renamed[['subject_id', 'date']] \n",
    "test_keys = pd.merge(merged_keys, train_keys, on=['subject_id', 'date'], how='left', indicator=True) \n",
    "test_keys = test_keys[test_keys['_merge'] == 'left_only'].drop(columns=['_merge']) \n",
    "test_df = pd.merge(test_keys, merged_df, on=['subject_id', 'date'], how='left') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08c98b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# âœ… íƒ€ê²Ÿ ë¦¬ìŠ¤íŠ¸ \n",
    "targets_binary = ['Q1', 'Q2', 'Q3', 'S2', 'S3'] \n",
    "target_multiclass = 'S1' \n",
    "\n",
    "# âœ… feature ì¤€ë¹„ \n",
    "X = train_df.drop(columns=['subject_id', 'sleep_date', 'date', 'Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']) \n",
    "X.fillna(0, inplace=True) # ê²°ì¸¡ê°’ ì²˜ë¦¬ \n",
    "\n",
    "test_X = test_df.drop(columns=['subject_id', 'date']) \n",
    "test_X.fillna(0, inplace=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12d497e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ì»¬ëŸ¼ ì´ë¦„ì—ì„œ íŠ¹ìˆ˜ ë¬¸ì ì œê±°/ë³€í™˜ \n",
    "def sanitize_column_names(df): \n",
    "    df.columns = ( \n",
    "        df.columns \n",
    "        .str.replace(r\"[^\\w]\", \"_\", regex=True) # íŠ¹ìˆ˜ë¬¸ì â†’ _ \n",
    "        .str.replace(r\"__+\", \"_\", regex=True) # ì—°ì†ëœ _ ì œê±° \n",
    "        .str.strip(\"_\") # ì•ë’¤ _ ì œê±° \n",
    "    ) \n",
    "    return df \n",
    "\n",
    "# ëª¨ë“  ì…ë ¥ì— ì ìš© \n",
    "X = sanitize_column_names(X) \n",
    "test_X = sanitize_column_names(test_X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b3ca789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ ì €ì¥ \n",
    "# ê·¸ë£¹ë³„ ì¤‘ìš” íŠ¹ì„± ì„ íƒ\n",
    "important_features = [\n",
    "    # ìŠ¤í¬ë¦° ê´€ë ¨ íŠ¹ì„±\n",
    "    'screen_on_ratio', 'screen_on_duration_avg', 'screen_on_duration_max',\n",
    "    \n",
    "    # ì™€ì´íŒŒì´ ì‹ í˜¸ ê´€ë ¨ íŠ¹ì„±\n",
    "    'wifi_rssi_max', 'wifi_rssi_mean', 'wifi_detected_cnt',\n",
    "    \n",
    "    # í™œë™ ê´€ë ¨ íŠ¹ì„±\n",
    "    'activity_3_ratio', 'activity_4_ratio',\n",
    "    \n",
    "    # ì¶©ì „ ê´€ë ¨ íŠ¹ì„±\n",
    "    'charging_ratio', 'max_charging_duration', 'avg_charging_duration',\n",
    "    \n",
    "    # ì‹ í˜¸ ê´€ë ¨ íŠ¹ì„±\n",
    "    'rssi_mean',\n",
    "    \n",
    "    # ë¹› ê´€ë ¨ íŠ¹ì„±\n",
    "    'light_night_mean', 'light_max', 'light_std',\n",
    "    \n",
    "    # ìœ„ì¹˜/ì›€ì§ì„ ê´€ë ¨ íŠ¹ì„±\n",
    "    'altitude_mean', 'speed_max_x',\n",
    "    \n",
    "    # ì•± ì‚¬ìš© ê´€ë ¨ íŠ¹ì„±\n",
    "    'ë©”ì‹œì§€_time', 'others_time', 'Narration_monologue'\n",
    "]\n",
    "\n",
    "# ì„ íƒëœ íŠ¹ì„±ë§Œìœ¼ë¡œ ë°ì´í„°ì…‹ êµ¬ì„±\n",
    "X_selected = X[important_features]\n",
    "test_X_selected = test_X[important_features]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12926f55",
   "metadata": {},
   "source": [
    "### ëª¨ë¸ë§ ì‹œì‘ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d1d732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCL ë¹Œë“œ >>  /home/user/torch_ubuntu/lib/python3.12/site-packages/lightgbm/__init__.py\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'lightgbm' has no attribute '_safe_eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightgbm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlgb\u001b[39;00m \u001b[38;5;66;03m# LightGBMì€ ê¸°ë³¸ì ìœ¼ë¡œ GPU ì§€ì›ì´ ë¹„í™œì„±í™”ëœ ì±„ë¡œ ë°°í¬ë¨\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOpenCL ë¹Œë“œ >> \u001b[39m\u001b[33m\"\u001b[39m, lgb.\u001b[34m__file__\u001b[39m) \u001b[38;5;66;03m# GPUë¥¼ ì“°ë ¤ë©´ ì†ŒìŠ¤ì—ì„œ OpenCL ì§€ì›ì„ ì¼œê³  ë¹Œë“œí•´ì•¼ í•¨\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mlgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_safe_eval\u001b[49m)  \u001b[38;5;66;03m# ì˜¤ë¥˜ ì•ˆ ë‚˜ì•¼ ì •ìƒ ì„¤ì¹˜\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# GridSearchë¥¼ ìœ„í•œ êµì°¨ ê²€ì¦ ì„¤ì •\u001b[39;00m\n\u001b[32m     12\u001b[39m cv = StratifiedKFold(n_splits=\u001b[32m3\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, random_state=\u001b[32m42\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'lightgbm' has no attribute '_safe_eval'"
     ]
    }
   ],
   "source": [
    "# LightGBM ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import lightgbm as lgb # LightGBMì€ ê¸°ë³¸ì ìœ¼ë¡œ GPU ì§€ì›ì´ ë¹„í™œì„±í™”ëœ ì±„ë¡œ ë°°í¬ë¨\n",
    "print(\"OpenCL ë¹Œë“œ >> \", lgb.__file__) # GPUë¥¼ ì“°ë ¤ë©´ ì†ŒìŠ¤ì—ì„œ OpenCL ì§€ì›ì„ ì¼œê³  ë¹Œë“œí•´ì•¼ í•¨\n",
    "# print(lgb._safe_eval)  # ì˜¤ë¥˜ ì•ˆ ë‚˜ì•¼ ì •ìƒ ì„¤ì¹˜\n",
    "\n",
    "\n",
    "# GridSearchë¥¼ ìœ„í•œ êµì°¨ ê²€ì¦ ì„¤ì •\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# LightGBM ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì„¤ì • (+ GPU ì„¤ì • ğŸ’¥ )\n",
    "lgbm_params = { \n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate': 0.03,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': -1,\n",
    "    'device': 'cuda', # 'gpu',\n",
    "\n",
    "}\n",
    "\n",
    "# ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ê·¸ë¦¬ë“œ ì„œì¹˜ íŒŒë¼ë¯¸í„°\n",
    "binary_param_grid = {\n",
    "    'learning_rate': [0.01, 0.03],\n",
    "    'n_estimators': [500, 1000],\n",
    "    'num_leaves': [50, 100],\n",
    "    'max_depth': [-1, 5],\n",
    "    'min_child_samples': [10, 30],\n",
    "    'reg_alpha': [0, 0.01, 0.1], # L1 ê·œì œ íŒŒë¼ë¯¸í„° \n",
    "    'reg_lambda': [0, 0.01, 0.1], # L2 ê·œì œ íŒŒë¼ë¯¸í„° \n",
    "}\n",
    "\n",
    "# ë‹¤ì¤‘ ë¶„ë¥˜ ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ê·¸ë¦¬ë“œ ì„œì¹˜ íŒŒë¼ë¯¸í„°\n",
    "multi_param_grid = {\n",
    "    'learning_rate': [0.01, 0.03],\n",
    "    'n_estimators': [500, 1000],\n",
    "    'num_leaves': [50, 100],\n",
    "    'max_depth': [-1, 5],\n",
    "    'min_child_samples': [10, 30],\n",
    "    'reg_alpha': [0, 0.01, 0.1], # L1 ê·œì œ íŒŒë¼ë¯¸í„° \n",
    "    'reg_lambda': [0, 0.01, 0.1], # L2 ê·œì œ íŒŒë¼ë¯¸í„° \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eb5fdf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q1 ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ ìµœì í™” ì¤‘...\n",
      "Fitting 3 folds for each of 288 candidates, totalling 864 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 864 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n864 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/user/torch_ubuntu/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/user/torch_ubuntu/lib/python3.12/site-packages/lightgbm/sklearn.py\", line 1560, in fit\n    super().fit(\n  File \"/home/user/torch_ubuntu/lib/python3.12/site-packages/lightgbm/sklearn.py\", line 1049, in fit\n    self._Booster = train(\n                    ^^^^^^\n  File \"/home/user/torch_ubuntu/lib/python3.12/site-packages/lightgbm/engine.py\", line 297, in train\n    booster = Booster(params=params, train_set=train_set)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/torch_ubuntu/lib/python3.12/site-packages/lightgbm/basic.py\", line 3660, in __init__\n    _safe_call(\n  File \"/home/user/torch_ubuntu/lib/python3.12/site-packages/lightgbm/basic.py\", line 313, in _safe_call\n    raise LightGBMError(_LIB.LGBM_GetLastError().decode(\"utf-8\"))\nlightgbm.basic.LightGBMError: No OpenCL device found\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     10\u001b[39m grid_search = GridSearchCV(\n\u001b[32m     11\u001b[39m     estimator=binary_model,\n\u001b[32m     12\u001b[39m     param_grid=binary_param_grid,\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     n_jobs=-\u001b[32m1\u001b[39m\n\u001b[32m     17\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ ìµœì í™” ì¤‘...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_selected\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mìµœì  íŒŒë¼ë¯¸í„°: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid_search.best_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mìµœê³  ì ìˆ˜: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid_search.best_score_\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch_ubuntu/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch_ubuntu/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1024\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1018\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1019\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1020\u001b[39m     )\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1027\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1028\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch_ubuntu/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1571\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1569\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1570\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1571\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch_ubuntu/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1001\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    994\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) != n_candidates * n_splits:\n\u001b[32m    995\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    996\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcv.split and cv.get_n_splits returned \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    997\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    998\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(n_splits, \u001b[38;5;28mlen\u001b[39m(out) // n_candidates)\n\u001b[32m    999\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[32m   1004\u001b[39m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[32m   1005\u001b[39m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[32m   1006\u001b[39m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.scoring):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/torch_ubuntu/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:517\u001b[39m, in \u001b[36m_warn_or_raise_about_fit_failures\u001b[39m\u001b[34m(results, error_score)\u001b[39m\n\u001b[32m    510\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits == num_fits:\n\u001b[32m    511\u001b[39m     all_fits_failed_message = (\n\u001b[32m    512\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    513\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    514\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou can try to debug the error by setting error_score=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    516\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[32m    519\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    520\u001b[39m     some_fits_failed_message = (\n\u001b[32m    521\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    522\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe score on these train-test partitions for these parameters\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    526\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    527\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: \nAll the 864 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n864 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/user/torch_ubuntu/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/user/torch_ubuntu/lib/python3.12/site-packages/lightgbm/sklearn.py\", line 1560, in fit\n    super().fit(\n  File \"/home/user/torch_ubuntu/lib/python3.12/site-packages/lightgbm/sklearn.py\", line 1049, in fit\n    self._Booster = train(\n                    ^^^^^^\n  File \"/home/user/torch_ubuntu/lib/python3.12/site-packages/lightgbm/engine.py\", line 297, in train\n    booster = Booster(params=params, train_set=train_set)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/torch_ubuntu/lib/python3.12/site-packages/lightgbm/basic.py\", line 3660, in __init__\n    _safe_call(\n  File \"/home/user/torch_ubuntu/lib/python3.12/site-packages/lightgbm/basic.py\", line 313, in _safe_call\n    raise LightGBMError(_LIB.LGBM_GetLastError().decode(\"utf-8\"))\nlightgbm.basic.LightGBMError: No OpenCL device found\n"
     ]
    }
   ],
   "source": [
    "# ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ ìµœì í™” ë° í•™ìŠµ\n",
    "binary_preds_selected = {}\n",
    "for col in targets_binary:\n",
    "    y = train_df[col]\n",
    "    \n",
    "    # ì´ì§„ ë¶„ë¥˜ìš© ëª¨ë¸ ì„¤ì •\n",
    "    binary_model = LGBMClassifier(objective='binary', **lgbm_params)\n",
    "    \n",
    "    # ê·¸ë¦¬ë“œ ì„œì¹˜ë¡œ ìµœì  íŒŒë¼ë¯¸í„° ì°¾ê¸°\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=binary_model,\n",
    "        param_grid=binary_param_grid,\n",
    "        scoring='f1',  # ë˜ëŠ” 'roc_auc', 'precision', 'recall' ë“± ë¬¸ì œì— ì í•©í•œ ì§€í‘œ ì„ íƒ\n",
    "        cv=cv,\n",
    "        verbose=1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{col} ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ ìµœì í™” ì¤‘...\")\n",
    "    grid_search.fit(X_selected, y)\n",
    "    \n",
    "    print(f\"ìµœì  íŒŒë¼ë¯¸í„°: {grid_search.best_params_}\")\n",
    "    print(f\"ìµœê³  ì ìˆ˜: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # ìµœì  ëª¨ë¸ë¡œ ì˜ˆì¸¡\n",
    "    best_model = grid_search.best_estimator_\n",
    "    binary_preds_selected[col] = best_model.predict(test_X_selected)\n",
    "\n",
    "# ë‹¤ì¤‘ ë¶„ë¥˜ (S1) ëª¨ë¸ ìµœì í™” ë° í•™ìŠµ\n",
    "y_multi = train_df['S1']\n",
    "n_classes = len(y_multi.unique())\n",
    "\n",
    "# ë‹¤ì¤‘ ë¶„ë¥˜ìš© ëª¨ë¸ ì„¤ì •\n",
    "multiclass_model = LGBMClassifier(objective='multiclass', num_class=n_classes, **lgbm_params)\n",
    "\n",
    "# ê·¸ë¦¬ë“œ ì„œì¹˜ë¡œ ìµœì  íŒŒë¼ë¯¸í„° ì°¾ê¸°\n",
    "grid_search_multi = GridSearchCV(\n",
    "    estimator=multiclass_model,\n",
    "    param_grid=multi_param_grid,\n",
    "    scoring='f1_macro',  # ë‹¤ì¤‘ ë¶„ë¥˜ì—ì„œëŠ” macro í‰ê·  ì‚¬ìš©\n",
    "    cv=cv,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\nS1 ë‹¤ì¤‘ ë¶„ë¥˜ ëª¨ë¸ ìµœì í™” ì¤‘...\")\n",
    "grid_search_multi.fit(X_selected, y_multi)\n",
    "\n",
    "print(f\"ìµœì  íŒŒë¼ë¯¸í„°: {grid_search_multi.best_params_}\")\n",
    "print(f\"ìµœê³  ì ìˆ˜: {grid_search_multi.best_score_:.4f}\")\n",
    "\n",
    "# ìµœì  ëª¨ë¸ë¡œ ì˜ˆì¸¡\n",
    "best_multi_model = grid_search_multi.best_estimator_\n",
    "multiclass_pred_selected = best_multi_model.predict(test_X_selected)\n",
    "\n",
    "# # ì´ì§„ ë¶„ë¥˜\n",
    "# binary_preds_selected = {}\n",
    "# for col in targets_binary:\n",
    "#     y = train_df[col]\n",
    "#     # ì´ì§„ ë¶„ë¥˜ìš© íŒŒë¼ë¯¸í„° ì¡°ì •\n",
    "#     binary_params = lgbm_params.copy()\n",
    "#     binary_params['objective'] = 'binary'\n",
    "    \n",
    "#     model = LGBMClassifier(**binary_params)\n",
    "#     model.fit(X_selected, y)\n",
    "#     binary_preds_selected[col] = model.predict(test_X_selected)\n",
    "\n",
    "# # ë‹¤ì¤‘ ë¶„ë¥˜ (S1)\n",
    "# y_multi = train_df['S1']\n",
    "# # ë‹¤ì¤‘ ë¶„ë¥˜ìš© íŒŒë¼ë¯¸í„° ì„¤ì • (í´ë˜ìŠ¤ ìˆ˜ ì¶”ê°€)\n",
    "# multiclass_params = lgbm_params.copy()\n",
    "# multiclass_params['num_class'] = len(y_multi.unique())\n",
    "\n",
    "# model_s1_selected = LGBMClassifier(**multiclass_params)\n",
    "# model_s1_selected.fit(X_selected, y_multi)\n",
    "# multiclass_pred_selected = model_s1_selected.predict(test_X_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aa052704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== LightGBM ìµœì í™” ëª¨ë¸ í‰ê°€ (S1) =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.38      0.39        29\n",
      "           1       0.52      0.69      0.59        45\n",
      "           2       0.33      0.06      0.11        16\n",
      "\n",
      "    accuracy                           0.48        90\n",
      "   macro avg       0.42      0.38      0.36        90\n",
      "weighted avg       0.45      0.48      0.44        90\n",
      "\n",
      "\n",
      "í˜¼ë™ í–‰ë ¬:\n",
      "[[11 18  0]\n",
      " [12 31  2]\n",
      " [ 4 11  1]]\n"
     ]
    }
   ],
   "source": [
    "# ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ ì„±ëŠ¥ í‰ê°€\n",
    "X_train_sel, X_val_sel, y_train, y_val = train_test_split(\n",
    "    X_selected, y_multi, test_size=0.2, random_state=42, stratify=y_multi\n",
    ")\n",
    "\n",
    "# ìµœì  íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ì¦ìš© ëª¨ë¸ í•™ìŠµ\n",
    "eval_model_sel = LGBMClassifier(**grid_search_multi.best_params_)\n",
    "eval_model_sel.fit(X_train_sel, y_train)\n",
    "\n",
    "# ì˜ˆì¸¡ ë° í‰ê°€\n",
    "y_pred_sel = eval_model_sel.predict(X_val_sel)\n",
    "\n",
    "print(\"\\n===== LightGBM ìµœì í™” ëª¨ë¸ í‰ê°€ (S1) =====\")\n",
    "print(classification_report(y_val, y_pred_sel))\n",
    "print(\"\\ní˜¼ë™ í–‰ë ¬:\")\n",
    "print(confusion_matrix(y_val, y_pred_sel))\n",
    "\n",
    "# # ì„±ëŠ¥ í‰ê°€ (ì›ë³¸ ëª¨ë¸ê³¼ ë¹„êµ)\n",
    "# # ê²€ì¦ ë°ì´í„° ë¶„í• \n",
    "# X_train_sel, X_val_sel, y_train, y_val = train_test_split(\n",
    "#     X_selected, y_multi, test_size=0.2, random_state=42, stratify=y_multi\n",
    "# )\n",
    "\n",
    "# # í‰ê°€ìš© ëª¨ë¸ í•™ìŠµ\n",
    "# eval_model_sel = LGBMClassifier(**multiclass_params)\n",
    "# eval_model_sel.fit(X_train_sel, y_train)\n",
    "\n",
    "# # ì˜ˆì¸¡ ë° í‰ê°€\n",
    "# y_pred_sel = eval_model_sel.predict(X_val_sel)\n",
    "\n",
    "# print(\"\\n===== LightGBM ê·¸ë£¹ ê¸°ë°˜ íŠ¹ì„± ì„ íƒ ëª¨ë¸ í‰ê°€ (S1) =====\")\n",
    "# print(classification_report(y_val, y_pred_sel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be418612",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # íŠ¹ì„± ì¤‘ìš”ë„ ì¶”ì¶œ ë° ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "# importances = model_s1_selected.feature_importances_\n",
    "# feature_importance = pd.DataFrame({\n",
    "#     'feature': important_features,\n",
    "#     'importance': importances\n",
    "# })\n",
    "# # ì¤‘ìš”ë„ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬\n",
    "# feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "# # ì‹œê°í™”\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# sns.barplot(x='importance', y='feature', data=feature_importance)\n",
    "# plt.title('Feature Importance')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('feature_importance.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36534b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ì´ì§„ ë¶„ë¥˜ í•™ìŠµ \n",
    "# for col in targets_binary: \n",
    "#     y = train_df[col] \n",
    "#     model = RandomForestClassifier(**common_params) \n",
    "#     model.fit(X, y) \n",
    "#     binary_preds[col] = model.predict(test_X) # ğŸ”¥ í™•ë¥ X, í´ë˜ìŠ¤ ì§ì ‘ ì˜ˆì¸¡ \n",
    "\n",
    "# # ë‹¤ì¤‘ ë¶„ë¥˜ í•™ìŠµ (S1) \n",
    "# y_multi = train_df['S1'] \n",
    "# model_s1 = RandomForestClassifier(**common_params) \n",
    "# model_s1.fit(X, y_multi) \n",
    "# multiclass_pred = model_s1.predict(test_X) # ğŸ”¥ í´ë˜ìŠ¤ ì§ì ‘ ì˜ˆì¸¡ \n",
    "\n",
    "# # importance ì¶œë ¥\n",
    "# feature_importance = pd.DataFrame({ \n",
    "#     'feature': X.columns, \n",
    "#     'importance': model_s1.feature_importances_ \n",
    "# }).sort_values('importance', ascending=False) \n",
    "\n",
    "# # ì‹œê°í™” \n",
    "# plt.figure(figsize=(10, 10)) \n",
    "# sns.barplot(x='importance', y='feature', data=feature_importance) \n",
    "# plt.title('Feature Importance')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('feature_importance.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0be857a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample ê¸°ë°˜ ì œì¶œ í¬ë§· ê°€ì ¸ì˜¤ê¸°\n",
    "submission_final = sample_submission[['subject_id', 'sleep_date', 'lifelog_date']].copy()\n",
    "\n",
    "# lifelog_date ê¸°ì¤€ìœ¼ë¡œ string â†’ date í˜•ì‹ í†µì¼\n",
    "submission_final['lifelog_date'] = pd.to_datetime(submission_final['lifelog_date']).dt.date\n",
    "\n",
    "# ID ë§Œë“¤ê¸° (submissionì—ì„œ ì˜ˆì¸¡í•œ ê²°ê³¼ì™€ ì—°ê²°í•˜ê¸° ìœ„í•´)\n",
    "submission_final['ID'] = submission_final['subject_id'] + '_' + submission_final['lifelog_date'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a2f142a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ì˜ˆì¸¡ ê²°ê³¼ ì—°ê²°í•  ìˆ˜ ìˆë„ë¡ ë™ì¼í•œ ìˆœì„œë¡œ ì •ë ¬\n",
    "# ë³´í†µ ì˜ˆì¸¡ ê²°ê³¼ëŠ” test_df ê¸°ì¤€ì´ë¯€ë¡œ ì •ë ¬ ë³´ì¥ë˜ì–´ì•¼ í•¨\n",
    "assert len(submission_final) == len(multiclass_pred_selected) # shape ì²´í¬\n",
    "\n",
    "# ë‹¤ì¤‘ ë¶„ë¥˜ ì˜ˆì¸¡ ë¶™ì´ê¸°\n",
    "submission_final['S1'] = multiclass_pred_selected\n",
    "\n",
    "# ì´ì§„ ë¶„ë¥˜ ê²°ê³¼ ë¶™ì´ê¸°\n",
    "for col in ['Q1', 'Q2', 'Q3', 'S2', 'S3']:\n",
    "    submission_final[col] = binary_preds_selected[col].astype(int) # í™•ë¥  ì•„ë‹Œ class ì˜ˆì¸¡\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "84ac68e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì œì¶œ íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: c:\\Users\\KTL\\Desktop\\submission_final_mod3_0519_1554.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ìµœì¢… ì œì¶œ í˜•ì‹ ì •ë ¬\n",
    "submission_final = submission_final[['subject_id', 'sleep_date', 'lifelog_date', 'Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']]\n",
    "\n",
    "# ì €ì¥\n",
    "submission_final.to_csv(submission_folder + submission_file, index=False)\n",
    "\n",
    "# VSCodeì—ì„œëŠ” files.download()ê°€ ì‘ë™í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ëŒ€ì²´\n",
    "\n",
    "print(f\"âœ… ì œì¶œ íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {os.path.abspath(submission_file)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa12e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ: binary_model_Q1.pkl\n",
      "âœ… ì €ì¥ ì™„ë£Œ: binary_model_Q2.pkl\n",
      "âœ… ì €ì¥ ì™„ë£Œ: binary_model_Q3.pkl\n",
      "âœ… ì €ì¥ ì™„ë£Œ: binary_model_S2.pkl\n",
      "âœ… ì €ì¥ ì™„ë£Œ: binary_model_S3.pkl\n",
      "âœ… ì €ì¥ ì™„ë£Œ: multiclass_model_S1.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # ëª¨ë¸ ì €ì¥ (ì¶”ê°€ ê¸°ëŠ¥)\n",
    "# import joblib\n",
    "\n",
    "# # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "# os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# # ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ ì €ì¥\n",
    "# for col in targets_binary:\n",
    "#     joblib.dump(binary_preds[col], f'models/binary_model_{col}.pkl')\n",
    "#     print(f\"âœ… ì €ì¥ ì™„ë£Œ: binary_model_{col}.pkl\")\n",
    "\n",
    "# # ë‹¤ì¤‘ ë¶„ë¥˜ ëª¨ë¸ ì €ì¥\n",
    "# joblib.dump(model_s1, 'models/multiclass_model_S1.pkl')\n",
    "# print(f\"âœ… ì €ì¥ ì™„ë£Œ: multiclass_model_S1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e85529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== ì£¼ìš” íŠ¹ì„± ì¤‘ìš”ë„ (ìƒìœ„ 20ê°œ) =====\n",
      "                   feature  importance\n",
      "44         screen_on_ratio    0.021762\n",
      "48             others_time    0.020832\n",
      "46  screen_on_duration_avg    0.020828\n",
      "7         activity_3_ratio    0.017365\n",
      "62           wifi_rssi_max    0.016938\n",
      "60          wifi_rssi_mean    0.016477\n",
      "3    max_charging_duration    0.016160\n",
      "26               rssi_mean    0.015765\n",
      "0           charging_ratio    0.015758\n",
      "8         activity_4_ratio    0.015606\n",
      "41        light_night_mean    0.015580\n",
      "47  screen_on_duration_max    0.015548\n",
      "39               light_max    0.015141\n",
      "2    avg_charging_duration    0.014409\n",
      "31           altitude_mean    0.013876\n",
      "38               light_std    0.013854\n",
      "63       wifi_detected_cnt    0.013744\n",
      "35             speed_max_x    0.013394\n",
      "57                ë©”ì‹œì§€_time    0.013303\n",
      "19     Narration_monologue    0.013183\n"
     ]
    }
   ],
   "source": [
    " \n",
    "# # ê²°ê³¼ ìš”ì•½ - íŠ¹ì„± ì¤‘ìš”ë„ ìƒìœ„ 20ê°œ í‘œì‹œ\n",
    "# print(\"\\n===== ì£¼ìš” íŠ¹ì„± ì¤‘ìš”ë„ (ìƒìœ„ 20ê°œ) =====\")\n",
    "# print(feature_importance.head(20))\n",
    "\n",
    "# # ëª¨ë¸ í‰ê°€ (ê²€ì¦ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# # ê²€ì¦ìš© ë°ì´í„° ë¶„ë¦¬\n",
    "# X_train, X_val, y_train, y_val = train_test_split(\n",
    "#     X, y_multi, test_size=0.2, random_state=42, stratify=y_multi\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1657b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== ë‹¤ì¤‘ ë¶„ë¥˜ ëª¨ë¸ í‰ê°€ (S1) =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.24      0.31        29\n",
      "           1       0.52      0.84      0.64        45\n",
      "           2       1.00      0.06      0.12        16\n",
      "\n",
      "    accuracy                           0.51        90\n",
      "   macro avg       0.65      0.38      0.36        90\n",
      "weighted avg       0.58      0.51      0.44        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # í‰ê°€ìš© ëª¨ë¸ í•™ìŠµ\n",
    "# eval_model = RandomForestClassifier(**common_params)\n",
    "# eval_model.fit(X_train, y_train)\n",
    "\n",
    "# # ì˜ˆì¸¡ ë° í‰ê°€\n",
    "# y_pred = eval_model.predict(X_val)\n",
    "# print(\"\\n===== ë‹¤ì¤‘ ë¶„ë¥˜ ëª¨ë¸ í‰ê°€ (S1) =====\")\n",
    "# print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623ab1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ í‰ê°€ (Q1) =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.58      0.58        45\n",
      "           1       0.59      0.60      0.59        45\n",
      "\n",
      "    accuracy                           0.59        90\n",
      "   macro avg       0.59      0.59      0.59        90\n",
      "weighted avg       0.59      0.59      0.59        90\n",
      "\n",
      "\n",
      "===== ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ í‰ê°€ (Q2) =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.54      0.51        39\n",
      "           1       0.62      0.57      0.59        51\n",
      "\n",
      "    accuracy                           0.56        90\n",
      "   macro avg       0.55      0.55      0.55        90\n",
      "weighted avg       0.56      0.56      0.56        90\n",
      "\n",
      "\n",
      "===== ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ í‰ê°€ (Q3) =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.28      0.36        36\n",
      "           1       0.63      0.81      0.71        54\n",
      "\n",
      "    accuracy                           0.60        90\n",
      "   macro avg       0.56      0.55      0.53        90\n",
      "weighted avg       0.58      0.60      0.57        90\n",
      "\n",
      "\n",
      "===== ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ í‰ê°€ (S2) =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.29      0.34        31\n",
      "           1       0.68      0.78      0.72        59\n",
      "\n",
      "    accuracy                           0.61        90\n",
      "   macro avg       0.54      0.53      0.53        90\n",
      "weighted avg       0.58      0.61      0.59        90\n",
      "\n",
      "\n",
      "===== ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ í‰ê°€ (S3) =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.40      0.44        30\n",
      "           1       0.72      0.78      0.75        60\n",
      "\n",
      "    accuracy                           0.66        90\n",
      "   macro avg       0.60      0.59      0.59        90\n",
      "weighted avg       0.64      0.66      0.65        90\n",
      "\n",
      "\n",
      "âœ… ëª¨ë“  ê³¼ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ í‰ê°€\n",
    "# for col in targets_binary:\n",
    "#     y_binary = train_df[col]\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(\n",
    "#         X, y_binary, test_size=0.2, random_state=42, stratify=y_binary\n",
    "#     )\n",
    "    \n",
    "#     eval_model = RandomForestClassifier(**common_params)\n",
    "#     eval_model.fit(X_train, y_train)\n",
    "    \n",
    "#     y_pred = eval_model.predict(X_val)\n",
    "#     print(f\"\\n===== ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ í‰ê°€ ({col}) =====\")\n",
    "#     print(classification_report(y_val, y_pred))\n",
    "\n",
    "# print(\"\\nâœ… ëª¨ë“  ê³¼ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba047669",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_ubuntu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
